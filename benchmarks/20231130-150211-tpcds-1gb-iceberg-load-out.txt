Listening for transport dt_socket at address: 40263
23/11/30 15:03:18 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.
23/11/30 15:03:18 WARN SparkConf: The configuration key 'spark.yarn.driver.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.driver.memoryOverhead' instead.
23/11/30 15:03:18 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.
23/11/30 15:03:18 WARN SparkConf: The configuration key 'spark.yarn.driver.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.driver.memoryOverhead' instead.
23/11/30 15:03:19 INFO SignalUtils: Registering signal handler for INT
23/11/30 15:03:19 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.
23/11/30 15:03:19 WARN SparkConf: The configuration key 'spark.yarn.driver.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.driver.memoryOverhead' instead.
23/11/30 15:03:23 INFO HiveConf: Found configuration file file:/var/lib/fk-pf-spark3/conf/hive-site.xml
23/11/30 15:03:23 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.
23/11/30 15:03:23 WARN SparkConf: The configuration key 'spark.yarn.driver.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.driver.memoryOverhead' instead.
23/11/30 15:03:23 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.
23/11/30 15:03:23 WARN SparkConf: The configuration key 'spark.yarn.driver.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.driver.memoryOverhead' instead.
23/11/30 15:03:23 INFO SparkContext: Running Spark version 3.1.2
23/11/30 15:03:23 INFO ResourceUtils: ==============================================================
23/11/30 15:03:23 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/30 15:03:23 INFO ResourceUtils: ==============================================================
23/11/30 15:03:23 INFO SparkContext: Submitted application: Spark shell
23/11/30 15:03:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 4096, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 10240, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/30 15:03:23 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/11/30 15:03:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/30 15:03:23 INFO SecurityManager: Changing view acls to: vanshika.yadav
23/11/30 15:03:23 INFO SecurityManager: Changing modify acls to: vanshika.yadav
23/11/30 15:03:23 INFO SecurityManager: Changing view acls groups to: 
23/11/30 15:03:23 INFO SecurityManager: Changing modify acls groups to: 
23/11/30 15:03:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(vanshika.yadav); groups with view permissions: Set(); users  with modify permissions: Set(vanshika.yadav); groups with modify permissions: Set()
23/11/30 15:03:23 INFO Utils: Successfully started service 'sparkDriver' on port 40443.
23/11/30 15:03:23 INFO SparkEnv: Registering MapOutputTracker
23/11/30 15:03:23 INFO SparkEnv: Registering BlockManagerMaster
23/11/30 15:03:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/30 15:03:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/30 15:03:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/30 15:03:24 INFO DiskBlockManager: Created local directory at /grid/1/spark3/tmp/blockmgr-b5a932c5-e69f-4da0-bd8c-b0f8d6d4bf29
23/11/30 15:03:24 INFO MemoryStore: MemoryStore started with capacity 2.5 GiB
23/11/30 15:03:24 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/30 15:03:24 INFO log: Logging initialized @6373ms to org.sparkproject.jetty.util.log.Slf4jLog
23/11/30 15:03:24 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_172-b11
23/11/30 15:03:24 INFO Server: Started @6479ms
23/11/30 15:03:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/11/30 15:03:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/11/30 15:03:24 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/11/30 15:03:24 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
23/11/30 15:03:24 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
23/11/30 15:03:24 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
23/11/30 15:03:24 INFO AbstractConnector: Started ServerConnector@1822a61b{HTTP/1.1, (http/1.1)}{0.0.0.0:4046}
23/11/30 15:03:24 INFO Utils: Successfully started service 'SparkUI' on port 4046.
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7fba492e{/jobs,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7a7bb083{/jobs/json,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@58c80957{/jobs/job,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7d44eab{/jobs/job/json,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@620f7a39{/stages,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2b5ca574{/stages/json,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3891b430{/stages/stage,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1f5f4d48{/stages/stage/json,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2c8e096{/stages/pool,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@587bf2e1{/stages/pool/json,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@20b921be{/storage,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@44723d95{/storage/json,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4df13b7e{/storage/rdd,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@12b66c89{/storage/rdd/json,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@10660795{/environment,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1dbfbd94{/environment/json,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2e3b4394{/executors,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@10ffe32f{/executors/json,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@67d55a46{/executors/threadDump,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1d982fbe{/executors/threadDump/json,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@779136bf{/static,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@34376069{/,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6cac0334{/api,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@67167f8f{/jobs/job/kill,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@733345f5{/stages/stage/kill,null,AVAILABLE,@Spark}
23/11/30 15:03:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:4046
23/11/30 15:03:24 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
23/11/30 15:03:24 INFO AHSProxy: Connecting to Application History server at fks-fdp-galaxy-pluto-hs-0001-m.c.fks-fdp-galaxy.internal/10.116.4.108:10200
23/11/30 15:03:24 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2
23/11/30 15:03:24 INFO Client: Requesting a new application from cluster with 30 NodeManagers
23/11/30 15:03:25 INFO Configuration: resource-types.xml not found
23/11/30 15:03:25 INFO ResourceUtils: Unable to find 'resource-types.xml'.
23/11/30 15:03:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (163840 MB per container)
23/11/30 15:03:25 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
23/11/30 15:03:25 INFO Client: Setting up container launch context for our AM
23/11/30 15:03:25 INFO Client: Setting up the launch environment for our AM container
23/11/30 15:03:25 INFO Client: Preparing resources for our AM container
23/11/30 15:03:26 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/HikariCP-2.5.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/HikariCP-2.5.1.jar
23/11/30 15:03:26 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:26 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/JLargeArrays-1.5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/JLargeArrays-1.5.jar
23/11/30 15:03:26 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:26 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/JTransforms-3.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/JTransforms-3.1.jar
23/11/30 15:03:26 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:26 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/RoaringBitmap-0.9.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/RoaringBitmap-0.9.0.jar
23/11/30 15:03:26 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:26 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/ST4-4.0.4.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/ST4-4.0.4.jar
23/11/30 15:03:26 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:26 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/accessors-smart-1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/accessors-smart-1.2.jar
23/11/30 15:03:26 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:26 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/activation-1.1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/activation-1.1.1.jar
23/11/30 15:03:27 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:27 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/aircompressor-0.10.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/aircompressor-0.10.jar
23/11/30 15:03:27 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:27 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/algebra_2.12-2.0.0-M2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/algebra_2.12-2.0.0-M2.jar
23/11/30 15:03:27 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:27 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/antlr-runtime-3.5.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/antlr-runtime-3.5.2.jar
23/11/30 15:03:27 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:27 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/antlr4-runtime-4.8-1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/antlr4-runtime-4.8-1.jar
23/11/30 15:03:27 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:27 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/aopalliance-1.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/aopalliance-1.0.jar
23/11/30 15:03:27 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:27 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/aopalliance-repackaged-2.6.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/aopalliance-repackaged-2.6.1.jar
23/11/30 15:03:27 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:27 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/arpack_combined_all-0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/arpack_combined_all-0.1.jar
23/11/30 15:03:27 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:27 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/arrow-format-2.0.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/arrow-format-2.0.0.jar
23/11/30 15:03:27 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:27 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/arrow-memory-core-2.0.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/arrow-memory-core-2.0.0.jar
23/11/30 15:03:27 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:27 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/arrow-memory-netty-2.0.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/arrow-memory-netty-2.0.0.jar
23/11/30 15:03:27 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:27 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/arrow-vector-2.0.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/arrow-vector-2.0.0.jar
23/11/30 15:03:28 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:28 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/audience-annotations-0.5.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/audience-annotations-0.5.0.jar
23/11/30 15:03:28 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:28 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/avro-1.8.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/avro-1.8.2.jar
23/11/30 15:03:28 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:28 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/avro-ipc-1.8.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/avro-ipc-1.8.2.jar
23/11/30 15:03:28 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:28 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/avro-mapred-1.8.2-hadoop2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/avro-mapred-1.8.2-hadoop2.jar
23/11/30 15:03:28 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:28 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/bcpkix-jdk15on-1.60.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/bcpkix-jdk15on-1.60.jar
23/11/30 15:03:28 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:28 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/bcprov-jdk15on-1.60.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/bcprov-jdk15on-1.60.jar
23/11/30 15:03:28 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:28 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/bigquery-connector-hadoop3-latest.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/bigquery-connector-hadoop3-latest.jar
23/11/30 15:03:28 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:29 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/bonecp-0.8.0.RELEASE.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/bonecp-0.8.0.RELEASE.jar
23/11/30 15:03:29 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:29 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/breeze-macros_2.12-1.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/breeze-macros_2.12-1.0.jar
23/11/30 15:03:29 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:29 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/breeze_2.12-1.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/breeze_2.12-1.0.jar
23/11/30 15:03:29 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:29 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/cats-kernel_2.12-2.0.0-M4.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/cats-kernel_2.12-2.0.0-M4.jar
23/11/30 15:03:29 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:29 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/chill-java-0.9.5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/chill-java-0.9.5.jar
23/11/30 15:03:29 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:29 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/chill_2.12-0.9.5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/chill_2.12-0.9.5.jar
23/11/30 15:03:29 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:29 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-beanutils-1.9.4.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-beanutils-1.9.4.jar
23/11/30 15:03:29 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:29 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-cli-1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-cli-1.2.jar
23/11/30 15:03:29 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:30 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-codec-1.10.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-codec-1.10.jar
23/11/30 15:03:30 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:30 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-collections-3.2.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-collections-3.2.2.jar
23/11/30 15:03:30 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:30 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-compiler-3.0.16.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-compiler-3.0.16.jar
23/11/30 15:03:30 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:30 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-compress-1.20.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-compress-1.20.jar
23/11/30 15:03:30 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:30 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-configuration2-2.1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-configuration2-2.1.1.jar
23/11/30 15:03:30 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:30 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-crypto-1.1.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-crypto-1.1.0.jar
23/11/30 15:03:30 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:30 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-daemon-1.0.13.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-daemon-1.0.13.jar
23/11/30 15:03:30 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:30 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-dbcp-1.4.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-dbcp-1.4.jar
23/11/30 15:03:30 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:30 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-httpclient-3.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-httpclient-3.1.jar
23/11/30 15:03:30 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:30 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-io-2.5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-io-2.5.jar
23/11/30 15:03:30 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:30 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-lang-2.6.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-lang-2.6.jar
23/11/30 15:03:31 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:31 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-lang3-3.10.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-lang3-3.10.jar
23/11/30 15:03:31 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:31 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-logging-1.1.3.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-logging-1.1.3.jar
23/11/30 15:03:31 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:31 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-math3-3.4.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-math3-3.4.1.jar
23/11/30 15:03:31 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:31 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-net-3.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-net-3.1.jar
23/11/30 15:03:31 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:31 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-pool-1.5.4.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-pool-1.5.4.jar
23/11/30 15:03:31 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:31 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/commons-text-1.6.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/commons-text-1.6.jar
23/11/30 15:03:31 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:31 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/compress-lzf-1.0.3.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/compress-lzf-1.0.3.jar
23/11/30 15:03:31 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:31 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/core-1.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/core-1.1.2.jar
23/11/30 15:03:31 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:31 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/curator-client-2.13.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/curator-client-2.13.0.jar
23/11/30 15:03:31 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:31 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/curator-framework-2.13.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/curator-framework-2.13.0.jar
23/11/30 15:03:31 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:32 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/curator-recipes-2.13.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/curator-recipes-2.13.0.jar
23/11/30 15:03:32 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:32 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/datanucleus-api-jdo-4.2.4.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/datanucleus-api-jdo-4.2.4.jar
23/11/30 15:03:32 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:32 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/datanucleus-core-4.1.17.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/datanucleus-core-4.1.17.jar
23/11/30 15:03:32 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:32 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/datanucleus-rdbms-4.1.19.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/datanucleus-rdbms-4.1.19.jar
23/11/30 15:03:32 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:32 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/derby-10.12.1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/derby-10.12.1.1.jar
23/11/30 15:03:32 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:32 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/dnsjava-2.1.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/dnsjava-2.1.7.jar
23/11/30 15:03:32 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:32 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar
23/11/30 15:03:32 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:32 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/ehcache-3.3.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/ehcache-3.3.1.jar
23/11/30 15:03:32 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:32 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/fdp-deps-3.1.2.fdp.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/fdp-deps-3.1.2.fdp.1.jar
23/11/30 15:03:32 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:32 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/flatbuffers-java-1.9.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/flatbuffers-java-1.9.0.jar
23/11/30 15:03:32 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:32 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/gcs-connector-hadoop3-2.2.7-shaded.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/gcs-connector-hadoop3-2.2.7-shaded.jar
23/11/30 15:03:33 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:33 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/geronimo-jcache_1.0_spec-1.0-alpha-1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/geronimo-jcache_1.0_spec-1.0-alpha-1.jar
23/11/30 15:03:33 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:33 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/gson-2.2.4.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/gson-2.2.4.jar
23/11/30 15:03:33 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:33 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/guava-27.0-jre.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/guava-27.0-jre.jar
23/11/30 15:03:33 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:33 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/guice-4.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/guice-4.0.jar
23/11/30 15:03:33 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:33 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/guice-servlet-4.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/guice-servlet-4.0.jar
23/11/30 15:03:34 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:34 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-annotations-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-annotations-3.2.1.jar
23/11/30 15:03:34 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:34 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-auth-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-auth-3.2.1.jar
23/11/30 15:03:34 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:34 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-client-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-client-3.2.1.jar
23/11/30 15:03:34 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:34 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-common-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-common-3.2.1.jar
23/11/30 15:03:34 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:34 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-hdfs-client-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-hdfs-client-3.2.1.jar
23/11/30 15:03:34 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:34 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-mapreduce-client-common-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-mapreduce-client-common-3.2.1.jar
23/11/30 15:03:34 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:34 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-mapreduce-client-core-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-mapreduce-client-core-3.2.1.jar
23/11/30 15:03:34 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:34 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-mapreduce-client-jobclient-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-mapreduce-client-jobclient-3.2.1.jar
23/11/30 15:03:34 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:34 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-yarn-api-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-yarn-api-3.2.1.jar
23/11/30 15:03:34 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:35 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-yarn-client-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-yarn-client-3.2.1.jar
23/11/30 15:03:35 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:35 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-yarn-common-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-yarn-common-3.2.1.jar
23/11/30 15:03:35 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:35 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-yarn-registry-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-yarn-registry-3.2.1.jar
23/11/30 15:03:35 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:35 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-yarn-server-common-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-yarn-server-common-3.2.1.jar
23/11/30 15:03:35 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:35 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hadoop-yarn-server-web-proxy-3.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-yarn-server-web-proxy-3.2.1.jar
23/11/30 15:03:35 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:35 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-beeline-2.3.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-beeline-2.3.7.jar
23/11/30 15:03:35 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:35 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-bigquery-connector-2.0.0-SNAPSHOT-with-dependencies.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-bigquery-connector-2.0.0-SNAPSHOT-with-dependencies.jar
23/11/30 15:03:35 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:36 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-cli-2.3.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-cli-2.3.7.jar
23/11/30 15:03:36 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:36 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-common-2.3.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-common-2.3.7.jar
23/11/30 15:03:37 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:37 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-exec-2.3.7-core.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-exec-2.3.7-core.jar
23/11/30 15:03:37 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:37 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-jdbc-2.3.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-jdbc-2.3.7.jar
23/11/30 15:03:37 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:37 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-llap-common-2.3.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-llap-common-2.3.7.jar
23/11/30 15:03:37 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:37 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-metastore-2.3.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-metastore-2.3.7.jar
23/11/30 15:03:37 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:37 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-serde-2.3.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-serde-2.3.7.jar
23/11/30 15:03:37 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:37 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-service-rpc-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-service-rpc-3.1.2.jar
23/11/30 15:03:37 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:37 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-shims-0.23-2.3.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-shims-0.23-2.3.7.jar
23/11/30 15:03:37 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:37 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-shims-2.3.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-shims-2.3.7.jar
23/11/30 15:03:37 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-shims-common-2.3.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-shims-common-2.3.7.jar
23/11/30 15:03:38 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-shims-scheduler-2.3.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-shims-scheduler-2.3.7.jar
23/11/30 15:03:38 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-storage-api-2.7.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-storage-api-2.7.2.jar
23/11/30 15:03:38 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hive-vector-code-gen-2.3.7.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hive-vector-code-gen-2.3.7.jar
23/11/30 15:03:38 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hk2-api-2.6.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hk2-api-2.6.1.jar
23/11/30 15:03:38 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hk2-locator-2.6.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hk2-locator-2.6.1.jar
23/11/30 15:03:38 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/hk2-utils-2.6.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hk2-utils-2.6.1.jar
23/11/30 15:03:38 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/htrace-core4-4.1.0-incubating.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/htrace-core4-4.1.0-incubating.jar
23/11/30 15:03:38 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/httpclient-4.4.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/httpclient-4.4.1.jar
23/11/30 15:03:38 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/httpcore-4.4.12.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/httpcore-4.4.12.jar
23/11/30 15:03:38 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/istack-commons-runtime-3.0.8.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/istack-commons-runtime-3.0.8.jar
23/11/30 15:03:38 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/ivy-2.4.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/ivy-2.4.0.jar
23/11/30 15:03:38 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:38 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jackson-annotations-2.10.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jackson-annotations-2.10.0.jar
23/11/30 15:03:39 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:39 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jackson-core-2.10.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jackson-core-2.10.0.jar
23/11/30 15:03:39 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:39 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jackson-core-asl-1.9.13.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jackson-core-asl-1.9.13.jar
23/11/30 15:03:39 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:39 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jackson-databind-2.10.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jackson-databind-2.10.0.jar
23/11/30 15:03:39 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:39 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jackson-jaxrs-base-2.9.8.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jackson-jaxrs-base-2.9.8.jar
23/11/30 15:03:39 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:39 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jackson-jaxrs-json-provider-2.9.8.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jackson-jaxrs-json-provider-2.9.8.jar
23/11/30 15:03:39 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:39 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jackson-mapper-asl-1.9.13.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jackson-mapper-asl-1.9.13.jar
23/11/30 15:03:39 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:39 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jackson-module-jaxb-annotations-2.10.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jackson-module-jaxb-annotations-2.10.0.jar
23/11/30 15:03:39 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:39 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jackson-module-paranamer-2.10.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jackson-module-paranamer-2.10.0.jar
23/11/30 15:03:39 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:39 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jackson-module-scala_2.12-2.10.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jackson-module-scala_2.12-2.10.0.jar
23/11/30 15:03:39 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:39 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jakarta.activation-api-1.2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jakarta.activation-api-1.2.1.jar
23/11/30 15:03:39 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:39 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jakarta.annotation-api-1.3.5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jakarta.annotation-api-1.3.5.jar
23/11/30 15:03:39 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:39 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jakarta.inject-2.6.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jakarta.inject-2.6.1.jar
23/11/30 15:03:40 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:40 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jakarta.servlet-api-4.0.3.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jakarta.servlet-api-4.0.3.jar
23/11/30 15:03:40 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:40 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jakarta.validation-api-2.0.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jakarta.validation-api-2.0.2.jar
23/11/30 15:03:40 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:40 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jakarta.ws.rs-api-2.1.6.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jakarta.ws.rs-api-2.1.6.jar
23/11/30 15:03:40 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:40 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jakarta.xml.bind-api-2.3.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jakarta.xml.bind-api-2.3.2.jar
23/11/30 15:03:40 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:40 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/janino-3.0.16.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/janino-3.0.16.jar
23/11/30 15:03:40 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:40 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/javassist-3.25.0-GA.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/javassist-3.25.0-GA.jar
23/11/30 15:03:40 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:40 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/javax.inject-1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/javax.inject-1.jar
23/11/30 15:03:40 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:40 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/javax.jdo-3.2.0-m3.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/javax.jdo-3.2.0-m3.jar
23/11/30 15:03:40 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:40 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/javolution-5.5.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/javolution-5.5.1.jar
23/11/30 15:03:40 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:40 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jaxb-api-2.2.11.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jaxb-api-2.2.11.jar
23/11/30 15:03:40 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:40 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jaxb-runtime-2.3.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jaxb-runtime-2.3.2.jar
23/11/30 15:03:40 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:40 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jcip-annotations-1.0-1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jcip-annotations-1.0-1.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:41 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jcl-over-slf4j-1.7.30.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jcl-over-slf4j-1.7.30.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:41 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jdo-api-3.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jdo-api-3.0.1.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:41 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jersey-client-2.30.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jersey-client-2.30.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:41 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jersey-common-2.30.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jersey-common-2.30.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:41 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jersey-container-servlet-2.30.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jersey-container-servlet-2.30.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:41 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jersey-container-servlet-core-2.30.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jersey-container-servlet-core-2.30.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:41 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jersey-hk2-2.30.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jersey-hk2-2.30.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:41 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jersey-media-jaxb-2.30.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jersey-media-jaxb-2.30.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:41 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jersey-server-2.30.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jersey-server-2.30.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:41 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jline-2.14.6.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jline-2.14.6.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:41 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/joda-time-2.10.5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/joda-time-2.10.5.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:41 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jodd-core-3.5.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jodd-core-3.5.2.jar
23/11/30 15:03:41 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jpam-1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jpam-1.1.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/json-1.8.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/json-1.8.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/json-smart-2.3.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/json-smart-2.3.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/json4s-ast_2.12-3.7.0-M5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/json4s-ast_2.12-3.7.0-M5.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/json4s-core_2.12-3.7.0-M5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/json4s-core_2.12-3.7.0-M5.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/json4s-jackson_2.12-3.7.0-M5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/json4s-jackson_2.12-3.7.0-M5.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/json4s-scalap_2.12-3.7.0-M5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/json4s-scalap_2.12-3.7.0-M5.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jsp-api-2.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jsp-api-2.1.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jsr305-3.0.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jsr305-3.0.0.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jta-1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jta-1.1.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/jul-to-slf4j-1.7.30.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/jul-to-slf4j-1.7.30.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerb-admin-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerb-admin-1.0.1.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerb-client-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerb-client-1.0.1.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:42 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerb-common-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerb-common-1.0.1.jar
23/11/30 15:03:42 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerb-core-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerb-core-1.0.1.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerb-crypto-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerb-crypto-1.0.1.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerb-identity-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerb-identity-1.0.1.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerb-server-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerb-server-1.0.1.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerb-simplekdc-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerb-simplekdc-1.0.1.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerb-util-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerb-util-1.0.1.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerby-asn1-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerby-asn1-1.0.1.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerby-config-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerby-config-1.0.1.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerby-pkix-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerby-pkix-1.0.1.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerby-util-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerby-util-1.0.1.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kerby-xdr-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kerby-xdr-1.0.1.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/kryo-shaded-4.0.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/kryo-shaded-4.0.2.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/leveldbjni-all-1.8.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/leveldbjni-all-1.8.jar
23/11/30 15:03:43 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:43 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/libfb303-0.9.3.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/libfb303-0.9.3.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:44 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/libthrift-0.12.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/libthrift-0.12.0.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:44 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/log4j-1.2.17.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/log4j-1.2.17.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:44 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/lz4-java-1.7.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/lz4-java-1.7.1.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:44 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/machinist_2.12-0.6.8.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/machinist_2.12-0.6.8.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:44 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/macro-compat_2.12-1.1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/macro-compat_2.12-1.1.1.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:44 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/metrics-core-4.1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/metrics-core-4.1.1.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:44 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/metrics-graphite-4.1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/metrics-graphite-4.1.1.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:44 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/metrics-jmx-4.1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/metrics-jmx-4.1.1.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:44 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/metrics-json-4.1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/metrics-json-4.1.1.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:44 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/metrics-jvm-4.1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/metrics-jvm-4.1.1.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:44 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/minlog-1.3.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/minlog-1.3.0.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:44 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/netty-all-4.1.51.Final.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/netty-all-4.1.51.Final.jar
23/11/30 15:03:44 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/nimbus-jose-jwt-4.41.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/nimbus-jose-jwt-4.41.1.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/objenesis-2.6.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/objenesis-2.6.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/okhttp-2.7.5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/okhttp-2.7.5.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/okio-1.14.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/okio-1.14.0.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/opencsv-2.3.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/opencsv-2.3.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/orc-core-1.5.12.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/orc-core-1.5.12.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/orc-mapreduce-1.5.12.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/orc-mapreduce-1.5.12.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/orc-shims-1.5.12.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/orc-shims-1.5.12.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/oro-2.0.8.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/oro-2.0.8.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/osgi-resource-locator-1.0.3.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/osgi-resource-locator-1.0.3.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/paranamer-2.8.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/paranamer-2.8.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/parquet-column-1.10.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/parquet-column-1.10.1.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/parquet-common-1.10.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/parquet-common-1.10.1.jar
23/11/30 15:03:45 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:45 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/parquet-encoding-1.10.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/parquet-encoding-1.10.1.jar
23/11/30 15:03:46 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:46 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/parquet-format-2.4.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/parquet-format-2.4.0.jar
23/11/30 15:03:46 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:46 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/parquet-hadoop-1.10.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/parquet-hadoop-1.10.1.jar
23/11/30 15:03:46 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:46 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/parquet-jackson-1.10.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/parquet-jackson-1.10.1.jar
23/11/30 15:03:46 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:46 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/protobuf-java-2.5.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/protobuf-java-2.5.0.jar
23/11/30 15:03:46 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:46 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/py4j-0.10.9.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/py4j-0.10.9.jar
23/11/30 15:03:46 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:46 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/pyrolite-4.30.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/pyrolite-4.30.jar
23/11/30 15:03:46 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:46 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/re2j-1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/re2j-1.1.jar
23/11/30 15:03:46 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:46 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/scala-collection-compat_2.12-2.1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/scala-collection-compat_2.12-2.1.1.jar
23/11/30 15:03:46 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:46 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/scala-compiler-2.12.10.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/scala-compiler-2.12.10.jar
23/11/30 15:03:46 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:46 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/scala-library-2.12.10.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/scala-library-2.12.10.jar
23/11/30 15:03:46 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:47 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/scala-parser-combinators_2.12-1.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/scala-parser-combinators_2.12-1.1.2.jar
23/11/30 15:03:47 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:47 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/scala-reflect-2.12.10.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/scala-reflect-2.12.10.jar
23/11/30 15:03:47 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:47 WARN DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
23/11/30 15:03:47 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/scala-xml_2.12-1.2.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/scala-xml_2.12-1.2.0.jar
23/11/30 15:03:47 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:47 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/shapeless_2.12-2.3.3.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/shapeless_2.12-2.3.3.jar
23/11/30 15:03:47 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:47 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/shims-0.9.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/shims-0.9.0.jar
23/11/30 15:03:47 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:47 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/slf4j-api-1.7.30.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/slf4j-api-1.7.30.jar
23/11/30 15:03:47 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:47 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/slf4j-log4j12-1.7.30.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/slf4j-log4j12-1.7.30.jar
23/11/30 15:03:47 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:47 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/snappy-java-1.1.8.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/snappy-java-1.1.8.2.jar
23/11/30 15:03:47 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:47 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-catalyst_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-catalyst_2.12-3.1.2.jar
23/11/30 15:03:47 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:48 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-core_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-core_2.12-3.1.2.jar
23/11/30 15:03:48 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:48 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-graphx_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-graphx_2.12-3.1.2.jar
23/11/30 15:03:48 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:48 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-hive-thriftserver_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-hive-thriftserver_2.12-3.1.2.jar
23/11/30 15:03:48 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:48 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-hive_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-hive_2.12-3.1.2.jar
23/11/30 15:03:48 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:48 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-kvstore_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-kvstore_2.12-3.1.2.jar
23/11/30 15:03:48 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:48 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-launcher_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-launcher_2.12-3.1.2.jar
23/11/30 15:03:48 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:48 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-mllib-local_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-mllib-local_2.12-3.1.2.jar
23/11/30 15:03:48 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:48 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-mllib_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-mllib_2.12-3.1.2.jar
23/11/30 15:03:48 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:48 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-network-common_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-network-common_2.12-3.1.2.jar
23/11/30 15:03:48 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:49 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-network-shuffle_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-network-shuffle_2.12-3.1.2.jar
23/11/30 15:03:49 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:49 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-repl_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-repl_2.12-3.1.2.jar
23/11/30 15:03:49 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:49 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-sketch_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-sketch_2.12-3.1.2.jar
23/11/30 15:03:49 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:49 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-sql_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-sql_2.12-3.1.2.jar
23/11/30 15:03:49 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:49 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-streaming_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-streaming_2.12-3.1.2.jar
23/11/30 15:03:49 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:49 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-tags_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-tags_2.12-3.1.2.jar
23/11/30 15:03:49 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:49 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-unsafe_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-unsafe_2.12-3.1.2.jar
23/11/30 15:03:49 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:49 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spark-yarn_2.12-3.1.2.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spark-yarn_2.12-3.1.2.jar
23/11/30 15:03:49 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:49 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spire-macros_2.12-0.17.0-M1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spire-macros_2.12-0.17.0-M1.jar
23/11/30 15:03:49 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:49 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spire-platform_2.12-0.17.0-M1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spire-platform_2.12-0.17.0-M1.jar
23/11/30 15:03:49 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:49 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spire-util_2.12-0.17.0-M1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spire-util_2.12-0.17.0-M1.jar
23/11/30 15:03:49 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:49 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/spire_2.12-0.17.0-M1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/spire_2.12-0.17.0-M1.jar
23/11/30 15:03:50 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:50 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/stax-api-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/stax-api-1.0.1.jar
23/11/30 15:03:50 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:50 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/stax2-api-3.1.4.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/stax2-api-3.1.4.jar
23/11/30 15:03:50 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:50 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/stream-2.9.6.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/stream-2.9.6.jar
23/11/30 15:03:50 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:50 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/super-csv-2.2.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/super-csv-2.2.0.jar
23/11/30 15:03:50 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:50 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/threeten-extra-1.5.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/threeten-extra-1.5.0.jar
23/11/30 15:03:50 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:50 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/token-provider-1.0.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/token-provider-1.0.1.jar
23/11/30 15:03:50 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:50 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/transaction-api-1.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/transaction-api-1.1.jar
23/11/30 15:03:50 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:50 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/univocity-parsers-2.9.1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/univocity-parsers-2.9.1.jar
23/11/30 15:03:50 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:50 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/velocity-1.5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/velocity-1.5.jar
23/11/30 15:03:50 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:50 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/woodstox-core-5.0.3.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/woodstox-core-5.0.3.jar
23/11/30 15:03:50 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:50 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/xbean-asm7-shaded-4.15.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/xbean-asm7-shaded-4.15.jar
23/11/30 15:03:50 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:50 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/xz-1.5.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/xz-1.5.jar
23/11/30 15:03:51 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:51 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/zookeeper-3.4.14.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/zookeeper-3.4.14.jar
23/11/30 15:03:51 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:51 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/zstd-jni-1.4.8-1.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/zstd-jni-1.4.8-1.jar
23/11/30 15:03:51 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:51 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-lzo-0.6.0.2.4.0.0-169.jar
23/11/30 15:03:51 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:51 INFO Client: Uploading resource file:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/hadoop-4mc-1.1.0.jar
23/11/30 15:03:51 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:51 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/json-serde-1.3-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/json-serde-1.3-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar
23/11/30 15:03:51 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:51 INFO Client: Uploading resource gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/dimlookup-hive-udf-1.0-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/dimlookup-hive-udf-1.0-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar
23/11/30 15:03:51 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:51 INFO Client: Uploading resource file:/home/vanshika.yadav/20231130-150211-tpcds-1gb-iceberg-load-benchmarks.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/20231130-150211-tpcds-1gb-iceberg-load-benchmarks.jar
23/11/30 15:03:51 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:51 INFO Client: Uploading resource file:/var/lib/fk-pf-spark3/jars/iceberg-hive-runtime-1.2.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/iceberg-hive-runtime-1.2.0.jar
23/11/30 15:03:51 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:51 WARN DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
23/11/30 15:03:51 INFO Client: Uploading resource file:/var/lib/fk-pf-spark3/jars/iceberg-spark-runtime-3.1_2.12-1.2.0.jar -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/iceberg-spark-runtime-3.1_2.12-1.2.0.jar
23/11/30 15:03:51 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:52 INFO Client: Uploading resource file:/grid/1/spark3/tmp/spark-843a47fb-2a18-4adb-a3b8-eb8af8ac4aa3/__spark_conf__4810346632097117094.zip -> hdfs://pluto/user/vanshika.yadav/.sparkStaging/application_1699449121496_0373/__spark_conf__.zip
23/11/30 15:03:52 INFO SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/30 15:03:52 INFO SecurityManager: Changing view acls to: vanshika.yadav
23/11/30 15:03:52 INFO SecurityManager: Changing modify acls to: vanshika.yadav
23/11/30 15:03:52 INFO SecurityManager: Changing view acls groups to: 
23/11/30 15:03:52 INFO SecurityManager: Changing modify acls groups to: 
23/11/30 15:03:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(vanshika.yadav); groups with view permissions: Set(); users  with modify permissions: Set(vanshika.yadav); groups with modify permissions: Set()
23/11/30 15:03:52 INFO Client: Submitting application application_1699449121496_0373 to ResourceManager
23/11/30 15:03:52 INFO YarnClientImpl: Submitted application application_1699449121496_0373
23/11/30 15:03:53 INFO Client: Application report for application_1699449121496_0373 (state: ACCEPTED)
23/11/30 15:03:53 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: de_adhoc
	 start time: 1701336832270
	 final status: UNDEFINED
	 tracking URL: http://fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal:8088/proxy/application_1699449121496_0373/
	 user: vanshika.yadav
23/11/30 15:03:54 INFO Client: Application report for application_1699449121496_0373 (state: ACCEPTED)
23/11/30 15:03:55 INFO Client: Application report for application_1699449121496_0373 (state: ACCEPTED)
23/11/30 15:03:56 INFO Client: Application report for application_1699449121496_0373 (state: ACCEPTED)
23/11/30 15:03:57 INFO Client: Application report for application_1699449121496_0373 (state: ACCEPTED)
23/11/30 15:03:58 INFO Client: Application report for application_1699449121496_0373 (state: ACCEPTED)
23/11/30 15:03:59 INFO Client: Application report for application_1699449121496_0373 (state: ACCEPTED)
23/11/30 15:04:00 INFO Client: Application report for application_1699449121496_0373 (state: ACCEPTED)
23/11/30 15:04:01 INFO Client: Application report for application_1699449121496_0373 (state: ACCEPTED)
23/11/30 15:04:02 INFO Client: Application report for application_1699449121496_0373 (state: ACCEPTED)
23/11/30 15:04:03 INFO Client: Application report for application_1699449121496_0373 (state: ACCEPTED)
23/11/30 15:04:04 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal,fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal, PROXY_URI_BASES -> http://fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal:8088/proxy/application_1699449121496_0373,http://fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal:8088/proxy/application_1699449121496_0373, RM_HA_URLS -> fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal:8088,fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal:8088), /proxy/application_1699449121496_0373
23/11/30 15:04:04 INFO Client: Application report for application_1699449121496_0373 (state: RUNNING)
23/11/30 15:04:04 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.116.4.58
	 ApplicationMaster RPC port: -1
	 queue: de_adhoc
	 start time: 1701336832270
	 final status: UNDEFINED
	 tracking URL: http://fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal:8088/proxy/application_1699449121496_0373/
	 user: vanshika.yadav
23/11/30 15:04:04 INFO YarnClientSchedulerBackend: Application application_1699449121496_0373 has started running.
23/11/30 15:04:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42453.
23/11/30 15:04:04 INFO NettyBlockTransferService: Server created on fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:42453
23/11/30 15:04:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/30 15:04:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal, 42453, None)
23/11/30 15:04:04 INFO BlockManagerMasterEndpoint: Registering block manager fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:42453 with 2.5 GiB RAM, BlockManagerId(driver, fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal, 42453, None)
23/11/30 15:04:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal, 42453, None)
23/11/30 15:04:04 INFO BlockManager: external shuffle service port = 7337
23/11/30 15:04:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal, 42453, None)
23/11/30 15:04:04 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
23/11/30 15:04:04 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5a07088b{/metrics/json,null,AVAILABLE,@Spark}
23/11/30 15:04:04 INFO GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
23/11/30 15:04:04 INFO SingleEventLogFileWriter: Logging events to gs://fks-fdp-infra-job-history/pluto/spark-job-history/application_1699449121496_0373.inprogress
23/11/30 15:04:05 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
23/11/30 15:04:05 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
23/11/30 15:04:05 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
23/11/30 15:04:05 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
23/11/30 15:04:05 INFO Main: Created Spark session with Hive support
Spark context Web UI available at http://fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:4046
Spark context available as 'sc' (master = yarn, app id = application_1699449121496_0373).
Spark session available as 'spark'.
20231130-150211-tpcds-1gb-iceberg-load_shell_init.scala:23: warning: This catches all Throwables. If this is really intended, use `case t : Throwable` to clear this warning.
try { benchmark.TPCDSDataLoad.main(Array[String]("--format", "ICEBERG", "--scale-in-gb", "1", "--exclude-nulls", "True", "--benchmark-path", "gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking", "--source-path", "gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb")) } catch { case t => println(t); println("FAILED"); System.exit(1) } ;;
                                                                                                                                                                                                                                                                                                                                                                        ^
2023-11-30T15:04:07.373 ================================================================================
2023-11-30T15:04:07.373 ================================================================================
23/11/30 15:04:07 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.
23/11/30 15:04:07 WARN SparkConf: The configuration key 'spark.yarn.driver.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.driver.memoryOverhead' instead.
23/11/30 15:04:07 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('gs://stage-hive-metastore-pluto/apps/hive/warehouse').
23/11/30 15:04:07 INFO SharedState: Warehouse path is 'gs://stage-hive-metastore-pluto/apps/hive/warehouse'.
23/11/30 15:04:07 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
23/11/30 15:04:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@562b6fb0{/SQL,null,AVAILABLE,@Spark}
23/11/30 15:04:07 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
23/11/30 15:04:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2cf22f15{/SQL/json,null,AVAILABLE,@Spark}
23/11/30 15:04:07 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
23/11/30 15:04:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7550b03f{/SQL/execution,null,AVAILABLE,@Spark}
23/11/30 15:04:07 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
23/11/30 15:04:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@68332e0e{/SQL/execution/json,null,AVAILABLE,@Spark}
23/11/30 15:04:07 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
23/11/30 15:04:07 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@30cfb802{/static/sql,null,AVAILABLE,@Spark}
23/11/30 15:04:07 WARN SparkSession$Builder: Using an existing SparkSession; some spark core configurations may not take effect.
2023-11-30T15:04:07.984 Spark started with configuration:
	spark.app.id: application_1699449121496_0373
	spark.app.name: Spark shell
	spark.app.startTime: 1701336803630
	spark.benchmarkId: 20231130-150211-tpcds-1gb-iceberg-load
	spark.cleaner.ttl: 86400
	spark.delta.logStore.gs.impl: io.delta.storage.GCSLogStore
	spark.driver.appUIAddress: http://fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:4046
	spark.driver.cores: 1
	spark.driver.extraClassPath: gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar
	spark.driver.extraJavaOptions: -Denv=prod -Dcom.sun.management.jmxremote.port=0 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=0 -XX:MaxDirectMemorySize=800M -XX:MaxMetaspaceSize=256M -XX:CompressedClassSpaceSize=100M -XX:+UnlockDiagnosticVMOptions -Djob.numOfRePartitions=30
	spark.driver.extraLibraryPath: /usr/flipkart/3.2.1-1/hadoop-hdfs/lib:/usr/flipkart/3.2.1-1/hadoop-hdfs/lib/native:/usr/lib/hadoop/lib:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/lib/x86_64-linux-gnu
	spark.driver.host: fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal
	spark.driver.memory: 5120m
	spark.driver.port: 40443
	spark.dynamicAllocation.enabled: true
	spark.dynamicAllocation.executorIdleTimeout: 60s
	spark.dynamicAllocation.maxExecutors: 200
	spark.eventLog.dir: gs://fks-fdp-infra-job-history/pluto/spark-job-history
	spark.eventLog.enabled: true
	spark.executor.cores: 1
	spark.executor.extraClassPath: gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar
	spark.executor.extraJavaOptions: -XX:MaxDirectMemorySize=8096m -Dio.netty.maxDirectMemory=8096m -XX:+UseG1GC -XX:ConcGCThreads=2
	spark.executor.extraLibraryPath: /usr/flipkart/3.2.1-1/hadoop-hdfs/lib:/usr/flipkart/3.2.1-1/hadoop-hdfs/lib/native:/usr/lib/hadoop/lib:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/lib/x86_64-linux-gnu
	spark.executor.id: driver
	spark.executor.memory: 10240m
	spark.fdp.orgqueue.cache.expire: 300
	spark.fdp.orgqueue.cache.size: 100
	spark.fdp.orgqueue.defaultQueue: adhoc
	spark.fdp.orgqueue.gringotts.clientId: QAAS
	spark.fdp.orgqueue.gringotts.clientSecret: 423de2b0-cc97-439d-a3f9-673e76d7bbea
	spark.fdp.orgqueue.gringotts.url: http://10.47.6.66/billingOrg/user
	spark.fdp.orgqueue.ironbank.url: http://10.47.4.16:/queue/
	spark.fdp.orgqueue.queueNotFound.errorMessage: Queue mapping not found
	spark.fdp.orgqueue.validInitiators: BADGER,QAAS
	spark.hadoop.fs.s3.useRequesterPaysHeader: true
	spark.hadoop.yarn.timeline-service.enabled: false
	spark.history.fs.cleaner.interval: 1d
	spark.history.fs.cleaner.maxAge: 60d
	spark.history.fs.logDirectory: gs://fks-fdp-infra-job-history/pluto/spark-job-history
	spark.history.provider: org.apache.spark.deploy.history.FsHistoryProvider
	spark.home: /var/lib/fk-pf-spark3
	spark.jars: 
	spark.master: yarn
	spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS: fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal,fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal
	spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES: http://fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal:8088/proxy/application_1699449121496_0373,http://fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal:8088/proxy/application_1699449121496_0373
	spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.RM_HA_URLS: fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal:8088,fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal:8088
	spark.queue.enforcer.class: com.flipkart.fdp.orgqueue.OrgQueueEnforcerForInitiator
	spark.repl.class.outputDir: /grid/1/spark3/tmp/spark-843a47fb-2a18-4adb-a3b8-eb8af8ac4aa3/repl-1cbc400f-d983-4d3b-a6b6-adcf6b33c856
	spark.repl.class.uri: spark://fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:40443/classes
	spark.repl.local.jars: file:///home/vanshika.yadav/20231130-150211-tpcds-1gb-iceberg-load-benchmarks.jar,file:///var/lib/fk-pf-spark3/jars/iceberg-hive-runtime-1.2.0.jar,file:///var/lib/fk-pf-spark3/jars/iceberg-spark-runtime-3.1_2.12-1.2.0.jar
	spark.shuffle.service.enabled: true
	spark.shuffle.useOldFetchProtocol: true
	spark.sql.catalog.hive_pluto: org.apache.iceberg.spark.SparkCatalog
	spark.sql.catalog.hive_pluto.type: hive
	spark.sql.catalog.hive_pluto.uri: thrift://10.116.17.2:9083
	spark.sql.catalogImplementation: hive
	spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
	spark.sql.sources.partitionOverwriteMode: dynamic
	spark.sql.warehouse.dir: gs://stage-hive-metastore-pluto/apps/hive/warehouse
	spark.streaming.concurrentJobs: 4
	spark.submit.deployMode: client
	spark.submit.pyFiles: 
	spark.ui.filters: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	spark.ui.proxyBase: 
	spark.ui.showConsoleProgress: true
	spark.yarn.dist.jars: gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar,file:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar,gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/json-serde-1.3-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/dimlookup-hive-udf-1.0-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,file:///home/vanshika.yadav/20231130-150211-tpcds-1gb-iceberg-load-benchmarks.jar,file:///var/lib/fk-pf-spark3/jars/iceberg-hive-runtime-1.2.0.jar,file:///var/lib/fk-pf-spark3/jars/iceberg-spark-runtime-3.1_2.12-1.2.0.jar
	spark.yarn.driver.memoryOverhead: 4096
	spark.yarn.executor.memoryOverhead: 4096
	spark.yarn.historyServer.address: http://fks-fdp-galaxy-pluto-hs-0001-m.c.fks-fdp-galaxy.internal:18080
	spark.yarn.jars: gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3/jars/*
	spark.yarn.queue: de_adhoc
	spark.yarn.report.interval: 60s
	spark.yarn.secondary.jars: hadoop-lzo-0.6.0.2.4.0.0-169.jar,hadoop-4mc-1.1.0.jar,json-serde-1.3-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,dimlookup-hive-udf-1.0-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,20231130-150211-tpcds-1gb-iceberg-load-benchmarks.jar,iceberg-hive-runtime-1.2.0.jar,iceberg-spark-runtime-3.1_2.12-1.2.0.jar

2023-11-30T15:04:07.988 ================================================================================
2023-11-30T15:04:07.988 START: drop-database
2023-11-30T15:04:07.989 SQL: DROP DATABASE IF EXISTS tpcds_sf1_ICEBERG CASCADE
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.exec.orc.default.compress does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.vectorized.complex.types.enabled does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.exec.orc.compression.strategy does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.enforce.sorting does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.metastore.client.scheme.handlers does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.auto.convert.sortmerge.join.noconditionaltask does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.vectorized.execution.ptf.enabled does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.support.sql11.reserved.keywords does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.vectorized.groupby.complex.types.enabled does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.heapsize does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.enforce.bucketing does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.fileformat.managed does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.optimize.mapjoin.mapreduce does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.exec.orc.default.stripe.size does not exist
23/11/30 15:04:08 WARN HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
23/11/30 15:04:08 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic
2023-11-30T15:04:11.065 END took 3075 ms: drop-database
2023-11-30T15:04:11.066 ================================================================================
2023-11-30T15:04:11.066 ================================================================================
2023-11-30T15:04:11.066 START: create-database
2023-11-30T15:04:11.066 SQL: CREATE DATABASE IF NOT EXISTS tpcds_sf1_ICEBERG
2023-11-30T15:04:11.304 END took 237 ms: create-database
2023-11-30T15:04:11.304 ================================================================================
2023-11-30T15:04:11.307 Generating call_center at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/call_center
2023-11-30T15:04:11.307 ================================================================================
2023-11-30T15:04:11.307 START: drop-table-call_center
2023-11-30T15:04:11.307 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`call_center`
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.exec.orc.default.compress does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.vectorized.complex.types.enabled does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.exec.orc.compression.strategy does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.enforce.sorting does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.metastore.client.scheme.handlers does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.auto.convert.sortmerge.join.noconditionaltask does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.vectorized.execution.ptf.enabled does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.support.sql11.reserved.keywords does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.vectorized.groupby.complex.types.enabled does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.heapsize does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.enforce.bucketing does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.fileformat.managed does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.optimize.mapjoin.mapreduce does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.exec.orc.default.stripe.size does not exist
23/11/30 15:04:11 WARN HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
2023-11-30T15:04:11.657 END took 349 ms: drop-table-call_center
2023-11-30T15:04:11.657 ================================================================================
2023-11-30T15:04:11.657 ================================================================================
2023-11-30T15:04:11.657 START: create-table-call_center
2023-11-30T15:04:11.657 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`call_center` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/call_center/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/call_center_1gb_parquet`  
[Stage 0:>                                                          (0 + 0) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                23/11/30 15:04:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 1:>                                                          (0 + 1) / 1]                                                                                2023-11-30T15:04:30.466 END took 18809 ms: create-table-call_center
2023-11-30T15:04:30.466 ================================================================================
2023-11-30T15:04:32.083 Generating catalog_page at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/catalog_page
2023-11-30T15:04:32.084 ================================================================================
2023-11-30T15:04:32.084 START: drop-table-catalog_page
2023-11-30T15:04:32.084 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`catalog_page`
2023-11-30T15:04:32.101 END took 16 ms: drop-table-catalog_page
2023-11-30T15:04:32.101 ================================================================================
2023-11-30T15:04:32.101 ================================================================================
2023-11-30T15:04:32.101 START: create-table-catalog_page
2023-11-30T15:04:32.101 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`catalog_page` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/catalog_page/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/catalog_page_1gb_parquet`  
2023-11-30T15:04:33.646 END took 1545 ms: create-table-catalog_page
2023-11-30T15:04:33.646 ================================================================================
2023-11-30T15:04:34.439 Generating catalog_returns at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/catalog_returns
2023-11-30T15:04:34.439 ================================================================================
2023-11-30T15:04:34.439 START: drop-table-catalog_returns
2023-11-30T15:04:34.439 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`catalog_returns`
2023-11-30T15:04:34.452 END took 12 ms: drop-table-catalog_returns
2023-11-30T15:04:34.452 ================================================================================
2023-11-30T15:04:34.452 ================================================================================
2023-11-30T15:04:34.452 START: create-table-catalog_returns
2023-11-30T15:04:34.452 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`catalog_returns` USING ICEBERG PARTITIONED BY (cr_returned_date_sk)  LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/catalog_returns/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/catalog_returns_1gb_parquet` WHERE cr_returned_date_sk IS NOT NULL 
[Stage 13:>                                                         (0 + 1) / 2][Stage 13:>                                                         (0 + 2) / 2][Stage 13:=============================>                            (1 + 1) / 2][Stage 13:=============================>                            (1 + 1) / 2][Stage 13:=============================>                            (1 + 1) / 2][Stage 13:=============================>                            (1 + 1) / 2]                                                                                2023-11-30T15:08:11.135 END took 216682 ms: create-table-catalog_returns
2023-11-30T15:08:11.135 ================================================================================
[Stage 17:>                                                        (0 + 1) / 65][Stage 17:>                                                        (1 + 1) / 65][Stage 17:=>                                                       (2 + 1) / 65][Stage 17:==>                                                      (3 + 1) / 65][Stage 17:===>                                                     (4 + 1) / 65][Stage 17:====>                                                    (5 + 1) / 65][Stage 17:====>                                                    (5 + 5) / 65][Stage 17:====>                                                    (5 + 6) / 65][Stage 17:=====>                                                   (6 + 6) / 65][Stage 17:=====>                                                   (6 + 8) / 65][Stage 17:=====>                                                  (6 + 16) / 65][Stage 17:=====>                                                  (6 + 17) / 65][Stage 17:=====>                                                  (6 + 22) / 65][Stage 17:=====>                                                  (6 + 24) / 65][Stage 17:=====>                                                  (6 + 31) / 65][Stage 17:======>                                                 (7 + 33) / 65][Stage 17:======>                                                 (8 + 33) / 65][Stage 17:=======>                                                (9 + 33) / 65][Stage 17:========>                                              (10 + 33) / 65][Stage 17:===========>                                           (13 + 33) / 65][Stage 17:===========>                                           (14 + 38) / 65][Stage 17:==============>                                        (17 + 48) / 65][Stage 17:==================>                                    (22 + 43) / 65][Stage 17:=========================>                             (30 + 35) / 65][Stage 17:===========================>                           (32 + 33) / 65][Stage 17:============================>                          (34 + 31) / 65][Stage 17:=============================>                         (35 + 30) / 65][Stage 17:=================================>                     (40 + 25) / 65][Stage 17:=======================================>               (47 + 18) / 65][Stage 17:==========================================>            (50 + 15) / 65][Stage 17:===========================================>           (51 + 14) / 65][Stage 17:=============================================>         (54 + 11) / 65][Stage 17:================================================>       (56 + 9) / 65][Stage 17:======================================================> (63 + 2) / 65][Stage 18:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:08:34.225 Generating catalog_sales at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/catalog_sales
2023-11-30T15:08:34.226 ================================================================================
2023-11-30T15:08:34.226 START: drop-table-catalog_sales
2023-11-30T15:08:34.226 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`catalog_sales`
2023-11-30T15:08:34.238 END took 12 ms: drop-table-catalog_sales
2023-11-30T15:08:34.238 ================================================================================
2023-11-30T15:08:34.238 ================================================================================
2023-11-30T15:08:34.238 START: create-table-catalog_sales
2023-11-30T15:08:34.238 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`catalog_sales` USING ICEBERG PARTITIONED BY (cs_sold_date_sk)  LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/catalog_sales/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/catalog_sales_1gb_parquet` WHERE cs_sold_date_sk IS NOT NULL 
[Stage 20:>                                                       (0 + 29) / 29][Stage 20:=======>                                                (4 + 25) / 29]23/11/30 15:08:35 WARN TaskSetManager: Lost task 20.0 in stage 20.0 (TID 106) (pluto-mig-adhoc-c-cimage-c69c6e1d-wkxs.c.fks-fdp-galaxy.internal executor 18): java.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonParser.getReadCapabilities()Lcom/fasterxml/jackson/core/util/JacksonFeatureSet;
	at com.fasterxml.jackson.databind.DeserializationContext.<init>(DeserializationContext.java:213)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.<init>(DefaultDeserializationContext.java:50)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.<init>(DefaultDeserializationContext.java:392)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.createInstance(DefaultDeserializationContext.java:414)
	at com.fasterxml.jackson.databind.ObjectReader.createDeserializationContext(ObjectReader.java:2335)
	at com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:2094)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1546)
	at org.json4s.jackson.JsonMethods.parse(JsonMethods.scala:25)
	at org.json4s.jackson.JsonMethods.parse$(JsonMethods.scala:19)
	at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:55)
	at org.apache.spark.sql.types.DataType$.fromJson(DataType.scala:169)
	at org.apache.spark.sql.types.StructType$.$anonfun$fromString$1(StructType.scala:524)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.types.StructType$.fromString(StructType.scala:524)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport.init(ParquetReadSupport.scala:82)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:141)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:153)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:329)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

[Stage 20:========================================>               (21 + 8) / 29][Stage 20:==========================================>             (22 + 7) / 29][Stage 20:============================================>           (23 + 6) / 29][Stage 20:==================================================>     (26 + 3) / 29][Stage 20:====================================================>   (27 + 2) / 29][Stage 20:======================================================> (28 + 1) / 29][Stage 20:======================================================> (28 + 1) / 29][Stage 20:======================================================> (28 + 1) / 29][Stage 20:======================================================> (28 + 1) / 29]                                                                                2023-11-30T15:11:50.392 END took 196154 ms: create-table-catalog_sales
2023-11-30T15:11:50.393 ================================================================================
[Stage 24:>                                                        (0 + 1) / 58][Stage 24:>                                                        (1 + 1) / 58][Stage 24:=>                                                       (2 + 1) / 58][Stage 24:==>                                                      (3 + 1) / 58][Stage 24:==>                                                      (3 + 2) / 58][Stage 24:===>                                                     (4 + 2) / 58][Stage 24:===>                                                     (4 + 3) / 58][Stage 24:===>                                                     (4 + 6) / 58][Stage 24:====>                                                    (5 + 6) / 58][Stage 24:====>                                                   (5 + 11) / 58][Stage 24:====>                                                   (5 + 14) / 58][Stage 24:====>                                                   (5 + 15) / 58][Stage 24:====>                                                   (5 + 21) / 58][Stage 24:=====>                                                  (6 + 24) / 58][Stage 24:======>                                                 (7 + 32) / 58][Stage 24:======>                                                 (7 + 37) / 58][Stage 24:======>                                                 (7 + 38) / 58][Stage 24:======>                                                 (7 + 39) / 58][Stage 24:=======>                                                (8 + 39) / 58][Stage 24:========>                                               (9 + 39) / 58][Stage 24:=========>                                             (10 + 39) / 58][Stage 24:===========>                                           (12 + 39) / 58][Stage 24:============>                                          (13 + 39) / 58][Stage 24:=============>                                         (14 + 39) / 58][Stage 24:=============>                                         (14 + 40) / 58][Stage 24:==================>                                    (19 + 39) / 58][Stage 24:====================>                                  (22 + 36) / 58][Stage 24:============================>                          (30 + 28) / 58][Stage 24:=============================>                         (31 + 27) / 58][Stage 24:================================>                      (34 + 24) / 58][Stage 24:====================================>                  (39 + 19) / 58][Stage 24:===========================================>           (46 + 12) / 58][Stage 24:====================================================>   (54 + 4) / 58][Stage 24:=====================================================>  (55 + 3) / 58][Stage 24:======================================================> (56 + 2) / 58][Stage 24:=======================================================>(57 + 1) / 58][Stage 25:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:12:10.729 Generating customer at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/customer
2023-11-30T15:12:10.729 ================================================================================
2023-11-30T15:12:10.729 START: drop-table-customer
2023-11-30T15:12:10.729 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`customer`
2023-11-30T15:12:10.740 END took 11 ms: drop-table-customer
2023-11-30T15:12:10.740 ================================================================================
2023-11-30T15:12:10.740 ================================================================================
2023-11-30T15:12:10.740 START: create-table-customer
2023-11-30T15:12:10.740 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`customer` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/customer/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/customer_1gb_parquet`  
[Stage 27:>                                                         (0 + 2) / 2]23/11/30 15:12:12 WARN TaskSetManager: Lost task 1.0 in stage 27.0 (TID 184) (pluto-mig-adhoc-c-cimage-c69c6e1d-wkxs.c.fks-fdp-galaxy.internal executor 67): java.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonParser.getReadCapabilities()Lcom/fasterxml/jackson/core/util/JacksonFeatureSet;
	at com.fasterxml.jackson.databind.DeserializationContext.<init>(DeserializationContext.java:213)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.<init>(DefaultDeserializationContext.java:50)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.<init>(DefaultDeserializationContext.java:392)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.createInstance(DefaultDeserializationContext.java:414)
	at com.fasterxml.jackson.databind.ObjectReader.createDeserializationContext(ObjectReader.java:2335)
	at com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:2094)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1546)
	at org.json4s.jackson.JsonMethods.parse(JsonMethods.scala:25)
	at org.json4s.jackson.JsonMethods.parse$(JsonMethods.scala:19)
	at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:55)
	at org.apache.spark.sql.types.DataType$.fromJson(DataType.scala:169)
	at org.apache.spark.sql.types.StructType$.$anonfun$fromString$1(StructType.scala:524)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.types.StructType$.fromString(StructType.scala:524)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport.init(ParquetReadSupport.scala:82)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:141)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:153)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:329)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

[Stage 27:=============================>                            (1 + 1) / 2]                                                                                2023-11-30T15:12:16.194 END took 5453 ms: create-table-customer
2023-11-30T15:12:16.194 ================================================================================
[Stage 31:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:12:20.082 Generating customer_address at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/customer_address
2023-11-30T15:12:20.083 ================================================================================
2023-11-30T15:12:20.083 START: drop-table-customer_address
2023-11-30T15:12:20.083 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`customer_address`
2023-11-30T15:12:20.093 END took 10 ms: drop-table-customer_address
2023-11-30T15:12:20.093 ================================================================================
2023-11-30T15:12:20.093 ================================================================================
2023-11-30T15:12:20.093 START: create-table-customer_address
2023-11-30T15:12:20.093 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`customer_address` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/customer_address/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/customer_address_1gb_parquet`  
[Stage 33:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:12:22.592 END took 2498 ms: create-table-customer_address
2023-11-30T15:12:22.592 ================================================================================
[Stage 37:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:12:26.755 Generating customer_demographics at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/customer_demographics
2023-11-30T15:12:26.755 ================================================================================
2023-11-30T15:12:26.755 START: drop-table-customer_demographics
2023-11-30T15:12:26.755 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`customer_demographics`
2023-11-30T15:12:26.765 END took 9 ms: drop-table-customer_demographics
2023-11-30T15:12:26.765 ================================================================================
2023-11-30T15:12:26.765 ================================================================================
2023-11-30T15:12:26.765 START: create-table-customer_demographics
2023-11-30T15:12:26.765 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`customer_demographics` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/customer_demographics/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/customer_demographics_1gb_parquet`  
[Stage 39:===================>                                      (1 + 2) / 3][Stage 39:======================================>                   (2 + 1) / 3]                                                                                2023-11-30T15:12:31.806 END took 5041 ms: create-table-customer_demographics
2023-11-30T15:12:31.806 ================================================================================
[Stage 41:===================>                                      (1 + 2) / 3][Stage 41:======================================>                   (2 + 1) / 3]                                                                                2023-11-30T15:12:35.145 Generating date_dim at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/date_dim
2023-11-30T15:12:35.145 ================================================================================
2023-11-30T15:12:35.145 START: drop-table-date_dim
2023-11-30T15:12:35.145 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`date_dim`
2023-11-30T15:12:35.156 END took 11 ms: drop-table-date_dim
2023-11-30T15:12:35.156 ================================================================================
2023-11-30T15:12:35.156 ================================================================================
2023-11-30T15:12:35.157 START: create-table-date_dim
2023-11-30T15:12:35.157 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`date_dim` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/date_dim/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/date_dim_1gb_parquet`  
[Stage 45:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:12:40.745 END took 5588 ms: create-table-date_dim
2023-11-30T15:12:40.745 ================================================================================
[Stage 48:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:12:42.715 Generating household_demographics at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/household_demographics
2023-11-30T15:12:42.715 ================================================================================
2023-11-30T15:12:42.715 START: drop-table-household_demographics
2023-11-30T15:12:42.715 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`household_demographics`
2023-11-30T15:12:42.726 END took 10 ms: drop-table-household_demographics
2023-11-30T15:12:42.726 ================================================================================
2023-11-30T15:12:42.726 ================================================================================
2023-11-30T15:12:42.726 START: create-table-household_demographics
2023-11-30T15:12:42.726 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`household_demographics` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/household_demographics/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/household_demographics_1gb_parquet`  
[Stage 51:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:12:45.002 END took 2275 ms: create-table-household_demographics
2023-11-30T15:12:45.002 ================================================================================
[Stage 55:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:12:47.744 Generating income_band at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/income_band
2023-11-30T15:12:47.745 ================================================================================
2023-11-30T15:12:47.745 START: drop-table-income_band
2023-11-30T15:12:47.745 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`income_band`
2023-11-30T15:12:47.754 END took 9 ms: drop-table-income_band
2023-11-30T15:12:47.754 ================================================================================
2023-11-30T15:12:47.754 ================================================================================
2023-11-30T15:12:47.754 START: create-table-income_band
2023-11-30T15:12:47.754 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`income_band` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/income_band/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/income_band_1gb_parquet`  
[Stage 57:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:12:51.947 END took 4192 ms: create-table-income_band
2023-11-30T15:12:51.947 ================================================================================
[Stage 59:>                                                         (0 + 1) / 1]                                                                                [Stage 61:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:12:55.740 Generating inventory at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/inventory
2023-11-30T15:12:55.740 ================================================================================
2023-11-30T15:12:55.740 START: drop-table-inventory
2023-11-30T15:12:55.740 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`inventory`
2023-11-30T15:12:55.750 END took 9 ms: drop-table-inventory
2023-11-30T15:12:55.750 ================================================================================
2023-11-30T15:12:55.750 ================================================================================
2023-11-30T15:12:55.750 START: create-table-inventory
2023-11-30T15:12:55.750 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`inventory` USING ICEBERG PARTITIONED BY (inv_date_sk)  LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/inventory/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/inventory_1gb_parquet` WHERE inv_date_sk IS NOT NULL 
[Stage 63:================================>                         (5 + 4) / 9][Stage 63:=============================================>            (7 + 2) / 9][Stage 63:===================================================>      (8 + 1) / 9]                                                                                2023-11-30T15:13:30.614 END took 34863 ms: create-table-inventory
2023-11-30T15:13:30.614 ================================================================================
23/11/30 15:13:31 WARN TaskSetManager: Lost task 0.0 in stage 65.0 (TID 236) (pluto-mig-adhoc-c-cimage-c69c6e1d-p7lh.c.fks-fdp-galaxy.internal executor 64): java.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonParser.getReadCapabilities()Lcom/fasterxml/jackson/core/util/JacksonFeatureSet;
	at com.fasterxml.jackson.databind.DeserializationContext.<init>(DeserializationContext.java:213)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.<init>(DefaultDeserializationContext.java:50)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.<init>(DefaultDeserializationContext.java:392)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.createInstance(DefaultDeserializationContext.java:414)
	at com.fasterxml.jackson.databind.ObjectReader.createDeserializationContext(ObjectReader.java:2335)
	at com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:2094)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1546)
	at org.json4s.jackson.JsonMethods.parse(JsonMethods.scala:25)
	at org.json4s.jackson.JsonMethods.parse$(JsonMethods.scala:19)
	at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:55)
	at org.apache.spark.sql.types.DataType$.fromJson(DataType.scala:169)
	at org.apache.spark.sql.types.StructType$.$anonfun$fromString$1(StructType.scala:524)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.types.StructType$.fromString(StructType.scala:524)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport.init(ParquetReadSupport.scala:82)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:141)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:153)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:329)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

[Stage 65:===================================================>      (8 + 1) / 9]                                                                                [Stage 67:======>                                                   (1 + 8) / 9][Stage 67:======================================>                   (6 + 3) / 9]                                                                                2023-11-30T15:13:33.918 Generating item at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/item
2023-11-30T15:13:33.918 ================================================================================
2023-11-30T15:13:33.919 START: drop-table-item
2023-11-30T15:13:33.919 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`item`
2023-11-30T15:13:33.928 END took 9 ms: drop-table-item
2023-11-30T15:13:33.928 ================================================================================
2023-11-30T15:13:33.928 ================================================================================
2023-11-30T15:13:33.928 START: create-table-item
2023-11-30T15:13:33.928 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`item` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/item/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/item_1gb_parquet`  
[Stage 70:>                                                         (0 + 1) / 1]23/11/30 15:13:35 WARN ThrowableSerializationWrapper: Task exception could not be deserialized
java.io.InvalidClassException: com.fasterxml.jackson.databind.JsonMappingException; local class incompatible: stream classdesc serialVersionUID = 3, local class serialVersionUID = 1
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:687)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1883)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1749)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2040)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.ThrowableSerializationWrapper.readObject(TaskEndReason.scala:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1158)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2176)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)
	at org.apache.spark.scheduler.TaskResultGetter.$anonfun$enqueueFailedTask$2(TaskResultGetter.scala:141)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)
	at org.apache.spark.scheduler.TaskResultGetter.$anonfun$enqueueFailedTask$1(TaskResultGetter.scala:137)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
23/11/30 15:13:35 WARN TaskSetManager: Lost task 0.0 in stage 70.0 (TID 258) (pluto-mig-adhoc-c-cimage-c69c6e1d-wclk.c.fks-fdp-galaxy.internal executor 100): UnknownReason
23/11/30 15:13:36 WARN ThrowableSerializationWrapper: Task exception could not be deserialized
java.io.InvalidClassException: com.fasterxml.jackson.databind.JsonMappingException; local class incompatible: stream classdesc serialVersionUID = 3, local class serialVersionUID = 1
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:687)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1883)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1749)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2040)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.ThrowableSerializationWrapper.readObject(TaskEndReason.scala:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1158)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2176)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)
	at org.apache.spark.scheduler.TaskResultGetter.$anonfun$enqueueFailedTask$2(TaskResultGetter.scala:141)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)
	at org.apache.spark.scheduler.TaskResultGetter.$anonfun$enqueueFailedTask$1(TaskResultGetter.scala:137)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
23/11/30 15:13:36 WARN TaskSetManager: Lost task 0.1 in stage 70.0 (TID 259) (pluto-mig-adhoc-c-cimage-c69c6e1d-wclk.c.fks-fdp-galaxy.internal executor 94): UnknownReason
                                                                                2023-11-30T15:13:38.395 END took 4467 ms: create-table-item
2023-11-30T15:13:38.395 ================================================================================
2023-11-30T15:13:39.193 Generating promotion at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/promotion
2023-11-30T15:13:39.193 ================================================================================
2023-11-30T15:13:39.193 START: drop-table-promotion
2023-11-30T15:13:39.193 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`promotion`
2023-11-30T15:13:39.202 END took 9 ms: drop-table-promotion
2023-11-30T15:13:39.202 ================================================================================
2023-11-30T15:13:39.202 ================================================================================
2023-11-30T15:13:39.202 START: create-table-promotion
2023-11-30T15:13:39.202 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`promotion` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/promotion/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/promotion_1gb_parquet`  
[Stage 76:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:13:41.163 END took 1960 ms: create-table-promotion
2023-11-30T15:13:41.163 ================================================================================
2023-11-30T15:13:41.951 Generating reason at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/reason
2023-11-30T15:13:41.951 ================================================================================
2023-11-30T15:13:41.951 START: drop-table-reason
2023-11-30T15:13:41.951 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`reason`
2023-11-30T15:13:41.960 END took 9 ms: drop-table-reason
2023-11-30T15:13:41.960 ================================================================================
2023-11-30T15:13:41.960 ================================================================================
2023-11-30T15:13:41.960 START: create-table-reason
2023-11-30T15:13:41.960 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`reason` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/reason/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/reason_1gb_parquet`  
2023-11-30T15:13:43.211 END took 1250 ms: create-table-reason
2023-11-30T15:13:43.211 ================================================================================
2023-11-30T15:13:44.361 Generating ship_mode at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/ship_mode
2023-11-30T15:13:44.361 ================================================================================
2023-11-30T15:13:44.361 START: drop-table-ship_mode
2023-11-30T15:13:44.361 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`ship_mode`
2023-11-30T15:13:44.370 END took 8 ms: drop-table-ship_mode
2023-11-30T15:13:44.370 ================================================================================
2023-11-30T15:13:44.370 ================================================================================
2023-11-30T15:13:44.370 START: create-table-ship_mode
2023-11-30T15:13:44.370 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`ship_mode` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/ship_mode/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/ship_mode_1gb_parquet`  
[Stage 88:>                                                         (0 + 1) / 1]                                                                                2023-11-30T15:13:46.051 END took 1680 ms: create-table-ship_mode
2023-11-30T15:13:46.051 ================================================================================
2023-11-30T15:13:46.795 Generating store at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/store
2023-11-30T15:13:46.795 ================================================================================
2023-11-30T15:13:46.795 START: drop-table-store
2023-11-30T15:13:46.795 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`store`
2023-11-30T15:13:46.805 END took 9 ms: drop-table-store
2023-11-30T15:13:46.805 ================================================================================
2023-11-30T15:13:46.805 ================================================================================
2023-11-30T15:13:46.805 START: create-table-store
2023-11-30T15:13:46.805 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`store` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/store/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/store_1gb_parquet`  
2023-11-30T15:13:48.455 END took 1650 ms: create-table-store
2023-11-30T15:13:48.456 ================================================================================
2023-11-30T15:13:49.111 Generating store_returns at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/store_returns
2023-11-30T15:13:49.111 ================================================================================
2023-11-30T15:13:49.111 START: drop-table-store_returns
2023-11-30T15:13:49.111 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`store_returns`
2023-11-30T15:13:49.120 END took 9 ms: drop-table-store_returns
2023-11-30T15:13:49.120 ================================================================================
2023-11-30T15:13:49.120 ================================================================================
2023-11-30T15:13:49.120 START: create-table-store_returns
2023-11-30T15:13:49.120 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`store_returns` USING ICEBERG PARTITIONED BY (sr_returned_date_sk)  LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/store_returns/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/store_returns_1gb_parquet` WHERE sr_returned_date_sk IS NOT NULL 
23/11/30 15:13:49 WARN TaskSetManager: Lost task 2.0 in stage 100.0 (TID 292) (pluto-mig-adhoc-c-cimage-c69c6e1d-p7lh.c.fks-fdp-galaxy.internal executor 64): java.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonParser.getReadCapabilities()Lcom/fasterxml/jackson/core/util/JacksonFeatureSet;
	at com.fasterxml.jackson.databind.DeserializationContext.<init>(DeserializationContext.java:213)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.<init>(DefaultDeserializationContext.java:50)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.<init>(DefaultDeserializationContext.java:392)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.createInstance(DefaultDeserializationContext.java:414)
	at com.fasterxml.jackson.databind.ObjectReader.createDeserializationContext(ObjectReader.java:2335)
	at com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:2094)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1546)
	at org.json4s.jackson.JsonMethods.parse(JsonMethods.scala:25)
	at org.json4s.jackson.JsonMethods.parse$(JsonMethods.scala:19)
	at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:55)
	at org.apache.spark.sql.types.DataType$.fromJson(DataType.scala:169)
	at org.apache.spark.sql.types.StructType$.$anonfun$fromString$1(StructType.scala:524)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.types.StructType$.fromString(StructType.scala:524)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport.init(ParquetReadSupport.scala:82)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:141)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:153)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:329)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

[Stage 100:=============================================>           (4 + 1) / 5][Stage 100:=============================================>           (4 + 1) / 5][Stage 100:=============================================>           (4 + 1) / 5][Stage 100:=============================================>           (4 + 1) / 5]                                                                                2023-11-30T15:17:23.344 END took 214223 ms: create-table-store_returns
2023-11-30T15:17:23.344 ================================================================================
[Stage 104:>                                                       (0 + 1) / 63][Stage 104:>                                                       (1 + 1) / 63][Stage 104:=>                                                      (2 + 1) / 63][Stage 104:==>                                                     (3 + 1) / 63][Stage 104:===>                                                    (4 + 1) / 63][Stage 104:====>                                                   (5 + 1) / 63][Stage 104:=====>                                                  (6 + 1) / 63][Stage 104:=====>                                                  (6 + 6) / 63][Stage 104:=====>                                                 (6 + 12) / 63][Stage 104:======>                                                (7 + 18) / 63][Stage 104:======>                                                (7 + 26) / 63][Stage 104:======>                                                (7 + 29) / 63][Stage 104:======>                                                (7 + 34) / 63][Stage 104:======>                                                (7 + 37) / 63][Stage 104:======>                                                (8 + 37) / 63][Stage 104:=======>                                               (9 + 38) / 63][Stage 104:=======>                                               (9 + 46) / 63][Stage 104:=======>                                               (9 + 47) / 63][Stage 104:========>                                             (10 + 47) / 63][Stage 104:=================>                                    (20 + 43) / 63][Stage 104:===================>                                  (23 + 40) / 63][Stage 104:====================>                                 (24 + 39) / 63][Stage 104:=========================>                            (30 + 33) / 63][Stage 104:==============================>                       (35 + 28) / 63][Stage 104:==============================>                       (36 + 27) / 63][Stage 104:=================================>                    (39 + 24) / 63][Stage 104:====================================>                 (42 + 21) / 63][Stage 104:======================================>               (45 + 18) / 63][Stage 104:===========================================>          (51 + 12) / 63][Stage 104:=============================================>        (53 + 10) / 63][Stage 104:===============================================>       (54 + 9) / 63][Stage 104:================================================>      (56 + 7) / 63][Stage 104:======================================================>(62 + 1) / 63]                                                                                2023-11-30T15:17:44.065 Generating store_sales at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/store_sales
2023-11-30T15:17:44.065 ================================================================================
2023-11-30T15:17:44.065 START: drop-table-store_sales
2023-11-30T15:17:44.065 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`store_sales`
2023-11-30T15:17:44.076 END took 11 ms: drop-table-store_sales
2023-11-30T15:17:44.076 ================================================================================
2023-11-30T15:17:44.076 ================================================================================
2023-11-30T15:17:44.076 START: create-table-store_sales
2023-11-30T15:17:44.076 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`store_sales` USING ICEBERG PARTITIONED BY (ss_sold_date_sk)  LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/store_sales/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/store_sales_1gb_parquet` WHERE ss_sold_date_sk IS NOT NULL 
[Stage 107:=>                                                     (1 + 39) / 40]23/11/30 15:17:45 WARN TaskSetManager: Lost task 2.0 in stage 107.0 (TID 367) (pluto-mig-adhoc-c-cimage-c69c6e1d-t7kx.c.fks-fdp-galaxy.internal executor 143): java.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonParser.getReadCapabilities()Lcom/fasterxml/jackson/core/util/JacksonFeatureSet;
	at com.fasterxml.jackson.databind.DeserializationContext.<init>(DeserializationContext.java:213)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.<init>(DefaultDeserializationContext.java:50)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.<init>(DefaultDeserializationContext.java:392)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.createInstance(DefaultDeserializationContext.java:414)
	at com.fasterxml.jackson.databind.ObjectReader.createDeserializationContext(ObjectReader.java:2335)
	at com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:2094)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1546)
	at org.json4s.jackson.JsonMethods.parse(JsonMethods.scala:25)
	at org.json4s.jackson.JsonMethods.parse$(JsonMethods.scala:19)
	at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:55)
	at org.apache.spark.sql.types.DataType$.fromJson(DataType.scala:169)
	at org.apache.spark.sql.types.StructType$.$anonfun$fromString$1(StructType.scala:524)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.types.StructType$.fromString(StructType.scala:524)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport.init(ParquetReadSupport.scala:82)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:141)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:153)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:329)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

[Stage 107:===========>                                           (8 + 32) / 40][Stage 107:================================>                     (24 + 16) / 40][Stage 107:=================================>                    (25 + 15) / 40][Stage 107:====================================>                 (27 + 13) / 40][Stage 107:=====================================>                (28 + 12) / 40][Stage 107:=======================================>              (29 + 11) / 40][Stage 107:=============================================>         (33 + 7) / 40][Stage 107:==================================================>    (37 + 3) / 40][Stage 107:====================================================>  (38 + 2) / 40][Stage 107:====================================================>  (38 + 2) / 40][Stage 107:====================================================>  (38 + 2) / 40][Stage 107:=====================================================> (39 + 1) / 40]                                                                                2023-11-30T15:20:55.001 END took 190924 ms: create-table-store_sales
2023-11-30T15:20:55.001 ================================================================================
[Stage 111:>                                                      (0 + 2) / 114][Stage 111:>                                                      (2 + 2) / 114][Stage 111:=>                                                     (4 + 2) / 114][Stage 111:==>                                                    (6 + 2) / 114][Stage 111:==>                                                    (6 + 3) / 114][Stage 111:===>                                                   (7 + 3) / 114][Stage 111:===>                                                   (8 + 3) / 114][Stage 111:====>                                                 (10 + 3) / 114][Stage 111:====>                                                 (10 + 7) / 114][Stage 111:=====>                                                (12 + 7) / 114][Stage 111:======>                                               (13 + 8) / 114][Stage 111:======>                                              (13 + 14) / 114][Stage 111:======>                                              (15 + 19) / 114][Stage 111:======>                                              (15 + 24) / 114][Stage 111:======>                                              (15 + 34) / 114][Stage 111:======>                                              (15 + 36) / 114][Stage 111:======>                                              (15 + 41) / 114][Stage 111:======>                                              (15 + 45) / 114][Stage 111:=======>                                             (17 + 46) / 114][Stage 111:========>                                            (18 + 51) / 114][Stage 111:========>                                            (18 + 56) / 114][Stage 111:========>                                            (18 + 64) / 114][Stage 111:========>                                            (18 + 71) / 114][Stage 111:========>                                            (18 + 85) / 114][Stage 111:=========>                                           (21 + 90) / 114][Stage 111:==========>                                          (23 + 90) / 114][Stage 111:===========>                                         (24 + 90) / 114][Stage 111:===========>                                         (25 + 89) / 114][Stage 111:============>                                        (26 + 88) / 114][Stage 111:=============>                                       (30 + 84) / 114][Stage 111:==============>                                      (31 + 83) / 114][Stage 111:==============>                                      (32 + 82) / 114][Stage 111:==================>                                  (40 + 74) / 114][Stage 111:======================>                              (48 + 66) / 114][Stage 111:======================>                              (49 + 65) / 114][Stage 111:========================>                            (53 + 61) / 114][Stage 111:==============================>                      (66 + 48) / 114][Stage 111:===============================>                     (68 + 46) / 114][Stage 111:================================>                    (69 + 45) / 114][Stage 111:================================>                    (70 + 44) / 114][Stage 111:=================================>                   (72 + 42) / 114][Stage 111:====================================>                (79 + 35) / 114][Stage 111:==========================================>          (91 + 23) / 114][Stage 111:==============================================>      (99 + 15) / 114][Stage 111:==============================================>     (103 + 11) / 114][Stage 111:===================================================> (111 + 3) / 114]                                                                                2023-11-30T15:21:14.783 Generating time_dim at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/time_dim
2023-11-30T15:21:14.783 ================================================================================
2023-11-30T15:21:14.783 START: drop-table-time_dim
2023-11-30T15:21:14.783 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`time_dim`
2023-11-30T15:21:14.793 END took 10 ms: drop-table-time_dim
2023-11-30T15:21:14.793 ================================================================================
2023-11-30T15:21:14.793 ================================================================================
2023-11-30T15:21:14.793 START: create-table-time_dim
2023-11-30T15:21:14.793 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`time_dim` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/time_dim/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/time_dim_1gb_parquet`  
[Stage 114:>                                                        (0 + 1) / 1]23/11/30 15:21:16 WARN TaskSetManager: Lost task 0.0 in stage 114.0 (TID 534) (pluto-mig-adhoc-c-cimage-c69c6e1d-zzgs.c.fks-fdp-galaxy.internal executor 269): java.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonParser.getReadCapabilities()Lcom/fasterxml/jackson/core/util/JacksonFeatureSet;
	at com.fasterxml.jackson.databind.DeserializationContext.<init>(DeserializationContext.java:213)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.<init>(DefaultDeserializationContext.java:50)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.<init>(DefaultDeserializationContext.java:392)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.createInstance(DefaultDeserializationContext.java:414)
	at com.fasterxml.jackson.databind.ObjectReader.createDeserializationContext(ObjectReader.java:2335)
	at com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:2094)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1546)
	at org.json4s.jackson.JsonMethods.parse(JsonMethods.scala:25)
	at org.json4s.jackson.JsonMethods.parse$(JsonMethods.scala:19)
	at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:55)
	at org.apache.spark.sql.types.DataType$.fromJson(DataType.scala:169)
	at org.apache.spark.sql.types.StructType$.$anonfun$fromString$1(StructType.scala:524)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.types.StructType$.fromString(StructType.scala:524)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport.init(ParquetReadSupport.scala:82)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:141)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:153)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:329)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

                                                                                2023-11-30T15:21:18.452 END took 3658 ms: create-table-time_dim
2023-11-30T15:21:18.452 ================================================================================
[Stage 118:>                                                        (0 + 1) / 1]                                                                                2023-11-30T15:21:21.917 Generating warehouse at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/warehouse
2023-11-30T15:21:21.917 ================================================================================
2023-11-30T15:21:21.917 START: drop-table-warehouse
2023-11-30T15:21:21.918 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`warehouse`
2023-11-30T15:21:21.926 END took 8 ms: drop-table-warehouse
2023-11-30T15:21:21.926 ================================================================================
2023-11-30T15:21:21.926 ================================================================================
2023-11-30T15:21:21.926 START: create-table-warehouse
2023-11-30T15:21:21.926 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`warehouse` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/warehouse/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/warehouse_1gb_parquet`  
[Stage 120:>                                                        (0 + 1) / 1]23/11/30 15:21:23 WARN TaskSetManager: Lost task 0.0 in stage 120.0 (TID 541) (pluto-mig-adhoc-c-cimage-c69c6e1d-zzgs.c.fks-fdp-galaxy.internal executor 263): java.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonParser.getReadCapabilities()Lcom/fasterxml/jackson/core/util/JacksonFeatureSet;
	at com.fasterxml.jackson.databind.DeserializationContext.<init>(DeserializationContext.java:213)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.<init>(DefaultDeserializationContext.java:50)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.<init>(DefaultDeserializationContext.java:392)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.createInstance(DefaultDeserializationContext.java:414)
	at com.fasterxml.jackson.databind.ObjectReader.createDeserializationContext(ObjectReader.java:2335)
	at com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:2094)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1546)
	at org.json4s.jackson.JsonMethods.parse(JsonMethods.scala:25)
	at org.json4s.jackson.JsonMethods.parse$(JsonMethods.scala:19)
	at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:55)
	at org.apache.spark.sql.types.DataType$.fromJson(DataType.scala:169)
	at org.apache.spark.sql.types.StructType$.$anonfun$fromString$1(StructType.scala:524)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.types.StructType$.fromString(StructType.scala:524)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport.init(ParquetReadSupport.scala:82)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:141)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:153)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:329)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

                                                                                2023-11-30T15:21:25.450 END took 3523 ms: create-table-warehouse
2023-11-30T15:21:25.450 ================================================================================
[Stage 121:>                                                        (0 + 1) / 1]                                                                                2023-11-30T15:21:27.935 Generating web_page at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/web_page
2023-11-30T15:21:27.935 ================================================================================
2023-11-30T15:21:27.935 START: drop-table-web_page
2023-11-30T15:21:27.935 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`web_page`
2023-11-30T15:21:27.943 END took 8 ms: drop-table-web_page
2023-11-30T15:21:27.944 ================================================================================
2023-11-30T15:21:27.944 ================================================================================
2023-11-30T15:21:27.944 START: create-table-web_page
2023-11-30T15:21:27.944 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`web_page` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/web_page/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/web_page_1gb_parquet`  
[Stage 126:>                                                        (0 + 1) / 1]                                                                                2023-11-30T15:21:30.478 END took 2533 ms: create-table-web_page
2023-11-30T15:21:30.478 ================================================================================
2023-11-30T15:21:31.972 Generating web_returns at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/web_returns
2023-11-30T15:21:31.972 ================================================================================
2023-11-30T15:21:31.972 START: drop-table-web_returns
2023-11-30T15:21:31.972 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`web_returns`
2023-11-30T15:21:31.981 END took 9 ms: drop-table-web_returns
2023-11-30T15:21:31.981 ================================================================================
2023-11-30T15:21:31.981 ================================================================================
2023-11-30T15:21:31.981 START: create-table-web_returns
2023-11-30T15:21:31.982 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`web_returns` USING ICEBERG PARTITIONED BY (wr_returned_date_sk)  LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/web_returns/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/web_returns_1gb_parquet` WHERE wr_returned_date_sk IS NOT NULL 
[Stage 132:============================>                            (1 + 1) / 2][Stage 132:============================>                            (1 + 1) / 2][Stage 132:============================>                            (1 + 1) / 2][Stage 132:============================>                            (1 + 1) / 2]                                                                                2023-11-30T15:25:09.416 END took 217434 ms: create-table-web_returns
2023-11-30T15:25:09.416 ================================================================================
[Stage 136:>                                                       (0 + 1) / 67][Stage 136:>                                                       (1 + 1) / 67][Stage 136:=>                                                      (2 + 1) / 67][Stage 136:=>                                                      (2 + 2) / 67][Stage 136:==>                                                     (3 + 2) / 67][Stage 136:==>                                                     (3 + 3) / 67][Stage 136:==>                                                     (3 + 5) / 67][Stage 136:===>                                                    (4 + 5) / 67][Stage 136:===>                                                    (4 + 8) / 67][Stage 136:===>                                                    (4 + 9) / 67][Stage 136:=====>                                                  (6 + 9) / 67][Stage 136:====>                                                  (6 + 15) / 67][Stage 136:====>                                                  (6 + 17) / 67][Stage 136:=====>                                                 (7 + 17) / 67][Stage 136:======>                                                (8 + 17) / 67][Stage 136:=======>                                               (9 + 18) / 67][Stage 136:=======>                                               (9 + 21) / 67][Stage 136:=======>                                               (9 + 23) / 67][Stage 136:========>                                             (10 + 24) / 67][Stage 136:==========>                                           (13 + 38) / 67][Stage 136:===========>                                          (14 + 44) / 67][Stage 136:===========>                                          (14 + 51) / 67][Stage 136:===========>                                          (14 + 53) / 67][Stage 136:============>                                         (15 + 52) / 67][Stage 136:================>                                     (20 + 47) / 67][Stage 136:=================>                                    (22 + 45) / 67][Stage 136:==================>                                   (23 + 44) / 67][Stage 136:===================>                                  (24 + 43) / 67][Stage 136:=====================>                                (27 + 40) / 67][Stage 136:========================>                             (31 + 36) / 67][Stage 136:=========================>                            (32 + 35) / 67][Stage 136:=============================>                        (36 + 31) / 67][Stage 136:=============================>                        (37 + 30) / 67][Stage 136:==============================>                       (38 + 29) / 67][Stage 136:===============================>                      (39 + 28) / 67][Stage 136:====================================>                 (45 + 22) / 67][Stage 136:=======================================>              (49 + 18) / 67][Stage 136:=========================================>            (52 + 15) / 67][Stage 136:================================================>      (59 + 8) / 67][Stage 136:======================================================>(66 + 1) / 67]                                                                                2023-11-30T15:25:27.777 Generating web_sales at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/web_sales
2023-11-30T15:25:27.777 ================================================================================
2023-11-30T15:25:27.777 START: drop-table-web_sales
2023-11-30T15:25:27.777 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`web_sales`
2023-11-30T15:25:27.787 END took 9 ms: drop-table-web_sales
2023-11-30T15:25:27.787 ================================================================================
2023-11-30T15:25:27.787 ================================================================================
2023-11-30T15:25:27.787 START: create-table-web_sales
2023-11-30T15:25:27.787 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`web_sales` USING ICEBERG PARTITIONED BY (ws_sold_date_sk)  LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/web_sales/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/web_sales_1gb_parquet` WHERE ws_sold_date_sk IS NOT NULL 
[Stage 138:>                                                        (0 + 1) / 1]                                                                                [Stage 139:====>                                                  (1 + 12) / 13]23/11/30 15:25:30 WARN TaskSetManager: Lost task 11.0 in stage 139.0 (TID 640) (pluto-mig-adhoc-c-cimage-c69c6e1d-wkxs.c.fks-fdp-galaxy.internal executor 290): java.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonParser.getReadCapabilities()Lcom/fasterxml/jackson/core/util/JacksonFeatureSet;
	at com.fasterxml.jackson.databind.DeserializationContext.<init>(DeserializationContext.java:213)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.<init>(DefaultDeserializationContext.java:50)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.<init>(DefaultDeserializationContext.java:392)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.createInstance(DefaultDeserializationContext.java:414)
	at com.fasterxml.jackson.databind.ObjectReader.createDeserializationContext(ObjectReader.java:2335)
	at com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:2094)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1546)
	at org.json4s.jackson.JsonMethods.parse(JsonMethods.scala:25)
	at org.json4s.jackson.JsonMethods.parse$(JsonMethods.scala:19)
	at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:55)
	at org.apache.spark.sql.types.DataType$.fromJson(DataType.scala:169)
	at org.apache.spark.sql.types.StructType$.$anonfun$fromString$1(StructType.scala:524)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.types.StructType$.fromString(StructType.scala:524)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport.init(ParquetReadSupport.scala:82)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:141)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:153)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:329)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

[Stage 139:==============================>                         (7 + 6) / 13][Stage 139:======================================>                 (9 + 4) / 13][Stage 139:==========================================>            (10 + 3) / 13][Stage 139:==============================================>        (11 + 2) / 13][Stage 139:==================================================>    (12 + 1) / 13][Stage 139:==================================================>    (12 + 1) / 13][Stage 139:==================================================>    (12 + 1) / 13][Stage 139:==================================================>    (12 + 1) / 13]                                                                                2023-11-30T15:28:36.290 END took 188502 ms: create-table-web_sales
2023-11-30T15:28:36.290 ================================================================================
[Stage 143:>                                                       (0 + 1) / 57][Stage 143:>                                                       (1 + 1) / 57][Stage 143:=>                                                      (2 + 1) / 57][Stage 143:=>                                                      (2 + 2) / 57][Stage 143:=>                                                      (2 + 3) / 57][Stage 143:==>                                                     (3 + 3) / 57][Stage 143:==>                                                     (3 + 4) / 57][Stage 143:==>                                                     (3 + 5) / 57][Stage 143:===>                                                    (4 + 5) / 57][Stage 143:===>                                                    (4 + 7) / 57][Stage 143:===>                                                    (4 + 9) / 57][Stage 143:====>                                                   (5 + 9) / 57][Stage 143:=====>                                                  (6 + 9) / 57][Stage 143:=====>                                                 (6 + 10) / 57][Stage 143:======>                                                (7 + 14) / 57][Stage 143:======>                                                (7 + 17) / 57][Stage 143:======>                                                (7 + 21) / 57][Stage 143:======>                                                (7 + 24) / 57][Stage 143:======>                                                (7 + 33) / 57][Stage 143:=======>                                               (8 + 33) / 57][Stage 143:========>                                              (9 + 33) / 57][Stage 143:=========>                                            (10 + 33) / 57][Stage 143:=========>                                            (10 + 35) / 57][Stage 143:==========>                                           (11 + 39) / 57][Stage 143:===========>                                          (12 + 44) / 57][Stage 143:==============>                                       (15 + 42) / 57][Stage 143:=================>                                    (18 + 39) / 57][Stage 143:==================>                                   (19 + 38) / 57][Stage 143:===================>                                  (21 + 36) / 57][Stage 143:====================>                                 (22 + 35) / 57][Stage 143:=========================>                            (27 + 30) / 57][Stage 143:================================>                     (34 + 23) / 57][Stage 143:=================================>                    (35 + 22) / 57][Stage 143:====================================>                 (38 + 19) / 57][Stage 143:========================================>             (43 + 14) / 57][Stage 143:===========================================>          (46 + 11) / 57][Stage 143:============================================>         (47 + 10) / 57][Stage 143:==============================================>        (48 + 9) / 57][Stage 143:====================================================>  (54 + 3) / 57][Stage 143:======================================================>(56 + 1) / 57][Stage 144:>                                                        (0 + 1) / 1]                                                                                2023-11-30T15:28:55.624 Generating web_site at gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/web_site
2023-11-30T15:28:55.625 ================================================================================
2023-11-30T15:28:55.625 START: drop-table-web_site
2023-11-30T15:28:55.625 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf1_ICEBERG`.`web_site`
2023-11-30T15:28:55.634 END took 9 ms: drop-table-web_site
2023-11-30T15:28:55.634 ================================================================================
2023-11-30T15:28:55.634 ================================================================================
2023-11-30T15:28:55.634 START: create-table-web_site
2023-11-30T15:28:55.634 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf1_ICEBERG`.`web_site` USING ICEBERG LOCATION 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load/web_site/' TBLPROPERTIES ('write.spark.fanout.enabled'='true') AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb/web_site_1gb_parquet`  
[Stage 146:>                                                        (0 + 1) / 1]23/11/30 15:28:57 WARN TaskSetManager: Lost task 0.0 in stage 146.0 (TID 707) (pluto-mig-adhoc-c-cimage-c69c6e1d-wkxs.c.fks-fdp-galaxy.internal executor 376): java.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonParser.getReadCapabilities()Lcom/fasterxml/jackson/core/util/JacksonFeatureSet;
	at com.fasterxml.jackson.databind.DeserializationContext.<init>(DeserializationContext.java:213)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.<init>(DefaultDeserializationContext.java:50)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.<init>(DefaultDeserializationContext.java:392)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext$Impl.createInstance(DefaultDeserializationContext.java:414)
	at com.fasterxml.jackson.databind.ObjectReader.createDeserializationContext(ObjectReader.java:2335)
	at com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:2094)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1546)
	at org.json4s.jackson.JsonMethods.parse(JsonMethods.scala:25)
	at org.json4s.jackson.JsonMethods.parse$(JsonMethods.scala:19)
	at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:55)
	at org.apache.spark.sql.types.DataType$.fromJson(DataType.scala:169)
	at org.apache.spark.sql.types.StructType$.$anonfun$fromString$1(StructType.scala:524)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.types.StructType$.fromString(StructType.scala:524)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport.init(ParquetReadSupport.scala:82)
	at org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:141)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:153)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:329)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

23/11/30 15:28:58 WARN ThrowableSerializationWrapper: Task exception could not be deserialized
java.io.InvalidClassException: com.fasterxml.jackson.databind.JsonMappingException; local class incompatible: stream classdesc serialVersionUID = 3, local class serialVersionUID = 1
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:687)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1883)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1749)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2040)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.ThrowableSerializationWrapper.readObject(TaskEndReason.scala:202)
	at sun.reflect.GeneratedMethodAccessor165.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1158)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2176)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)
	at org.apache.spark.scheduler.TaskResultGetter.$anonfun$enqueueFailedTask$2(TaskResultGetter.scala:141)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)
	at org.apache.spark.scheduler.TaskResultGetter.$anonfun$enqueueFailedTask$1(TaskResultGetter.scala:137)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
23/11/30 15:28:58 WARN TaskSetManager: Lost task 0.1 in stage 146.0 (TID 708) (pluto-mig-adhoc-c-cimage-c69c6e1d-n3cm.c.fks-fdp-galaxy.internal executor 359): UnknownReason
23/11/30 15:28:59 WARN ThrowableSerializationWrapper: Task exception could not be deserialized
java.io.InvalidClassException: com.fasterxml.jackson.databind.JsonMappingException; local class incompatible: stream classdesc serialVersionUID = 3, local class serialVersionUID = 1
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:687)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1883)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1749)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2040)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.ThrowableSerializationWrapper.readObject(TaskEndReason.scala:202)
	at sun.reflect.GeneratedMethodAccessor165.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1158)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2176)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)
	at org.apache.spark.scheduler.TaskResultGetter.$anonfun$enqueueFailedTask$2(TaskResultGetter.scala:141)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)
	at org.apache.spark.scheduler.TaskResultGetter.$anonfun$enqueueFailedTask$1(TaskResultGetter.scala:137)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
23/11/30 15:28:59 WARN TaskSetManager: Lost task 0.2 in stage 146.0 (TID 709) (pluto-mig-adhoc-c-cimage-c69c6e1d-4kf9.c.fks-fdp-galaxy.internal executor 348): UnknownReason
                                                                                2023-11-30T15:29:01.483 END took 5848 ms: create-table-web_site
2023-11-30T15:29:01.483 ================================================================================
2023-11-30T15:29:02.996 ====== Created all tables in database tpcds_sf1_ICEBERG at 'gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/databases/tpcds_sf1_ICEBERG_20231130_150211_tpcds_1gb_iceberg_load' =======
2023-11-30T15:29:02.996 ================================================================================
2023-11-30T15:29:02.996 START: USE hive_pluto.tpcds_sf1_ICEBERG;
2023-11-30T15:29:02.996 SQL: USE hive_pluto.tpcds_sf1_ICEBERG;
2023-11-30T15:29:03.110 END took 113 ms: USE hive_pluto.tpcds_sf1_ICEBERG;
2023-11-30T15:29:03.110 ================================================================================
2023-11-30T15:29:03.110 ================================================================================
2023-11-30T15:29:03.110 START: SHOW TABLES
2023-11-30T15:29:03.110 SQL: SHOW TABLES
23/11/30 15:29:03 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.
23/11/30 15:29:03 WARN SparkConf: The configuration key 'spark.yarn.driver.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.driver.memoryOverhead' instead.
+-----------------+----------------------+
|namespace        |tableName             |
+-----------------+----------------------+
|tpcds_sf1_ICEBERG|call_center           |
|tpcds_sf1_ICEBERG|catalog_page          |
|tpcds_sf1_ICEBERG|catalog_returns       |
|tpcds_sf1_ICEBERG|catalog_sales         |
|tpcds_sf1_ICEBERG|customer              |
|tpcds_sf1_ICEBERG|customer_address      |
|tpcds_sf1_ICEBERG|customer_demographics |
|tpcds_sf1_ICEBERG|date_dim              |
|tpcds_sf1_ICEBERG|household_demographics|
|tpcds_sf1_ICEBERG|income_band           |
|tpcds_sf1_ICEBERG|inventory             |
|tpcds_sf1_ICEBERG|item                  |
|tpcds_sf1_ICEBERG|promotion             |
|tpcds_sf1_ICEBERG|reason                |
|tpcds_sf1_ICEBERG|ship_mode             |
|tpcds_sf1_ICEBERG|store                 |
|tpcds_sf1_ICEBERG|store_returns         |
|tpcds_sf1_ICEBERG|store_sales           |
|tpcds_sf1_ICEBERG|time_dim              |
|tpcds_sf1_ICEBERG|warehouse             |
+-----------------+----------------------+
only showing top 20 rows

2023-11-30T15:29:03.497 END took 323 ms: SHOW TABLES
2023-11-30T15:29:03.498 ================================================================================
2023-11-30T15:29:03.498 ================================================================================
RESULT:
{
  "benchmarkSpecs" : {
    "benchmarkId" : "20231130-150211-tpcds-1gb-iceberg-load",
    "benchmarkPath" : "gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking",
    "excludeNulls" : "true",
    "format" : "ICEBERG",
    "scaleInGB" : "1",
    "sourcePath" : "gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/1gb"
  },
  "extraMetrics" : { },
  "queryResults" : [ {
    "durationMs" : 3075,
    "name" : "drop-database"
  }, {
    "durationMs" : 237,
    "name" : "create-database"
  }, {
    "durationMs" : 349,
    "name" : "drop-table-call_center"
  }, {
    "durationMs" : 18809,
    "name" : "create-table-call_center"
  }, {
    "durationMs" : 16,
    "name" : "drop-table-catalog_page"
  }, {
    "durationMs" : 1545,
    "name" : "create-table-catalog_page"
  }, {
    "durationMs" : 12,
    "name" : "drop-table-catalog_returns"
  }, {
    "durationMs" : 216682,
    "name" : "create-table-catalog_returns"
  }, {
    "durationMs" : 12,
    "name" : "drop-table-catalog_sales"
  }, {
    "durationMs" : 196154,
    "name" : "create-table-catalog_sales"
  }, {
    "durationMs" : 11,
    "name" : "drop-table-customer"
  }, {
    "durationMs" : 5453,
    "name" : "create-table-customer"
  }, {
    "durationMs" : 10,
    "name" : "drop-table-customer_address"
  }, {
    "durationMs" : 2498,
    "name" : "create-table-customer_address"
  }, {
    "durationMs" : 9,
    "name" : "drop-table-customer_demographics"
  }, {
    "durationMs" : 5041,
    "name" : "create-table-customer_demographics"
  }, {
    "durationMs" : 11,
    "name" : "drop-table-date_dim"
  }, {
    "durationMs" : 5588,
    "name" : "create-table-date_dim"
  }, {
    "durationMs" : 10,
    "name" : "drop-table-household_demographics"
  }, {
    "durationMs" : 2275,
    "name" : "create-table-household_demographics"
  }, {
    "durationMs" : 9,
    "name" : "drop-table-income_band"
  }, {
    "durationMs" : 4192,
    "name" : "create-table-income_band"
  }, {
    "durationMs" : 9,
    "name" : "drop-table-inventory"
  }, {
    "durationMs" : 34863,
    "name" : "create-table-inventory"
  }, {
    "durationMs" : 9,
    "name" : "drop-table-item"
  }, {
    "durationMs" : 4467,
    "name" : "create-table-item"
  }, {
    "durationMs" : 9,
    "name" : "drop-table-promotion"
  }, {
    "durationMs" : 1960,
    "name" : "create-table-promotion"
  }, {
    "durationMs" : 9,
    "name" : "drop-table-reason"
  }, {
    "durationMs" : 1250,
    "name" : "create-table-reason"
  }, {
    "durationMs" : 8,
    "name" : "drop-table-ship_mode"
  }, {
    "durationMs" : 1680,
    "name" : "create-table-ship_mode"
  }, {
    "durationMs" : 9,
    "name" : "drop-table-store"
  }, {
    "durationMs" : 1650,
    "name" : "create-table-store"
  }, {
    "durationMs" : 9,
    "name" : "drop-table-store_returns"
  }, {
    "durationMs" : 214223,
    "name" : "create-table-store_returns"
  }, {
    "durationMs" : 11,
    "name" : "drop-table-store_sales"
  }, {
    "durationMs" : 190924,
    "name" : "create-table-store_sales"
  }, {
    "durationMs" : 10,
    "name" : "drop-table-time_dim"
  }, {
    "durationMs" : 3658,
    "name" : "create-table-time_dim"
  }, {
    "durationMs" : 8,
    "name" : "drop-table-warehouse"
  }, {
    "durationMs" : 3523,
    "name" : "create-table-warehouse"
  }, {
    "durationMs" : 8,
    "name" : "drop-table-web_page"
  }, {
    "durationMs" : 2533,
    "name" : "create-table-web_page"
  }, {
    "durationMs" : 9,
    "name" : "drop-table-web_returns"
  }, {
    "durationMs" : 217434,
    "name" : "create-table-web_returns"
  }, {
    "durationMs" : 9,
    "name" : "drop-table-web_sales"
  }, {
    "durationMs" : 188502,
    "name" : "create-table-web_sales"
  }, {
    "durationMs" : 9,
    "name" : "drop-table-web_site"
  }, {
    "durationMs" : 5848,
    "name" : "create-table-web_site"
  }, {
    "durationMs" : 113,
    "name" : ""
  }, {
    "durationMs" : 323,
    "name" : ""
  } ],
  "sparkEnvInfo" : {
    "classpathEntries" : {
      "//fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar" : "System Classpath",
      "/etc/hadoop/conf/" : "System Classpath",
      "/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/conf/" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/HikariCP-2.5.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/JLargeArrays-1.5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/JTransforms-3.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/RoaringBitmap-0.9.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/ST4-4.0.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/accessors-smart-1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/activation-1.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/aircompressor-0.10.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/algebra_2.12-2.0.0-M2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/antlr-runtime-3.5.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/antlr4-runtime-4.8-1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/aopalliance-1.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/aopalliance-repackaged-2.6.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/arpack_combined_all-0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/arrow-format-2.0.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/arrow-memory-core-2.0.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/arrow-memory-netty-2.0.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/arrow-vector-2.0.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/audience-annotations-0.5.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/avro-1.8.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/avro-ipc-1.8.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/avro-mapred-1.8.2-hadoop2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/bcpkix-jdk15on-1.60.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/bcprov-jdk15on-1.60.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/bonecp-0.8.0.RELEASE.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/breeze-macros_2.12-1.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/breeze_2.12-1.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/cats-kernel_2.12-2.0.0-M4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/chill-java-0.9.5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/chill_2.12-0.9.5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-beanutils-1.9.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-cli-1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-codec-1.10.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-collections-3.2.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-compiler-3.0.16.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-compress-1.20.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-configuration2-2.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-crypto-1.1.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-daemon-1.0.13.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-dbcp-1.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-httpclient-3.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-io-2.5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-lang-2.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-lang3-3.10.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-logging-1.1.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-math3-3.4.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-net-3.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-pool-1.5.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/commons-text-1.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/compress-lzf-1.0.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/core-1.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/curator-client-2.13.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/curator-framework-2.13.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/curator-recipes-2.13.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/datanucleus-api-jdo-4.2.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/datanucleus-core-4.1.17.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/datanucleus-rdbms-4.1.19.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/derby-10.12.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/dnsjava-2.1.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/ehcache-3.3.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/flatbuffers-java-1.9.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/gcs-connector-hadoop3-2.2.7-shaded.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/geronimo-jcache_1.0_spec-1.0-alpha-1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/gson-2.2.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/guava-27.0-jre.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/guice-4.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/guice-servlet-4.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-annotations-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-auth-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-client-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-common-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-hdfs-client-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-mapreduce-client-common-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-mapreduce-client-core-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-mapreduce-client-jobclient-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-yarn-api-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-yarn-client-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-yarn-common-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-yarn-registry-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-yarn-server-common-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hadoop-yarn-server-web-proxy-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-beeline-2.3.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-cli-2.3.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-common-2.3.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-exec-2.3.7-core.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-jdbc-2.3.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-llap-common-2.3.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-metastore-2.3.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-serde-2.3.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-service-rpc-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-shims-0.23-2.3.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-shims-2.3.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-shims-common-2.3.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-shims-scheduler-2.3.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-storage-api-2.7.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hive-vector-code-gen-2.3.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hk2-api-2.6.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hk2-locator-2.6.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hk2-utils-2.6.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/htrace-core4-4.1.0-incubating.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/httpclient-4.4.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/httpcore-4.4.12.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/hudi-spark3.1-bundle_2.12-0.13.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/iceberg-hive-runtime-1.2.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/iceberg-spark-runtime-3.1_2.12-1.2.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/istack-commons-runtime-3.0.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/ivy-2.4.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jackson-annotations-2.10.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jackson-core-2.10.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jackson-core-asl-1.9.13.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jackson-databind-2.10.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jackson-jaxrs-base-2.9.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jackson-jaxrs-json-provider-2.9.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jackson-mapper-asl-1.9.13.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jackson-module-jaxb-annotations-2.10.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jackson-module-paranamer-2.10.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jackson-module-scala_2.12-2.10.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jakarta.activation-api-1.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jakarta.annotation-api-1.3.5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jakarta.inject-2.6.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jakarta.servlet-api-4.0.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jakarta.validation-api-2.0.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jakarta.ws.rs-api-2.1.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jakarta.xml.bind-api-2.3.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/janino-3.0.16.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/javassist-3.25.0-GA.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/javax.inject-1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/javax.jdo-3.2.0-m3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/javolution-5.5.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jaxb-api-2.2.11.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jaxb-runtime-2.3.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jcip-annotations-1.0-1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jcl-over-slf4j-1.7.30.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jdo-api-3.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jersey-client-2.30.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jersey-common-2.30.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jersey-container-servlet-2.30.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jersey-container-servlet-core-2.30.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jersey-hk2-2.30.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jersey-media-jaxb-2.30.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jersey-server-2.30.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jline-2.14.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/joda-time-2.10.5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jodd-core-3.5.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jpam-1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/json-1.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/json-smart-2.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/json4s-ast_2.12-3.7.0-M5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/json4s-core_2.12-3.7.0-M5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/json4s-jackson_2.12-3.7.0-M5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/json4s-scalap_2.12-3.7.0-M5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jsp-api-2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jsr305-3.0.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jta-1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/jul-to-slf4j-1.7.30.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerb-admin-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerb-client-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerb-common-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerb-core-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerb-crypto-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerb-identity-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerb-server-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerb-simplekdc-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerb-util-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerby-asn1-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerby-config-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerby-pkix-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerby-util-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kerby-xdr-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/kryo-shaded-4.0.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/leveldbjni-all-1.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/libfb303-0.9.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/libthrift-0.12.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/log4j-1.2.17.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/lz4-java-1.7.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/machinist_2.12-0.6.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/macro-compat_2.12-1.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/metrics-core-4.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/metrics-graphite-4.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/metrics-jmx-4.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/metrics-json-4.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/metrics-jvm-4.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/minlog-1.3.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/netty-all-4.1.51.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/nimbus-jose-jwt-4.41.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/objenesis-2.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/okhttp-2.7.5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/okio-1.14.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/opencsv-2.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/orc-core-1.5.12.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/orc-mapreduce-1.5.12.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/orc-shims-1.5.12.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/oro-2.0.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/osgi-resource-locator-1.0.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/paranamer-2.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/parquet-column-1.10.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/parquet-common-1.10.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/parquet-encoding-1.10.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/parquet-format-2.4.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/parquet-hadoop-1.10.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/parquet-jackson-1.10.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/protobuf-java-2.5.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/py4j-0.10.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/pyrolite-4.30.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/re2j-1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/scala-collection-compat_2.12-2.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/scala-compiler-2.12.10.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/scala-library-2.12.10.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/scala-parser-combinators_2.12-1.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/scala-reflect-2.12.10.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/scala-xml_2.12-1.2.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/shapeless_2.12-2.3.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/shims-0.9.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/slf4j-api-1.7.30.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/slf4j-log4j12-1.7.30.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/snappy-java-1.1.8.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-catalyst_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-core_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-graphx_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-hive-thriftserver_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-hive_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-kvstore_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-launcher_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-mllib-local_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-mllib_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-network-common_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-network-shuffle_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-repl_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-sketch_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-sql_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-streaming_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-tags_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-unsafe_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spark-yarn_2.12-3.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spire-macros_2.12-0.17.0-M1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spire-platform_2.12-0.17.0-M1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spire-util_2.12-0.17.0-M1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/spire_2.12-0.17.0-M1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/stax-api-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/stax2-api-3.1.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/stream-2.9.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/super-csv-2.2.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/threeten-extra-1.5.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/token-provider-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/transaction-api-1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/univocity-parsers-2.9.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/velocity-1.5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/woodstox-core-5.0.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/xbean-asm7-shaded-4.15.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/xz-1.5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/zookeeper-3.4.14.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3/jars/zstd-jni-1.4.8-1.jar" : "System Classpath",
      "gs" : "System Classpath"
    },
    "hadoopProps" : {
      "adl.feature.ownerandgroup.enableupn" : "false",
      "adl.http.timeout" : "-1",
      "ambari.hive.db.schema.name" : "hive",
      "datanucleus.cache.level2.type" : "none",
      "datanucleus.connectionPool.maxPoolSize" : "10",
      "datanucleus.schema.autoCreateTables" : "true",
      "dfs.blocksize" : "536870912",
      "dfs.bytes-per-checksum" : "512",
      "dfs.client.failover.proxy.provider.pluto" : "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",
      "dfs.client.failover.random.order" : "true",
      "dfs.client.read.shortcircuit" : "true",
      "dfs.client.read.shortcircuit.streams.cache.size" : "4096",
      "dfs.domain.socket.path" : "/var/lib/hadoop-hdfs/dn_socket",
      "dfs.ha.fencing.ssh.connect-timeout" : "30000",
      "dfs.ha.namenodes.pluto" : "nn1,nn2",
      "dfs.namenode.http-address.pluto.nn1" : "fks-fdp-galaxy-pluto-nn-0001.c.fks-fdp-galaxy.internal:50070",
      "dfs.namenode.http-address.pluto.nn2" : "fks-fdp-galaxy-pluto-nn-0002.c.fks-fdp-galaxy.internal:50070",
      "dfs.namenode.https-address.pluto.nn1" : "fks-fdp-galaxy-pluto-nn-0001.c.fks-fdp-galaxy.internal:50470",
      "dfs.namenode.https-address.pluto.nn2" : "fks-fdp-galaxy-pluto-nn-0002.c.fks-fdp-galaxy.internal:50470",
      "dfs.namenode.rpc-address.pluto.nn1" : "fks-fdp-galaxy-pluto-nn-0001.c.fks-fdp-galaxy.internal:8020",
      "dfs.namenode.rpc-address.pluto.nn2" : "fks-fdp-galaxy-pluto-nn-0002.c.fks-fdp-galaxy.internal:8020",
      "dfs.nameservices" : "pluto",
      "dfs.replication" : "3",
      "fdp.default.tier" : "Regular",
      "fdp.ironbank.url" : "http://console.fdp-ironbank-prod.fkcloud.in/",
      "fdp.orgqueue.cache.expire" : "300",
      "fdp.orgqueue.cache.size" : "100",
      "fdp.orgqueue.defaultQueue" : "adhoc",
      "fdp.orgqueue.gringotts.clientId" : "QAAS",
      "fdp.orgqueue.gringotts.clientSecret" : "*********(redacted)",
      "fdp.orgqueue.gringotts.url" : "http://gringotts.fdp-gringotts-prod.fkcloud.in/billingOrg/user",
      "fdp.orgqueue.queueNotFound.errorMessage" : "Queue mapping not found",
      "fdp.orgqueue.validInitiators" : "BADGER,QAAS,LQE,SUPERBI",
      "fdp.tier.expression" : "Regular",
      "fdp.tier.lqe.lock.wait.time.secs" : "30",
      "fdp.tier.lqe.max.sessions" : "1",
      "file.blocksize" : "67108864",
      "file.bytes-per-checksum" : "512",
      "file.client-write-packet-size" : "65536",
      "file.replication" : "1",
      "file.stream-buffer-size" : "4096",
      "fs.AbstractFileSystem.abfs.impl" : "org.apache.hadoop.fs.azurebfs.Abfs",
      "fs.AbstractFileSystem.abfss.impl" : "org.apache.hadoop.fs.azurebfs.Abfss",
      "fs.AbstractFileSystem.adl.impl" : "org.apache.hadoop.fs.adl.Adl",
      "fs.AbstractFileSystem.file.impl" : "org.apache.hadoop.fs.local.LocalFs",
      "fs.AbstractFileSystem.ftp.impl" : "org.apache.hadoop.fs.ftp.FtpFs",
      "fs.AbstractFileSystem.gs.impl" : "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS",
      "fs.AbstractFileSystem.har.impl" : "org.apache.hadoop.fs.HarFs",
      "fs.AbstractFileSystem.hdfs.impl" : "org.apache.hadoop.fs.Hdfs",
      "fs.AbstractFileSystem.s3a.impl" : "org.apache.hadoop.fs.s3a.S3A",
      "fs.AbstractFileSystem.swebhdfs.impl" : "org.apache.hadoop.fs.SWebHdfs",
      "fs.AbstractFileSystem.viewfs.impl" : "org.apache.hadoop.fs.viewfs.ViewFs",
      "fs.AbstractFileSystem.wasb.impl" : "org.apache.hadoop.fs.azure.Wasb",
      "fs.AbstractFileSystem.wasbs.impl" : "org.apache.hadoop.fs.azure.Wasbs",
      "fs.AbstractFileSystem.webhdfs.impl" : "org.apache.hadoop.fs.WebHdfs",
      "fs.abfs.impl" : "org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem",
      "fs.abfss.impl" : "org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem",
      "fs.adl.impl" : "org.apache.hadoop.fs.adl.AdlFileSystem",
      "fs.adl.oauth2.access.token.provider.type" : "*********(redacted)",
      "fs.automatic.close" : "true",
      "fs.azure.authorization" : "false",
      "fs.azure.authorization.caching.enable" : "true",
      "fs.azure.local.sas.key.mode" : "false",
      "fs.azure.sas.expiry.period" : "90d",
      "fs.azure.saskey.usecontainersaskeyforallaccess" : "true",
      "fs.azure.secure.mode" : "false",
      "fs.azure.user.agent.prefix" : "unknown",
      "fs.client.resolve.remote.symlinks" : "true",
      "fs.client.resolve.topology.enabled" : "false",
      "fs.defaultFS" : "hdfs://pluto",
      "fs.df.interval" : "60000",
      "fs.du.interval" : "600000",
      "fs.ftp.data.connection.mode" : "ACTIVE_LOCAL_DATA_CONNECTION_MODE",
      "fs.ftp.host" : "0.0.0.0",
      "fs.ftp.host.port" : "21",
      "fs.ftp.impl" : "org.apache.hadoop.fs.ftp.FTPFileSystem",
      "fs.ftp.transfer.mode" : "BLOCK_TRANSFER_MODE",
      "fs.gs.auth.service.account.enable" : "true",
      "fs.gs.batch.threads" : "60",
      "fs.gs.impl" : "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem",
      "fs.gs.max.requests.per.batch" : "300",
      "fs.gs.reported.permissions" : "777",
      "fs.har.impl.disable.cache" : "true",
      "fs.permissions.umask-mode" : "022",
      "fs.s3.useRequesterPaysHeader" : "true",
      "fs.s3a.assumed.role.credentials.provider" : "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider",
      "fs.s3a.assumed.role.session.duration" : "30m",
      "fs.s3a.assumed.role.sts.endpoint.region" : "us-west-1",
      "fs.s3a.attempts.maximum" : "20",
      "fs.s3a.block.size" : "32M",
      "fs.s3a.buffer.dir" : "${hadoop.tmp.dir}/s3a",
      "fs.s3a.change.detection.mode" : "server",
      "fs.s3a.change.detection.source" : "etag",
      "fs.s3a.change.detection.version.required" : "true",
      "fs.s3a.committer.magic.enabled" : "false",
      "fs.s3a.committer.name" : "file",
      "fs.s3a.committer.staging.abort.pending.uploads" : "true",
      "fs.s3a.committer.staging.conflict-mode" : "fail",
      "fs.s3a.committer.staging.tmp.path" : "tmp/staging",
      "fs.s3a.committer.staging.unique-filenames" : "true",
      "fs.s3a.committer.threads" : "8",
      "fs.s3a.connection.establish.timeout" : "5000",
      "fs.s3a.connection.maximum" : "15",
      "fs.s3a.connection.ssl.enabled" : "true",
      "fs.s3a.connection.timeout" : "200000",
      "fs.s3a.etag.checksum.enabled" : "false",
      "fs.s3a.fast.upload.active.blocks" : "4",
      "fs.s3a.fast.upload.buffer" : "disk",
      "fs.s3a.impl" : "org.apache.hadoop.fs.s3a.S3AFileSystem",
      "fs.s3a.list.version" : "2",
      "fs.s3a.max.total.tasks" : "5",
      "fs.s3a.metadatastore.authoritative" : "false",
      "fs.s3a.metadatastore.impl" : "org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore",
      "fs.s3a.multiobjectdelete.enable" : "true",
      "fs.s3a.multipart.purge" : "false",
      "fs.s3a.multipart.purge.age" : "86400",
      "fs.s3a.multipart.size" : "100M",
      "fs.s3a.multipart.threshold" : "2147483647",
      "fs.s3a.paging.maximum" : "5000",
      "fs.s3a.path.style.access" : "false",
      "fs.s3a.readahead.range" : "64K",
      "fs.s3a.retry.interval" : "500ms",
      "fs.s3a.retry.limit" : "${fs.s3a.attempts.maximum}",
      "fs.s3a.retry.throttle.interval" : "1000ms",
      "fs.s3a.retry.throttle.limit" : "${fs.s3a.attempts.maximum}",
      "fs.s3a.s3guard.cli.prune.age" : "86400000",
      "fs.s3a.s3guard.ddb.background.sleep" : "25ms",
      "fs.s3a.s3guard.ddb.max.retries" : "9",
      "fs.s3a.s3guard.ddb.table.capacity.read" : "500",
      "fs.s3a.s3guard.ddb.table.capacity.write" : "100",
      "fs.s3a.s3guard.ddb.table.create" : "false",
      "fs.s3a.s3guard.ddb.throttle.retry.interval" : "100ms",
      "fs.s3a.socket.recv.buffer" : "8192",
      "fs.s3a.socket.send.buffer" : "8192",
      "fs.s3a.threads.keepalivetime" : "60",
      "fs.s3a.threads.max" : "10",
      "fs.swift.impl" : "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem",
      "fs.trash.checkpoint.interval" : "0",
      "fs.trash.interval" : "0",
      "fs.viewfs.rename.strategy" : "SAME_MOUNTPOINT",
      "fs.wasb.impl" : "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
      "fs.wasbs.impl" : "org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure",
      "ftp.blocksize" : "67108864",
      "ftp.bytes-per-checksum" : "512",
      "ftp.client-write-packet-size" : "65536",
      "ftp.replication" : "3",
      "ftp.stream-buffer-size" : "4096",
      "google.cloud.auth.service.account.enable" : "true",
      "ha.failover-controller.cli-check.rpc-timeout.ms" : "20000",
      "ha.failover-controller.graceful-fence.connection.retries" : "1",
      "ha.failover-controller.graceful-fence.rpc-timeout.ms" : "5000",
      "ha.failover-controller.new-active.rpc-timeout.ms" : "60000",
      "ha.health-monitor.check-interval.ms" : "1000",
      "ha.health-monitor.connect-retry-interval.ms" : "1000",
      "ha.health-monitor.rpc-timeout.ms" : "45000",
      "ha.health-monitor.sleep-after-disconnect.ms" : "1000",
      "ha.zookeeper.acl" : "world:anyone:rwcda",
      "ha.zookeeper.parent-znode" : "/hadoop-ha",
      "ha.zookeeper.session-timeout.ms" : "10000",
      "hadoop.caller.context.enabled" : "false",
      "hadoop.caller.context.max.size" : "128",
      "hadoop.caller.context.signature.max.size" : "40",
      "hadoop.common.configuration.version" : "3.0.0",
      "hadoop.http.authentication.kerberos.keytab" : "${user.home}/hadoop.keytab",
      "hadoop.http.authentication.kerberos.principal" : "HTTP/_HOST@LOCALHOST",
      "hadoop.http.authentication.signature.secret.file" : "*********(redacted)",
      "hadoop.http.authentication.simple.anonymous.allowed" : "true",
      "hadoop.http.authentication.token.validity" : "*********(redacted)",
      "hadoop.http.authentication.type" : "simple",
      "hadoop.http.cross-origin.allowed-headers" : "X-Requested-With,Content-Type,Accept,Origin",
      "hadoop.http.cross-origin.allowed-methods" : "GET,POST,HEAD",
      "hadoop.http.cross-origin.allowed-origins" : "*",
      "hadoop.http.cross-origin.enabled" : "false",
      "hadoop.http.cross-origin.max-age" : "1800",
      "hadoop.http.filter.initializers" : "org.apache.hadoop.http.lib.StaticUserWebFilter",
      "hadoop.http.logs.enabled" : "true",
      "hadoop.http.staticuser.user" : "dr.who",
      "hadoop.jetty.logs.serve.aliases" : "true",
      "hadoop.kerberos.kinit.command" : "kinit",
      "hadoop.kerberos.min.seconds.before.relogin" : "60",
      "hadoop.proxyuser.hive.groups" : "*",
      "hadoop.proxyuser.hive.hosts" : "*",
      "hadoop.registry.jaas.context" : "Client",
      "hadoop.registry.secure" : "false",
      "hadoop.registry.system.acls" : "sasl:yarn@, sasl:mapred@, sasl:hdfs@",
      "hadoop.registry.zk.connection.timeout.ms" : "15000",
      "hadoop.registry.zk.quorum" : "localhost:2181",
      "hadoop.registry.zk.retry.ceiling.ms" : "60000",
      "hadoop.registry.zk.retry.interval.ms" : "1000",
      "hadoop.registry.zk.retry.times" : "5",
      "hadoop.registry.zk.root" : "/registry",
      "hadoop.registry.zk.session.timeout.ms" : "60000",
      "hadoop.rpc.protection" : "authentication",
      "hadoop.rpc.socket.factory.class.default" : "org.apache.hadoop.net.StandardSocketFactory",
      "hadoop.security.auth_to_local" : "DEFAULT",
      "hadoop.security.auth_to_local.mechanism" : "hadoop",
      "hadoop.security.authentication" : "simple",
      "hadoop.security.authorization" : "false",
      "hadoop.security.credential.clear-text-fallback" : "true",
      "hadoop.security.crypto.buffer.size" : "8192",
      "hadoop.security.crypto.cipher.suite" : "AES/CTR/NoPadding",
      "hadoop.security.crypto.codec.classes.aes.ctr.nopadding" : "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec",
      "hadoop.security.dns.log-slow-lookups.enabled" : "false",
      "hadoop.security.dns.log-slow-lookups.threshold.ms" : "1000",
      "hadoop.security.group.mapping" : "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback",
      "hadoop.security.group.mapping.ldap.connection.timeout.ms" : "60000",
      "hadoop.security.group.mapping.ldap.conversion.rule" : "none",
      "hadoop.security.group.mapping.ldap.directory.search.timeout" : "10000",
      "hadoop.security.group.mapping.ldap.num.attempts" : "3",
      "hadoop.security.group.mapping.ldap.num.attempts.before.failover" : "3",
      "hadoop.security.group.mapping.ldap.posix.attr.gid.name" : "gidNumber",
      "hadoop.security.group.mapping.ldap.posix.attr.uid.name" : "uidNumber",
      "hadoop.security.group.mapping.ldap.read.timeout.ms" : "60000",
      "hadoop.security.group.mapping.ldap.search.attr.group.name" : "cn",
      "hadoop.security.group.mapping.ldap.search.attr.member" : "member",
      "hadoop.security.group.mapping.ldap.search.filter.group" : "(objectClass=group)",
      "hadoop.security.group.mapping.ldap.search.filter.user" : "(&(objectClass=user)(sAMAccountName={0}))",
      "hadoop.security.group.mapping.ldap.search.group.hierarchy.levels" : "0",
      "hadoop.security.group.mapping.ldap.ssl" : "false",
      "hadoop.security.group.mapping.providers.combined" : "true",
      "hadoop.security.groups.cache.background.reload" : "false",
      "hadoop.security.groups.cache.background.reload.threads" : "3",
      "hadoop.security.groups.cache.secs" : "300",
      "hadoop.security.groups.cache.warn.after.ms" : "5000",
      "hadoop.security.groups.negative-cache.secs" : "30",
      "hadoop.security.groups.shell.command.timeout" : "0s",
      "hadoop.security.instrumentation.requires.admin" : "false",
      "hadoop.security.java.secure.random.algorithm" : "SHA1PRNG",
      "hadoop.security.key.default.bitlength" : "128",
      "hadoop.security.key.default.cipher" : "AES/CTR/NoPadding",
      "hadoop.security.kms.client.authentication.retry-count" : "1",
      "hadoop.security.kms.client.encrypted.key.cache.expiry" : "43200000",
      "hadoop.security.kms.client.encrypted.key.cache.low-watermark" : "0.3f",
      "hadoop.security.kms.client.encrypted.key.cache.num.refill.threads" : "2",
      "hadoop.security.kms.client.encrypted.key.cache.size" : "500",
      "hadoop.security.kms.client.failover.sleep.base.millis" : "100",
      "hadoop.security.kms.client.failover.sleep.max.millis" : "2000",
      "hadoop.security.kms.client.timeout" : "60",
      "hadoop.security.random.device.file.path" : "/dev/urandom",
      "hadoop.security.sensitive-config-keys" : "*********(redacted)",
      "hadoop.security.uid.cache.secs" : "14400",
      "hadoop.service.shutdown.timeout" : "30s",
      "hadoop.shell.missing.defaultFs.warning" : "false",
      "hadoop.shell.safely.delete.limit.num.files" : "100",
      "hadoop.ssl.client.conf" : "ssl-client.xml",
      "hadoop.ssl.enabled" : "false",
      "hadoop.ssl.enabled.protocols" : "TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2",
      "hadoop.ssl.hostname.verifier" : "DEFAULT",
      "hadoop.ssl.keystores.factory.class" : "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory",
      "hadoop.ssl.require.client.cert" : "false",
      "hadoop.ssl.server.conf" : "ssl-server.xml",
      "hadoop.system.tags" : "YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT\n      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL",
      "hadoop.tags.system" : "YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT\n      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL",
      "hadoop.tmp.dir" : "/tmp/hadoop-${user.name}",
      "hadoop.user.group.static.mapping.overrides" : "dr.who=;",
      "hadoop.util.hash.type" : "murmur",
      "hadoop.workaround.non.threadsafe.getpwuid" : "true",
      "hadoop.zk.acl" : "world:anyone:rwcda",
      "hadoop.zk.num-retries" : "1000",
      "hadoop.zk.retry-interval-ms" : "1000",
      "hadoop.zk.timeout-ms" : "10000",
      "hbase.zookeeper.quorum" : "fks-fdp-galaxy-pluto-zkjn-0001.c.fks-fdp-galaxy.internal:2181,fks-fdp-galaxy-pluto-zkjn-0002.c.fks-fdp-galaxy.internal:2181,fks-fdp-galaxy-pluto-zkjn-0003.c.fks-fdp-galaxy.internal:2181",
      "hive.auto.convert.join" : "false",
      "hive.auto.convert.join.noconditionaltask" : "false",
      "hive.auto.convert.join.noconditionaltask.size" : "134217728",
      "hive.auto.convert.sortmerge.join" : "false",
      "hive.auto.convert.sortmerge.join.noconditionaltask" : "false",
      "hive.auto.convert.sortmerge.join.to.mapjoin" : "false",
      "hive.cbo.enable" : "true",
      "hive.cli.print.header" : "false",
      "hive.cluster.delegation.token.store.class" : "*********(redacted)",
      "hive.cluster.delegation.token.store.zookeeper.connectString" : "*********(redacted)",
      "hive.cluster.delegation.token.store.zookeeper.znode" : "*********(redacted)",
      "hive.compactor.abortedtxn.threshold" : "1000",
      "hive.compactor.check.interval" : "300L",
      "hive.compactor.delta.num.threshold" : "10",
      "hive.compactor.delta.pct.threshold" : "0.1f",
      "hive.compactor.initiator.on" : "false",
      "hive.compactor.worker.threads" : "0",
      "hive.compactor.worker.timeout" : "86400L",
      "hive.compute.query.using.stats" : "false",
      "hive.conf.restricted.list" : "hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role",
      "hive.convert.join.bucket.mapjoin.tez" : "true",
      "hive.default.fileformat.managed" : "ORC",
      "hive.driver.parallel.compilation" : "true",
      "hive.enforce.bucketing" : "true",
      "hive.enforce.sorting" : "true",
      "hive.enforce.sortmergebucketmapjoin" : "true",
      "hive.exec.compress.intermediate" : "true",
      "hive.exec.compress.output" : "false",
      "hive.exec.dynamic.partition" : "true",
      "hive.exec.dynamic.partition.mode" : "nonstrict",
      "hive.exec.failure.hooks" : "com.flipkart.fdp.hive.hooks.EnforceSingleQueryPostExecHook",
      "hive.exec.max.created.files" : "100000",
      "hive.exec.max.dynamic.partitions" : "1000000",
      "hive.exec.max.dynamic.partitions.pernode" : "1000000",
      "hive.exec.orc.compression.strategy" : "SPEED",
      "hive.exec.orc.default.compress" : "ZLIB",
      "hive.exec.orc.default.stripe.size" : "67108864",
      "hive.exec.parallel" : "true",
      "hive.exec.parallel.thread.number" : "8",
      "hive.exec.post.hooks" : "org.apache.hadoop.hive.ql.hooks.LineageLogger,com.flipkart.fdp.hive.hooks.EnforceSingleQueryPostExecHook",
      "hive.exec.pre.hooks" : "com.flipkart.fdp.hive.hooks.EnforceSingleQueryPreExecHook",
      "hive.exec.reducers.bytes.per.reducer" : "134217728",
      "hive.exec.reducers.max" : "250",
      "hive.exec.scratchdir" : "gs://stage-hive-metastore-pluto/user/hiveexternaldir",
      "hive.exec.submit.local.task.via.child" : "true",
      "hive.exec.submitviachild" : "false",
      "hive.execution.engine" : "tez",
      "hive.fetch.task.aggr" : "true",
      "hive.fetch.task.conversion" : "more",
      "hive.fetch.task.conversion.threshold" : "1073741824",
      "hive.fileformat.managed" : "ORC",
      "hive.heapsize" : "2048",
      "hive.input.format" : "org.apache.hadoop.hive.ql.io.CombineHiveInputFormat",
      "hive.limit.optimize.enable" : "true",
      "hive.limit.pushdown.memory.usage" : "0.2",
      "hive.map.aggr" : "true",
      "hive.map.aggr.hash.force.flush.memory.threshold" : "0.5",
      "hive.map.aggr.hash.min.reduction" : "0.5",
      "hive.map.aggr.hash.percentmemory" : "0.5",
      "hive.mapjoin.bucket.cache.size" : "1000",
      "hive.mapjoin.localtask.max.memory.usage" : "0.4",
      "hive.mapjoin.optimized.hashtable" : "false",
      "hive.mapred.reduce.tasks.speculative.execution" : "false",
      "hive.merge.mapfiles" : "true",
      "hive.merge.mapredfiles" : "false",
      "hive.merge.orcfile.stripe.level" : "true",
      "hive.merge.rcfile.block.level" : "true",
      "hive.merge.size.per.task" : "256000000",
      "hive.merge.smallfiles.avgsize" : "16000000",
      "hive.merge.tezfiles" : "true",
      "hive.metastore.authorization.storage.checks" : "false",
      "hive.metastore.cache.pinobjtypes" : "Table,Database,Type,FieldSchema,Order",
      "hive.metastore.client.connect.retry.delay" : "5s",
      "hive.metastore.client.scheme.handlers" : "com.flipkart.fdp.hive.metastore.ELBSchemeHandler",
      "hive.metastore.client.socket.timeout" : "1800s",
      "hive.metastore.connect.retries" : "24",
      "hive.metastore.execute.setugi" : "true",
      "hive.metastore.failure.retries" : "24",
      "hive.metastore.fshandler.threads" : "15",
      "hive.metastore.kerberos.keytab.file" : "/etc/security/keytabs/hive.service.keytab",
      "hive.metastore.kerberos.principal" : "hive/_HOST@EXAMPLE.COM",
      "hive.metastore.limit.partition.request" : "-1",
      "hive.metastore.metrics.enabled" : "true",
      "hive.metastore.pre.event.listeners" : "org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener",
      "hive.metastore.sasl.enabled" : "false",
      "hive.metastore.server.max.threads" : "100000",
      "hive.metastore.uris" : "thrift://10.116.17.2:9083",
      "hive.metastore.warehouse.dir" : "gs://stage-hive-metastore-pluto/apps/hive/warehouse",
      "hive.msck.path.validation" : "ignore",
      "hive.msck.repair.batch.size" : "0",
      "hive.optimize.bucketmapjoin" : "true",
      "hive.optimize.bucketmapjoin.sortedmerge" : "true",
      "hive.optimize.constant.propagation" : "true",
      "hive.optimize.index.filter" : "true",
      "hive.optimize.mapjoin.mapreduce" : "true",
      "hive.optimize.metadataonly" : "true",
      "hive.optimize.null.scan" : "true",
      "hive.optimize.reducededuplication" : "true",
      "hive.optimize.reducededuplication.min.reducer" : "4",
      "hive.optimize.sort.dynamic.partition" : "true",
      "hive.orc.compute.splits.num.threads" : "10",
      "hive.orc.splits.include.file.footer" : "false",
      "hive.prewarm.enabled" : "false",
      "hive.prewarm.numcontainers" : "10",
      "hive.querylog.enable.plan.progress" : "false",
      "hive.security.authenticator.manager" : "org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator",
      "hive.security.authorization.enabled" : "false",
      "hive.security.authorization.manager" : "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory",
      "hive.security.authorization.sqlstd.confwhitelist.append" : "|initiator.*|job.*|mapred.*|badger.*|azkaban.*|tez.*|dfs.*|mapreduce.*|hive.*|hbase.*|light.*|beeline.*|orc.*|fdp.*|.*impersonation.*|fs.gs.*",
      "hive.security.metastore.authenticator.manager" : "org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator",
      "hive.security.metastore.authorization.auth.reads" : "true",
      "hive.security.metastore.authorization.manager" : "\n            org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider\n        ",
      "hive.server2.allow.user.substitution" : "true",
      "hive.server2.authentication" : "NONE",
      "hive.server2.authentication.spnego.keytab" : "HTTP/_HOST@EXAMPLE.COM",
      "hive.server2.authentication.spnego.principal" : "/etc/security/keytabs/spnego.service.keytab",
      "hive.server2.clear.dangling.scratchdir" : "true",
      "hive.server2.clear.dangling.scratchdir.interval" : "1800",
      "hive.server2.enable.doAs" : "true",
      "hive.server2.enable.impersonation" : "true",
      "hive.server2.idle.operation.timeout" : "1d",
      "hive.server2.idle.session.timeout" : "1d",
      "hive.server2.logging.operation.enabled" : "true",
      "hive.server2.logging.operation.log.location" : "${java.io.tmpdir}/${user.name}/operation_logs",
      "hive.server2.metrics.enabled" : "true",
      "hive.server2.session.check.interval" : "60m",
      "hive.server2.support.dynamic.service.discovery" : "true",
      "hive.server2.table.type.mapping" : "CLASSIC",
      "hive.server2.tez.default.queues" : "default",
      "hive.server2.tez.initialize.default.sessions" : "false",
      "hive.server2.tez.sessions.per.default.queue" : "1",
      "hive.server2.thrift.http.path" : "cliservice",
      "hive.server2.thrift.http.port" : "10001",
      "hive.server2.thrift.max.worker.threads" : "500",
      "hive.server2.thrift.sasl.qop" : "auth",
      "hive.server2.transport.mode" : "http",
      "hive.server2.use.SSL" : "false",
      "hive.server2.zookeeper.namespace" : "fks-fdp-galaxy-hive3-hs2-pluto",
      "hive.session.history.enabled" : "false",
      "hive.smbjoin.cache.rows" : "1000",
      "hive.stats.autogather" : "true",
      "hive.stats.dbclass" : "fs",
      "hive.stats.fetch.column.stats" : "false",
      "hive.stats.fetch.partition.stats" : "false",
      "hive.strict.checks.cartesian.product" : "false",
      "hive.strict.checks.type.safety" : "false",
      "hive.support.concurrency" : "false",
      "hive.support.sql11.reserved.keywords" : "true",
      "hive.tez.auto.reducer.parallelism" : "false",
      "hive.tez.container.size" : "3072",
      "hive.tez.cpu.vcores" : "-1",
      "hive.tez.dynamic.partition.pruning" : "true",
      "hive.tez.dynamic.partition.pruning.max.data.size" : "104857600",
      "hive.tez.dynamic.partition.pruning.max.event.size" : "1048576",
      "hive.tez.input.format" : "org.apache.hadoop.hive.ql.io.HiveInputFormat",
      "hive.tez.log.level" : "INFO",
      "hive.tez.max.partition.factor" : "2.0",
      "hive.tez.min.partition.factor" : "0.25",
      "hive.tez.smb.number.waves" : "0.5",
      "hive.txn.manager" : "org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager",
      "hive.txn.max.open.batch" : "1000",
      "hive.txn.timeout" : "300",
      "hive.user.install.directory" : "gs://stage-hive-metastore-pluto/user/",
      "hive.vectorized.complex.types.enabled" : "false",
      "hive.vectorized.execution.enabled" : "true",
      "hive.vectorized.execution.ptf.enabled" : "false",
      "hive.vectorized.execution.reduce.enabled" : "true",
      "hive.vectorized.groupby.checkinterval" : "4096",
      "hive.vectorized.groupby.complex.types.enabled" : "false",
      "hive.vectorized.groupby.flush.percent" : "0.1",
      "hive.vectorized.groupby.maxentries" : "10000",
      "hive.zookeeper.client.port" : "2181",
      "hive.zookeeper.namespace" : "fks-sco-hive-pluto-zookeeper-namespace",
      "hive.zookeeper.quorum" : "fks-fdp-galaxy-pluto-zkjn-0001.c.fks-fdp-galaxy.internal:2181,fks-fdp-galaxy-pluto-zkjn-0002.c.fks-fdp-galaxy.internal:2181,fks-fdp-galaxy-pluto-zkjn-0003.c.fks-fdp-galaxy.internal:2181",
      "hive.zookeeper.session.timeout" : "120s",
      "io.compression.codec.bzip2.library" : "system-native",
      "io.compression.codec.lzo.class" : "com.hadoop.compression.lzo.LzoCodec",
      "io.compression.codecs" : "org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,com.hadoop.compression.fourmc.Lz4Codec,com.hadoop.compression.fourmc.Lz4MediumCodec,com.hadoop.compression.fourmc.Lz4HighCodec,com.hadoop.compression.fourmc.Lz4UltraCodec,com.hadoop.compression.fourmc.FourMcCodec,com.hadoop.compression.fourmc.FourMcMediumCodec,com.hadoop.compression.fourmc.FourMcHighCodec,com.hadoop.compression.fourmc.FourMcUltraCodec",
      "io.erasurecode.codec.rs-legacy.rawcoders" : "rs-legacy_java",
      "io.erasurecode.codec.rs.rawcoders" : "rs_native,rs_java",
      "io.erasurecode.codec.xor.rawcoders" : "xor_native,xor_java",
      "io.file.buffer.size" : "65536",
      "io.map.index.interval" : "128",
      "io.map.index.skip" : "0",
      "io.mapfile.bloom.error.rate" : "0.005",
      "io.mapfile.bloom.size" : "1048576",
      "io.seqfile.compress.blocksize" : "1000000",
      "io.seqfile.local.dir" : "${hadoop.tmp.dir}/io/local",
      "io.serializations" : "org.apache.hadoop.io.serializer.WritableSerialization",
      "io.skip.checksum.errors" : "false",
      "ipc.client.bind.wildcard.addr" : "false",
      "ipc.client.connect.max.retries" : "50",
      "ipc.client.connect.max.retries.on.timeouts" : "45",
      "ipc.client.connect.retry.interval" : "1000",
      "ipc.client.connect.timeout" : "20000",
      "ipc.client.connection.maxidletime" : "30000",
      "ipc.client.fallback-to-simple-auth-allowed" : "false",
      "ipc.client.idlethreshold" : "8000",
      "ipc.client.kill.max" : "10",
      "ipc.client.low-latency" : "false",
      "ipc.client.ping" : "true",
      "ipc.client.rpc-timeout.ms" : "0",
      "ipc.client.tcpnodelay" : "true",
      "ipc.maximum.data.length" : "67108864",
      "ipc.maximum.response.length" : "134217728",
      "ipc.ping.interval" : "60000",
      "ipc.server.listen.queue.size" : "128",
      "ipc.server.log.slow.rpc" : "false",
      "ipc.server.max.connections" : "0",
      "javax.jdo.option.ConnectionDriverName" : "com.mysql.jdbc.Driver",
      "javax.jdo.option.ConnectionPassword" : "*********(redacted)",
      "javax.jdo.option.ConnectionURL" : "jdbc:mysql://10.117.192.58/stage_hive_metastore?createDatabaseIfNotExist=true",
      "javax.jdo.option.ConnectionUserName" : "stage_sco_rw",
      "jobname.enricher.class" : "org.apache.hadoop.hive.ql.propertymodifier.NoEnrichment",
      "map.sort.class" : "org.apache.hadoop.util.QuickSort",
      "net.topology.impl" : "org.apache.hadoop.net.NetworkTopology",
      "net.topology.node.switch.mapping.impl" : "org.apache.hadoop.net.ScriptBasedMapping",
      "net.topology.script.file.name" : "/etc/hadoop/conf/topology.py",
      "net.topology.script.number.args" : "100",
      "nfs.exports.allowed.hosts" : "* rw",
      "orc.schema.evolution.case.sensitive" : "false",
      "parser.timeoutSec" : "900",
      "queue.enforcer.class" : "com.flipkart.fdp.hive.orgqueue.OrgQueueEnforcerForInitiator",
      "rpc.metrics.quantile.enable" : "false",
      "seq.io.sort.factor" : "100",
      "seq.io.sort.mb" : "100",
      "tfile.fs.input.buffer.size" : "262144",
      "tfile.fs.output.buffer.size" : "262144",
      "tfile.io.chunk.size" : "1048576",
      "zookeeper.znode.parent" : "/hbase-unsecure"
    },
    "runtimeInfo" : {
      "javaHome" : "/usr/lib/jvm/oracle-java8-jdk-amd64/jre",
      "javaVersion" : "1.8.0_172 (Oracle Corporation)",
      "scalaVersion" : "version 2.12.10"
    },
    "sparkBuildInfo" : {
      "sparkBuildBranch" : "fdp-3.1.2-hadoop-3.2",
      "sparkBuildDate" : "2023-04-05T09:07:25Z",
      "sparkBuildRevision" : "1f03c4907e323e2f782742ceae6feff6c8ddcd12",
      "sparkBuildUser" : "somi.biswas",
      "sparkBuildVersion" : "3.1.2"
    },
    "sparkProps" : {
      "spark.app.id" : "application_1699449121496_0373",
      "spark.app.name" : "Spark shell",
      "spark.app.startTime" : "1701336803630",
      "spark.benchmarkId" : "20231130-150211-tpcds-1gb-iceberg-load",
      "spark.cleaner.ttl" : "86400",
      "spark.delta.logStore.gs.impl" : "io.delta.storage.GCSLogStore",
      "spark.driver.appUIAddress" : "http://fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:4046",
      "spark.driver.cores" : "1",
      "spark.driver.extraClassPath" : "gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar",
      "spark.driver.extraJavaOptions" : "-Denv=prod -Dcom.sun.management.jmxremote.port=0 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=0 -XX:MaxDirectMemorySize=800M -XX:MaxMetaspaceSize=256M -XX:CompressedClassSpaceSize=100M -XX:+UnlockDiagnosticVMOptions -Djob.numOfRePartitions=30",
      "spark.driver.extraLibraryPath" : "/usr/flipkart/3.2.1-1/hadoop-hdfs/lib:/usr/flipkart/3.2.1-1/hadoop-hdfs/lib/native:/usr/lib/hadoop/lib:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/lib/x86_64-linux-gnu",
      "spark.driver.host" : "fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal",
      "spark.driver.memory" : "5120m",
      "spark.driver.port" : "40443",
      "spark.dynamicAllocation.enabled" : "true",
      "spark.dynamicAllocation.executorIdleTimeout" : "60s",
      "spark.dynamicAllocation.maxExecutors" : "200",
      "spark.eventLog.dir" : "gs://fks-fdp-infra-job-history/pluto/spark-job-history",
      "spark.eventLog.enabled" : "true",
      "spark.executor.cores" : "1",
      "spark.executor.extraClassPath" : "gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar",
      "spark.executor.extraJavaOptions" : "-XX:MaxDirectMemorySize=8096m -Dio.netty.maxDirectMemory=8096m -XX:+UseG1GC -XX:ConcGCThreads=2",
      "spark.executor.extraLibraryPath" : "/usr/flipkart/3.2.1-1/hadoop-hdfs/lib:/usr/flipkart/3.2.1-1/hadoop-hdfs/lib/native:/usr/lib/hadoop/lib:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/lib/x86_64-linux-gnu",
      "spark.executor.id" : "driver",
      "spark.executor.memory" : "10240m",
      "spark.fdp.orgqueue.cache.expire" : "300",
      "spark.fdp.orgqueue.cache.size" : "100",
      "spark.fdp.orgqueue.defaultQueue" : "adhoc",
      "spark.fdp.orgqueue.gringotts.clientId" : "QAAS",
      "spark.fdp.orgqueue.gringotts.clientSecret" : "*********(redacted)",
      "spark.fdp.orgqueue.gringotts.url" : "http://10.47.6.66/billingOrg/user",
      "spark.fdp.orgqueue.ironbank.url" : "http://10.47.4.16:/queue/",
      "spark.fdp.orgqueue.queueNotFound.errorMessage" : "Queue mapping not found",
      "spark.fdp.orgqueue.validInitiators" : "BADGER,QAAS",
      "spark.hadoop.fs.s3.useRequesterPaysHeader" : "true",
      "spark.hadoop.yarn.timeline-service.enabled" : "false",
      "spark.history.fs.cleaner.interval" : "1d",
      "spark.history.fs.cleaner.maxAge" : "60d",
      "spark.history.fs.logDirectory" : "gs://fks-fdp-infra-job-history/pluto/spark-job-history",
      "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
      "spark.home" : "/var/lib/fk-pf-spark3",
      "spark.jars" : "",
      "spark.master" : "yarn",
      "spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS" : "fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal,fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal",
      "spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES" : "http://fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal:8088/proxy/application_1699449121496_0373,http://fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal:8088/proxy/application_1699449121496_0373",
      "spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.RM_HA_URLS" : "fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal:8088,fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal:8088",
      "spark.queue.enforcer.class" : "com.flipkart.fdp.orgqueue.OrgQueueEnforcerForInitiator",
      "spark.repl.class.outputDir" : "/grid/1/spark3/tmp/spark-843a47fb-2a18-4adb-a3b8-eb8af8ac4aa3/repl-1cbc400f-d983-4d3b-a6b6-adcf6b33c856",
      "spark.repl.class.uri" : "spark://fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:40443/classes",
      "spark.repl.local.jars" : "file:///home/vanshika.yadav/20231130-150211-tpcds-1gb-iceberg-load-benchmarks.jar,file:///var/lib/fk-pf-spark3/jars/iceberg-hive-runtime-1.2.0.jar,file:///var/lib/fk-pf-spark3/jars/iceberg-spark-runtime-3.1_2.12-1.2.0.jar",
      "spark.scheduler.mode" : "FIFO",
      "spark.shuffle.service.enabled" : "true",
      "spark.shuffle.useOldFetchProtocol" : "true",
      "spark.sql.catalog.hive_pluto" : "org.apache.iceberg.spark.SparkCatalog",
      "spark.sql.catalog.hive_pluto.type" : "hive",
      "spark.sql.catalog.hive_pluto.uri" : "thrift://10.116.17.2:9083",
      "spark.sql.catalogImplementation" : "hive",
      "spark.sql.extensions" : "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions",
      "spark.sql.sources.partitionOverwriteMode" : "dynamic",
      "spark.streaming.concurrentJobs" : "4",
      "spark.submit.deployMode" : "client",
      "spark.submit.pyFiles" : "",
      "spark.ui.filters" : "org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter",
      "spark.ui.showConsoleProgress" : "true",
      "spark.yarn.dist.jars" : "gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar,file:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar,gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/json-serde-1.3-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/dimlookup-hive-udf-1.0-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,file:///home/vanshika.yadav/20231130-150211-tpcds-1gb-iceberg-load-benchmarks.jar,file:///var/lib/fk-pf-spark3/jars/iceberg-hive-runtime-1.2.0.jar,file:///var/lib/fk-pf-spark3/jars/iceberg-spark-runtime-3.1_2.12-1.2.0.jar",
      "spark.yarn.driver.memoryOverhead" : "4096",
      "spark.yarn.executor.memoryOverhead" : "4096",
      "spark.yarn.historyServer.address" : "http://fks-fdp-galaxy-pluto-hs-0001-m.c.fks-fdp-galaxy.internal:18080",
      "spark.yarn.jars" : "",
      "spark.yarn.queue" : "de_adhoc",
      "spark.yarn.report.interval" : "60s",
      "spark.yarn.secondary.jars" : "hadoop-lzo-0.6.0.2.4.0.0-169.jar,hadoop-4mc-1.1.0.jar,json-serde-1.3-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,dimlookup-hive-udf-1.0-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,20231130-150211-tpcds-1gb-iceberg-load-benchmarks.jar,iceberg-hive-runtime-1.2.0.jar,iceberg-spark-runtime-3.1_2.12-1.2.0.jar"
    },
    "systemProps" : {
      "SPARK_SUBMIT" : "true",
      "awt.toolkit" : "sun.awt.X11.XToolkit",
      "com.sun.management.jmxremote.authenticate" : "false",
      "com.sun.management.jmxremote.port" : "0",
      "com.sun.management.jmxremote.ssl" : "false",
      "env" : "prod",
      "file.encoding" : "ANSI_X3.4-1968",
      "file.encoding.pkg" : "sun.io",
      "file.separator" : "/",
      "java.awt.graphicsenv" : "sun.awt.X11GraphicsEnvironment",
      "java.awt.printerjob" : "sun.print.PSPrinterJob",
      "java.class.version" : "52.0",
      "java.endorsed.dirs" : "/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/endorsed",
      "java.ext.dirs" : "/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/ext:/usr/java/packages/lib/ext",
      "java.home" : "/usr/lib/jvm/oracle-java8-jdk-amd64/jre",
      "java.io.tmpdir" : "/tmp",
      "java.library.path" : "/usr/flipkart/3.2.1-1/hadoop-hdfs/lib:/usr/flipkart/3.2.1-1/hadoop-hdfs/lib/native:/usr/lib/hadoop/lib:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/lib/x86_64-linux-gnu:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib",
      "java.rmi.server.randomIDs" : "true",
      "java.runtime.name" : "Java(TM) SE Runtime Environment",
      "java.runtime.version" : "1.8.0_172-b11",
      "java.specification.name" : "Java Platform API Specification",
      "java.specification.vendor" : "Oracle Corporation",
      "java.specification.version" : "1.8",
      "java.vendor" : "Oracle Corporation",
      "java.vendor.url" : "http://java.oracle.com/",
      "java.vendor.url.bug" : "http://bugreport.sun.com/bugreport/",
      "java.version" : "1.8.0_172",
      "java.vm.info" : "mixed mode",
      "java.vm.name" : "Java HotSpot(TM) 64-Bit Server VM",
      "java.vm.specification.name" : "Java Virtual Machine Specification",
      "java.vm.specification.vendor" : "Oracle Corporation",
      "java.vm.specification.version" : "1.8",
      "java.vm.vendor" : "Oracle Corporation",
      "java.vm.version" : "25.172-b11",
      "jetty.git.hash" : "b881a572662e1943a14ae12e7e1207989f218b74",
      "job.numOfRePartitions" : "30",
      "line.separator" : "\n",
      "os.arch" : "amd64",
      "os.name" : "Linux",
      "os.version" : "4.19.0-19-cloud-amd64",
      "path.separator" : ":",
      "scala.usejavacp" : "true",
      "sun.arch.data.model" : "64",
      "sun.boot.class.path" : "/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/classes",
      "sun.boot.library.path" : "/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/amd64",
      "sun.cpu.endian" : "little",
      "sun.cpu.isalist" : "",
      "sun.io.unicode.encoding" : "UnicodeLittle",
      "sun.java.command" : "org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.sql.sources.partitionOverwriteMode=dynamic --conf spark.executor.memory=10240m --conf spark.sql.catalog.hive_pluto=org.apache.iceberg.spark.SparkCatalog --conf spark.driver.memory=5120m --conf spark.sql.catalog.hive_pluto.uri=thrift://10.116.17.2:9083 --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.hive_pluto.type=hive --conf spark.benchmarkId=20231130-150211-tpcds-1gb-iceberg-load --conf spark.executor.extraJavaOptions=-XX:MaxDirectMemorySize=8096m -Dio.netty.maxDirectMemory=8096m -XX:+UseG1GC -XX:ConcGCThreads=2 --conf spark.hadoop.fs.s3.useRequesterPaysHeader=true --conf spark.delta.logStore.gs.impl=io.delta.storage.GCSLogStore --class org.apache.spark.repl.Main --name Spark shell --queue de_adhoc --jars /home/vanshika.yadav/20231130-150211-tpcds-1gb-iceberg-load-benchmarks.jar,/var/lib/fk-pf-spark3/jars/iceberg-hive-runtime-1.2.0.jar,/var/lib/fk-pf-spark3/jars/iceberg-spark-runtime-3.1_2.12-1.2.0.jar spark-shell -I 20231130-150211-tpcds-1gb-iceberg-load_shell_init.scala",
      "sun.java.launcher" : "SUN_STANDARD",
      "sun.jnu.encoding" : "ANSI_X3.4-1968",
      "sun.management.compiler" : "HotSpot 64-Bit Tiered Compilers",
      "sun.nio.ch.bugLevel" : "",
      "sun.os.patch.level" : "unknown",
      "user.country" : "US",
      "user.dir" : "/home/vanshika.yadav",
      "user.home" : "/home/vanshika.yadav",
      "user.language" : "en",
      "user.name" : "vanshika.yadav",
      "user.timezone" : "Asia/Kolkata"
    }
  }
}
Copying file:///home/vanshika.yadav/20231130-150211-tpcds-1gb-iceberg-load-report.json [Content-Type=application/json]...
/ [0 files][    0.0 B/ 65.8 KiB]                                                / [1 files][ 65.8 KiB/ 65.8 KiB]                                                
Operation completed over 1 objects/65.8 KiB.                                     
FILE UPLOAD: Uploaded /home/vanshika.yadav/20231130-150211-tpcds-1gb-iceberg-load-report.json to gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/reports/json/
Copying file:///home/vanshika.yadav/20231130-150211-tpcds-1gb-iceberg-load-report.csv [Content-Type=text/csv]...
/ [0 files][    0.0 B/  1.5 KiB]                                                / [1 files][  1.5 KiB/  1.5 KiB]                                                
Operation completed over 1 objects/1.5 KiB.                                      
FILE UPLOAD: Uploaded /home/vanshika.yadav/20231130-150211-tpcds-1gb-iceberg-load-report.csv to gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/reports/csv/
SUCCESS
