Listening for transport dt_socket at address: 33027
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/12/18 19:22:29 WARN DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
23/12/18 19:22:34 WARN DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
23/12/18 19:22:46 WARN DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
23/12/18 19:22:49 WARN DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
23/12/18 19:22:54 WARN DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
Spark context Web UI available at http://fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:4040
Spark context available as 'sc' (master = yarn, app id = application_1699449121496_0763).
Spark session available as 'spark'.
20231218-192117-tpcds-100gb-iceberg-load_shell_init.scala:22: warning: This catches all Throwables. If this is really intended, use `case t : Throwable` to clear this warning.
try { benchmark.TPCDSDataLoad.main(Array[String]("--format", "ICEBERG", "--scale-in-gb", "100", "--exclude-nulls", "True", "--benchmark-path", "hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking", "--source-path", "gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb")) } catch { case t => println(t); println("FAILED"); System.exit(1) } ;;
                                                                                                                                                                                                                                                                                                                                      ^
2023-12-18T19:23:10.179 ================================================================================
2023-12-18T19:23:10.179 ================================================================================
23/12/18 19:23:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
2023-12-18T19:23:11.009 Spark started with configuration:
	spark.app.id: application_1699449121496_0763
	spark.app.name: Spark shell
	spark.app.startTime: 1702907543468
	spark.app.submitTime: 1702907539300
	spark.benchmarkId: 20231218-192117-tpcds-100gb-iceberg-load
	spark.cleaner.ttl: 86400
	spark.delta.logStore.gs.impl: io.delta.storage.GCSLogStore
	spark.driver.appUIAddress: http://fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:4040
	spark.driver.cores: 1
	spark.driver.extraClassPath: gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar
	spark.driver.extraJavaOptions: -Denv=prod -Dcom.sun.management.jmxremote.port=0 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=0 -XX:MaxDirectMemorySize=800M -XX:MaxMetaspaceSize=256M -XX:CompressedClassSpaceSize=100M -XX:+UnlockDiagnosticVMOptions -Djob.numOfRePartitions=30
	spark.driver.extraLibraryPath: /usr/flipkart/3.2.1-1/hadoop-hdfs/lib:/usr/flipkart/3.2.1-1/hadoop-hdfs/lib/native:/usr/lib/hadoop/lib:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/lib/x86_64-linux-gnu
	spark.driver.host: fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal
	spark.driver.memory: 5120m
	spark.driver.memoryOverhead: 4096
	spark.driver.port: 43135
	spark.dynamicAllocation.enabled: true
	spark.dynamicAllocation.executorIdleTimeout: 60s
	spark.dynamicAllocation.maxExecutors: 200
	spark.eventLog.dir: gs://fks-fdp-infra-job-history/pluto/spark-job-history
	spark.eventLog.enabled: true
	spark.executor.cores: 1
	spark.executor.extraClassPath: gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar
	spark.executor.extraJavaOptions: -XX:MaxDirectMemorySize=8096m -Dio.netty.maxDirectMemory=8096m -XX:+UseG1GC -XX:ConcGCThreads=2
	spark.executor.extraLibraryPath: /usr/flipkart/3.2.1-1/hadoop-hdfs/lib:/usr/flipkart/3.2.1-1/hadoop-hdfs/lib/native:/usr/lib/hadoop/lib:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/lib/x86_64-linux-gnu
	spark.executor.id: driver
	spark.executor.memory: 10240m
	spark.executor.memoryOverhead: 4096
	spark.fdp.orgqueue.cache.expire: 300
	spark.fdp.orgqueue.cache.size: 100
	spark.fdp.orgqueue.defaultQueue: adhoc
	spark.fdp.orgqueue.gringotts.clientId: QAAS
	spark.fdp.orgqueue.gringotts.clientSecret: 423de2b0-cc97-439d-a3f9-673e76d7bbea
	spark.fdp.orgqueue.gringotts.url: http://gringotts.fdp-gringotts-prod.fkcloud.in/billingOrg/user
	spark.fdp.orgqueue.ironbank.url: http://console.fdp-ironbank-prod.fkcloud.in/queue
	spark.fdp.orgqueue.queueNotFound.errorMessage: Queue mapping not found
	spark.fdp.orgqueue.validInitiators: BADGER,QAAS
	spark.hadoop.fs.s3.useRequesterPaysHeader: true
	spark.hadoop.yarn.timeline-service.enabled: false
	spark.history.fs.cleaner.interval: 1d
	spark.history.fs.cleaner.maxAge: 60d
	spark.history.fs.logDirectory: gs://fks-fdp-infra-job-history/pluto/spark-job-history
	spark.history.provider: org.apache.spark.deploy.history.FsHistoryProvider
	spark.home: /var/lib/fk-pf-spark3-4
	spark.jars: 
	spark.master: yarn
	spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS: fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal,fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal
	spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES: http://fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal:8088/proxy/application_1699449121496_0763,http://fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal:8088/proxy/application_1699449121496_0763
	spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.RM_HA_URLS: fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal:8088,fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal:8088
	spark.queue.enforcer.class: com.flipkart.fdp.orgqueue.OrgQueueEnforcerForInitiator
	spark.repl.class.outputDir: /grid/1/spark3-4/tmp/spark-a3663513-5f27-4905-bf52-03c680aeae93/repl-bf162d39-3be6-4e96-8df1-6d1260105b27
	spark.repl.class.uri: spark://fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:43135/classes
	spark.repl.local.jars: file:///home/vanshika.yadav/20231218-192117-tpcds-100gb-iceberg-load-benchmarks.jar,file:///var/lib/fk-pf-spark3-4/jars/iceberg-spark-runtime-3.4_2.12-1.4.0.jar
	spark.shuffle.service.enabled: true
	spark.shuffle.useOldFetchProtocol: true
	spark.sql.catalog.hive_pluto: org.apache.iceberg.spark.SparkCatalog
	spark.sql.catalog.hive_pluto.type: hive
	spark.sql.catalog.hive_pluto.uri: thrift://10.116.17.2:9083
	spark.sql.catalogImplementation: hive
	spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
	spark.sql.sources.partitionOverwriteMode: dynamic
	spark.sql.warehouse.dir: gs://stage-hive-metastore-pluto/apps/hive/warehouse
	spark.streaming.concurrentJobs: 4
	spark.submit.deployMode: client
	spark.submit.pyFiles: 
	spark.ui.filters: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
	spark.ui.proxyBase: 
	spark.ui.showConsoleProgress: true
	spark.yarn.dist.jars: gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar,file:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar,gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/json-serde-1.3-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/dimlookup-hive-udf-1.0-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,file:///home/vanshika.yadav/20231218-192117-tpcds-100gb-iceberg-load-benchmarks.jar,file:///var/lib/fk-pf-spark3-4/jars/iceberg-spark-runtime-3.4_2.12-1.4.0.jar
	spark.yarn.historyServer.address: http://fks-fdp-galaxy-pluto-hs-0001-m.c.fks-fdp-galaxy.internal:18080
	spark.yarn.jars: gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/spark3-4/jars/*
	spark.yarn.queue: de_adhoc
	spark.yarn.report.interval: 60s
	spark.yarn.secondary.jars: hadoop-lzo-0.6.0.2.4.0.0-169.jar,hadoop-4mc-1.1.0.jar,json-serde-1.3-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,dimlookup-hive-udf-1.0-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,20231218-192117-tpcds-100gb-iceberg-load-benchmarks.jar,iceberg-spark-runtime-3.4_2.12-1.4.0.jar

2023-12-18T19:23:11.014 ================================================================================
2023-12-18T19:23:11.015 START: drop-database
2023-12-18T19:23:11.015 SQL: DROP DATABASE IF EXISTS tpcds_sf100_ICEBERG CASCADE
23/12/18 19:23:12 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic
2023-12-18T19:23:12.775 END took 1758 ms: drop-database
2023-12-18T19:23:12.775 ================================================================================
2023-12-18T19:23:12.775 ================================================================================
2023-12-18T19:23:12.775 START: create-database
2023-12-18T19:23:12.775 SQL: CREATE DATABASE IF NOT EXISTS tpcds_sf100_ICEBERG
2023-12-18T19:23:12.994 END took 219 ms: create-database
2023-12-18T19:23:12.994 ================================================================================
2023-12-18T19:23:12.996 Generating call_center at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/call_center
2023-12-18T19:23:12.996 ================================================================================
2023-12-18T19:23:12.996 START: drop-table-call_center
2023-12-18T19:23:12.996 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`call_center`
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.exec.orc.default.compress does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.vectorized.complex.types.enabled does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.exec.orc.compression.strategy does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.enforce.sorting does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.metastore.client.scheme.handlers does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.auto.convert.sortmerge.join.noconditionaltask does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.vectorized.execution.ptf.enabled does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.support.sql11.reserved.keywords does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.vectorized.groupby.complex.types.enabled does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.heapsize does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.enforce.bucketing does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.fileformat.managed does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.optimize.mapjoin.mapreduce does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.exec.orc.default.stripe.size does not exist
23/12/18 19:23:13 WARN HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
2023-12-18T19:23:13.434 END took 437 ms: drop-table-call_center
2023-12-18T19:23:13.434 ================================================================================
2023-12-18T19:23:13.434 ================================================================================
2023-12-18T19:23:13.434 START: create-table-call_center
2023-12-18T19:23:13.434 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`call_center` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/call_center/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/call_center_100gb_parquet`  
[Stage 0:>                                                          (0 + 0) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                23/12/18 19:23:28 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[Stage 1:>                                                          (0 + 1) / 1]                                                                                2023-12-18T19:23:32.404 END took 18970 ms: create-table-call_center
2023-12-18T19:23:32.405 ================================================================================
2023-12-18T19:23:33.889 Generating catalog_page at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/catalog_page
2023-12-18T19:23:33.889 ================================================================================
2023-12-18T19:23:33.889 START: drop-table-catalog_page
2023-12-18T19:23:33.889 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`catalog_page`
2023-12-18T19:23:33.907 END took 17 ms: drop-table-catalog_page
2023-12-18T19:23:33.907 ================================================================================
2023-12-18T19:23:33.907 ================================================================================
2023-12-18T19:23:33.907 START: create-table-catalog_page
2023-12-18T19:23:33.907 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`catalog_page` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/catalog_page/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/catalog_page_100gb_parquet`  
[Stage 10:>                                                       (0 + 1) / 141][Stage 10:>                                                       (0 + 2) / 141][Stage 10:>                                                       (0 + 3) / 141][Stage 10:>                                                       (0 + 5) / 141][Stage 10:>                                                       (0 + 6) / 141][Stage 10:>                                                       (1 + 9) / 141][Stage 10:>                                                      (1 + 10) / 141][Stage 10:>                                                      (1 + 16) / 141][Stage 10:>                                                      (1 + 22) / 141][Stage 10:>                                                      (1 + 24) / 141][Stage 10:>                                                      (1 + 33) / 141][Stage 10:>                                                      (1 + 44) / 141][Stage 10:>                                                      (1 + 54) / 141][Stage 10:>                                                      (1 + 59) / 141][Stage 10:>                                                      (1 + 65) / 141][Stage 10:>                                                      (1 + 66) / 141][Stage 10:>                                                      (2 + 76) / 141][Stage 10:>                                                      (2 + 80) / 141][Stage 10:>                                                      (2 + 96) / 141][Stage 10:=>                                                    (3 + 125) / 141][Stage 10:=>                                                    (3 + 138) / 141][Stage 10:=>                                                    (4 + 137) / 141][Stage 10:=>                                                    (5 + 136) / 141][Stage 10:==>                                                   (6 + 135) / 141][Stage 10:===>                                                  (9 + 132) / 141][Stage 10:===>                                                 (10 + 131) / 141][Stage 10:====>                                                (11 + 130) / 141][Stage 10:====>                                                (13 + 128) / 141][Stage 10:======>                                              (17 + 124) / 141][Stage 10:======>                                              (18 + 123) / 141][Stage 10:========>                                            (22 + 119) / 141][Stage 10:========>                                            (23 + 118) / 141][Stage 10:=========>                                           (24 + 117) / 141][Stage 10:=========>                                           (25 + 116) / 141][Stage 10:==========>                                          (27 + 114) / 141][Stage 10:==========>                                          (29 + 112) / 141][Stage 10:===========>                                         (31 + 110) / 141][Stage 10:============>                                        (32 + 109) / 141][Stage 10:=============>                                       (35 + 106) / 141][Stage 10:================>                                     (43 + 98) / 141][Stage 10:===================>                                  (52 + 89) / 141][Stage 10:=======================>                              (61 + 80) / 141][Stage 10:========================>                             (63 + 78) / 141][Stage 10:=========================>                            (66 + 75) / 141][Stage 10:==========================>                           (68 + 73) / 141][Stage 10:===========================>                          (71 + 70) / 141][Stage 10:==============================>                       (79 + 62) / 141][Stage 10:================================>                     (86 + 55) / 141][Stage 10:==================================>                   (89 + 52) / 141][Stage 10:===================================>                  (92 + 49) / 141][Stage 10:====================================>                 (94 + 47) / 141][Stage 10:====================================>                 (96 + 45) / 141][Stage 10:=====================================>               (100 + 41) / 141][Stage 10:=======================================>             (105 + 36) / 141][Stage 10:========================================>            (107 + 34) / 141][Stage 10:=========================================>           (110 + 31) / 141][Stage 10:===========================================>         (117 + 24) / 141][Stage 10:==============================================>      (125 + 16) / 141][Stage 10:================================================>    (130 + 11) / 141][Stage 10:==================================================>   (132 + 9) / 141][Stage 10:==================================================>   (133 + 8) / 141][Stage 10:===================================================>  (134 + 7) / 141][Stage 10:===================================================>  (135 + 6) / 141][Stage 10:====================================================> (136 + 5) / 141][Stage 10:====================================================> (137 + 4) / 141][Stage 10:=====================================================>(139 + 2) / 141][Stage 10:=====================================================>(140 + 1) / 141]                                                                                2023-12-18T19:24:10.326 END took 36418 ms: create-table-catalog_page
2023-12-18T19:24:10.326 ================================================================================
[Stage 12:==============================================>      (125 + 16) / 141][Stage 12:===============================================>     (126 + 15) / 141][Stage 12:================================================>    (128 + 13) / 141][Stage 12:===================================================>  (135 + 6) / 141][Stage 12:=====================================================>(139 + 2) / 141][Stage 12:=====================================================>(140 + 1) / 141]                                                                                2023-12-18T19:24:14.087 Generating catalog_returns at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/catalog_returns
2023-12-18T19:24:14.087 ================================================================================
2023-12-18T19:24:14.087 START: drop-table-catalog_returns
2023-12-18T19:24:14.087 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`catalog_returns`
2023-12-18T19:24:14.101 END took 13 ms: drop-table-catalog_returns
2023-12-18T19:24:14.101 ================================================================================
2023-12-18T19:24:14.101 ================================================================================
2023-12-18T19:24:14.101 START: create-table-catalog_returns
2023-12-18T19:24:14.101 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`catalog_returns` USING ICEBERG PARTITIONED BY (cr_returned_date_sk)  LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/catalog_returns/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/catalog_returns_100gb_parquet` WHERE cr_returned_date_sk IS NOT NULL 
[Stage 19:=====================================>                (98 + 42) / 140][Stage 19:================================================>    (128 + 12) / 140][Stage 19:================================================>    (129 + 11) / 140][Stage 19:=================================================>   (130 + 10) / 140][Stage 19:==================================================>   (131 + 9) / 140][Stage 19:==================================================>   (132 + 8) / 140][Stage 19:===================================================>  (133 + 7) / 140][Stage 19:===================================================>  (134 + 6) / 140][Stage 19:====================================================> (135 + 5) / 140][Stage 19:====================================================> (136 + 4) / 140][Stage 19:====================================================> (137 + 3) / 140][Stage 19:=====================================================>(138 + 2) / 140]                                                                                23/12/18 19:24:52 WARN DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
2023-12-18T19:24:52.788 END took 38686 ms: create-table-catalog_returns
2023-12-18T19:24:52.788 ================================================================================
2023-12-18T19:24:54.475 Generating catalog_sales at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/catalog_sales
2023-12-18T19:24:54.475 ================================================================================
2023-12-18T19:24:54.475 START: drop-table-catalog_sales
2023-12-18T19:24:54.475 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`catalog_sales`
2023-12-18T19:24:54.493 END took 17 ms: drop-table-catalog_sales
2023-12-18T19:24:54.493 ================================================================================
2023-12-18T19:24:54.493 ================================================================================
2023-12-18T19:24:54.493 START: create-table-catalog_sales
2023-12-18T19:24:54.493 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`catalog_sales` USING ICEBERG PARTITIONED BY (cs_sold_date_sk)  LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/catalog_sales/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/catalog_sales_100gb_parquet` WHERE cs_sold_date_sk IS NOT NULL 
[Stage 28:==========>                                          (29 + 115) / 144][Stage 28:============>                                        (34 + 110) / 144][Stage 28:==============>                                      (40 + 104) / 144][Stage 28:==================>                                   (49 + 95) / 144][Stage 28:====================>                                 (54 + 90) / 144][Stage 28:====================>                                 (55 + 89) / 144][Stage 28:=====================>                                (56 + 88) / 144][Stage 28:=====================>                                (57 + 87) / 144][Stage 28:=====================>                                (58 + 86) / 144][Stage 28:======================>                               (59 + 85) / 144][Stage 28:======================>                               (61 + 83) / 144][Stage 28:=======================>                              (62 + 82) / 144][Stage 28:========================>                             (64 + 80) / 144][Stage 28:========================>                             (65 + 79) / 144][Stage 28:=========================>                            (67 + 77) / 144][Stage 28:=========================>                            (69 + 75) / 144][Stage 28:============================>                         (76 + 68) / 144][Stage 28:================================>                     (86 + 58) / 144][Stage 28:====================================>                 (96 + 48) / 144][Stage 28:=====================================>               (103 + 41) / 144][Stage 28:========================================>            (110 + 34) / 144][Stage 28:==========================================>          (116 + 28) / 144][Stage 28:===========================================>         (119 + 25) / 144][Stage 28:==============================================>      (127 + 17) / 144][Stage 28:==================================================>   (135 + 9) / 144][Stage 28:===================================================>  (137 + 7) / 144][Stage 28:=====================================================>(143 + 1) / 144]                                                                                2023-12-18T19:25:08.658 END took 14164 ms: create-table-catalog_sales
2023-12-18T19:25:08.658 ================================================================================
2023-12-18T19:25:10.077 Generating customer at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/customer
2023-12-18T19:25:10.077 ================================================================================
2023-12-18T19:25:10.077 START: drop-table-customer
2023-12-18T19:25:10.077 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`customer`
2023-12-18T19:25:10.092 END took 15 ms: drop-table-customer
2023-12-18T19:25:10.092 ================================================================================
2023-12-18T19:25:10.092 ================================================================================
2023-12-18T19:25:10.092 START: create-table-customer
2023-12-18T19:25:10.092 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`customer` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/customer/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/customer_100gb_parquet`  
[Stage 37:====================================================>   (34 + 2) / 36][Stage 37:======================================================> (35 + 1) / 36]                                                                                2023-12-18T19:25:18.001 END took 7908 ms: create-table-customer
2023-12-18T19:25:18.001 ================================================================================
2023-12-18T19:25:18.915 Generating customer_address at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/customer_address
2023-12-18T19:25:18.915 ================================================================================
2023-12-18T19:25:18.915 START: drop-table-customer_address
2023-12-18T19:25:18.916 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`customer_address`
2023-12-18T19:25:18.930 END took 14 ms: drop-table-customer_address
2023-12-18T19:25:18.930 ================================================================================
2023-12-18T19:25:18.930 ================================================================================
2023-12-18T19:25:18.930 START: create-table-customer_address
2023-12-18T19:25:18.930 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`customer_address` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/customer_address/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/customer_address_100gb_parquet`  
[Stage 46:===================================================>      (8 + 1) / 9]                                                                                2023-12-18T19:25:23.255 END took 4324 ms: create-table-customer_address
2023-12-18T19:25:23.255 ================================================================================
2023-12-18T19:25:24.191 Generating customer_demographics at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/customer_demographics
2023-12-18T19:25:24.191 ================================================================================
2023-12-18T19:25:24.191 START: drop-table-customer_demographics
2023-12-18T19:25:24.191 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`customer_demographics`
2023-12-18T19:25:24.205 END took 13 ms: drop-table-customer_demographics
2023-12-18T19:25:24.205 ================================================================================
2023-12-18T19:25:24.205 ================================================================================
2023-12-18T19:25:24.205 START: create-table-customer_demographics
2023-12-18T19:25:24.205 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`customer_demographics` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/customer_demographics/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/customer_demographics_100gb_parquet`  
[Stage 55:======================================>                   (2 + 1) / 3]                                                                                2023-12-18T19:25:26.771 END took 2566 ms: create-table-customer_demographics
2023-12-18T19:25:26.771 ================================================================================
2023-12-18T19:25:27.657 Generating date_dim at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/date_dim
2023-12-18T19:25:27.657 ================================================================================
2023-12-18T19:25:27.657 START: drop-table-date_dim
2023-12-18T19:25:27.657 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`date_dim`
2023-12-18T19:25:27.672 END took 14 ms: drop-table-date_dim
2023-12-18T19:25:27.672 ================================================================================
2023-12-18T19:25:27.672 ================================================================================
2023-12-18T19:25:27.672 START: create-table-date_dim
2023-12-18T19:25:27.672 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`date_dim` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/date_dim/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/date_dim_100gb_parquet`  
[Stage 64:>                                                         (0 + 1) / 1]                                                                                2023-12-18T19:25:29.154 END took 1481 ms: create-table-date_dim
2023-12-18T19:25:29.154 ================================================================================
2023-12-18T19:25:30.014 Generating household_demographics at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/household_demographics
2023-12-18T19:25:30.014 ================================================================================
2023-12-18T19:25:30.014 START: drop-table-household_demographics
2023-12-18T19:25:30.014 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`household_demographics`
2023-12-18T19:25:30.027 END took 12 ms: drop-table-household_demographics
2023-12-18T19:25:30.027 ================================================================================
2023-12-18T19:25:30.027 ================================================================================
2023-12-18T19:25:30.027 START: create-table-household_demographics
2023-12-18T19:25:30.027 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`household_demographics` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/household_demographics/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/household_demographics_100gb_parquet`  
2023-12-18T19:25:31.004 END took 976 ms: create-table-household_demographics
2023-12-18T19:25:31.004 ================================================================================
2023-12-18T19:25:31.801 Generating income_band at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/income_band
2023-12-18T19:25:31.801 ================================================================================
2023-12-18T19:25:31.801 START: drop-table-income_band
2023-12-18T19:25:31.801 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`income_band`
2023-12-18T19:25:31.814 END took 12 ms: drop-table-income_band
2023-12-18T19:25:31.814 ================================================================================
2023-12-18T19:25:31.814 ================================================================================
2023-12-18T19:25:31.814 START: create-table-income_band
2023-12-18T19:25:31.814 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`income_band` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/income_band/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/income_band_100gb_parquet`  
2023-12-18T19:25:32.574 END took 760 ms: create-table-income_band
2023-12-18T19:25:32.574 ================================================================================
2023-12-18T19:25:33.414 Generating inventory at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/inventory
2023-12-18T19:25:33.415 ================================================================================
2023-12-18T19:25:33.415 START: drop-table-inventory
2023-12-18T19:25:33.415 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`inventory`
2023-12-18T19:25:33.426 END took 10 ms: drop-table-inventory
2023-12-18T19:25:33.426 ================================================================================
2023-12-18T19:25:33.426 ================================================================================
2023-12-18T19:25:33.426 START: create-table-inventory
2023-12-18T19:25:33.426 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`inventory` USING ICEBERG PARTITIONED BY (inv_date_sk)  LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/inventory/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/inventory_100gb_parquet` WHERE inv_date_sk IS NOT NULL 
[Stage 91:===============================================>     (124 + 15) / 139][Stage 91:===============================================>     (125 + 14) / 139][Stage 91:================================================>    (126 + 13) / 139][Stage 91:=================================================>   (129 + 10) / 139][Stage 91:==================================================>   (130 + 9) / 139][Stage 91:==================================================>   (131 + 8) / 139][Stage 91:===================================================>  (132 + 7) / 139][Stage 91:===================================================>  (133 + 6) / 139][Stage 91:====================================================> (134 + 5) / 139][Stage 91:=====================================================>(137 + 2) / 139]                                                                                2023-12-18T19:25:49.861 END took 16435 ms: create-table-inventory
2023-12-18T19:25:49.861 ================================================================================
2023-12-18T19:25:51.071 Generating item at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/item
2023-12-18T19:25:51.071 ================================================================================
2023-12-18T19:25:51.071 START: drop-table-item
2023-12-18T19:25:51.071 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`item`
2023-12-18T19:25:51.083 END took 11 ms: drop-table-item
2023-12-18T19:25:51.083 ================================================================================
2023-12-18T19:25:51.083 ================================================================================
2023-12-18T19:25:51.083 START: create-table-item
2023-12-18T19:25:51.083 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`item` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/item/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/item_100gb_parquet`  
[Stage 100:================================================>        (6 + 1) / 7]                                                                                2023-12-18T19:25:53.864 END took 2781 ms: create-table-item
2023-12-18T19:25:53.864 ================================================================================
2023-12-18T19:25:54.787 Generating promotion at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/promotion
2023-12-18T19:25:54.788 ================================================================================
2023-12-18T19:25:54.788 START: drop-table-promotion
2023-12-18T19:25:54.788 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`promotion`
2023-12-18T19:25:54.798 END took 10 ms: drop-table-promotion
2023-12-18T19:25:54.798 ================================================================================
2023-12-18T19:25:54.798 ================================================================================
2023-12-18T19:25:54.798 START: create-table-promotion
2023-12-18T19:25:54.798 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`promotion` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/promotion/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/promotion_100gb_parquet`  
2023-12-18T19:25:55.669 END took 870 ms: create-table-promotion
2023-12-18T19:25:55.669 ================================================================================
2023-12-18T19:25:56.447 Generating reason at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/reason
2023-12-18T19:25:56.447 ================================================================================
2023-12-18T19:25:56.447 START: drop-table-reason
2023-12-18T19:25:56.447 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`reason`
2023-12-18T19:25:56.458 END took 11 ms: drop-table-reason
2023-12-18T19:25:56.458 ================================================================================
2023-12-18T19:25:56.458 ================================================================================
2023-12-18T19:25:56.458 START: create-table-reason
2023-12-18T19:25:56.458 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`reason` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/reason/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/reason_100gb_parquet`  
2023-12-18T19:25:57.266 END took 807 ms: create-table-reason
2023-12-18T19:25:57.266 ================================================================================
2023-12-18T19:25:57.964 Generating ship_mode at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/ship_mode
2023-12-18T19:25:57.964 ================================================================================
2023-12-18T19:25:57.964 START: drop-table-ship_mode
2023-12-18T19:25:57.964 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`ship_mode`
2023-12-18T19:25:57.976 END took 12 ms: drop-table-ship_mode
2023-12-18T19:25:57.976 ================================================================================
2023-12-18T19:25:57.976 ================================================================================
2023-12-18T19:25:57.976 START: create-table-ship_mode
2023-12-18T19:25:57.976 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`ship_mode` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/ship_mode/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/ship_mode_100gb_parquet`  
2023-12-18T19:25:58.759 END took 782 ms: create-table-ship_mode
2023-12-18T19:25:58.759 ================================================================================
2023-12-18T19:25:59.460 Generating store at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/store
2023-12-18T19:25:59.460 ================================================================================
2023-12-18T19:25:59.460 START: drop-table-store
2023-12-18T19:25:59.460 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`store`
2023-12-18T19:25:59.471 END took 10 ms: drop-table-store
2023-12-18T19:25:59.471 ================================================================================
2023-12-18T19:25:59.471 ================================================================================
2023-12-18T19:25:59.471 START: create-table-store
2023-12-18T19:25:59.471 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`store` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/store/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/store_100gb_parquet`  
2023-12-18T19:26:00.649 END took 1178 ms: create-table-store
2023-12-18T19:26:00.649 ================================================================================
2023-12-18T19:26:01.303 Generating store_returns at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/store_returns
2023-12-18T19:26:01.303 ================================================================================
2023-12-18T19:26:01.303 START: drop-table-store_returns
2023-12-18T19:26:01.303 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`store_returns`
2023-12-18T19:26:01.315 END took 12 ms: drop-table-store_returns
2023-12-18T19:26:01.315 ================================================================================
2023-12-18T19:26:01.315 ================================================================================
2023-12-18T19:26:01.315 START: create-table-store_returns
2023-12-18T19:26:01.315 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`store_returns` USING ICEBERG PARTITIONED BY (sr_returned_date_sk)  LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/store_returns/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/store_returns_100gb_parquet` WHERE sr_returned_date_sk IS NOT NULL 
[Stage 145:==============================================>     (124 + 16) / 140][Stage 145:==============================================>     (125 + 15) / 140][Stage 145:==============================================>     (126 + 14) / 140][Stage 145:===============================================>    (127 + 13) / 140][Stage 145:===============================================>    (128 + 12) / 140][Stage 145:===============================================>    (129 + 11) / 140][Stage 145:================================================>   (130 + 10) / 140][Stage 145:=================================================>   (132 + 8) / 140][Stage 145:==================================================>  (133 + 7) / 140][Stage 145:==================================================>  (134 + 6) / 140][Stage 145:===================================================> (135 + 5) / 140][Stage 145:===================================================> (137 + 3) / 140][Stage 145:====================================================>(138 + 2) / 140][Stage 145:====================================================>(139 + 1) / 140]                                                                                23/12/18 19:27:17 WARN DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
2023-12-18T19:27:18.171 END took 76855 ms: create-table-store_returns
2023-12-18T19:27:18.171 ================================================================================
2023-12-18T19:27:19.800 Generating store_sales at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/store_sales
2023-12-18T19:27:19.800 ================================================================================
2023-12-18T19:27:19.800 START: drop-table-store_sales
2023-12-18T19:27:19.800 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`store_sales`
2023-12-18T19:27:19.812 END took 11 ms: drop-table-store_sales
2023-12-18T19:27:19.812 ================================================================================
2023-12-18T19:27:19.812 ================================================================================
2023-12-18T19:27:19.812 START: create-table-store_sales
2023-12-18T19:27:19.812 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`store_sales` USING ICEBERG PARTITIONED BY (ss_sold_date_sk)  LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/store_sales/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/store_sales_100gb_parquet` WHERE ss_sold_date_sk IS NOT NULL 
[Stage 154:>                                                     (0 + 16) / 133][Stage 154:>                                                     (0 + 17) / 133][Stage 154:>                                                     (0 + 18) / 133][Stage 154:>                                                     (0 + 19) / 133][Stage 154:>                                                     (0 + 20) / 133][Stage 154:>                                                     (0 + 23) / 133][Stage 154:>                                                     (0 + 24) / 133][Stage 154:>                                                     (0 + 25) / 133][Stage 154:>                                                     (0 + 27) / 133][Stage 154:>                                                     (0 + 32) / 133][Stage 154:>                                                     (0 + 33) / 133][Stage 154:>                                                     (0 + 35) / 133][Stage 154:>                                                     (0 + 38) / 133][Stage 154:>                                                     (0 + 48) / 133][Stage 154:>                                                     (0 + 53) / 133][Stage 154:>                                                     (0 + 59) / 133][Stage 154:>                                                     (0 + 60) / 133][Stage 154:>                                                     (0 + 69) / 133][Stage 154:>                                                     (0 + 70) / 133][Stage 154:>                                                     (0 + 77) / 133][Stage 154:>                                                     (0 + 97) / 133][Stage 154:>                                                    (0 + 111) / 133][Stage 154:>                                                    (0 + 114) / 133][Stage 154:>                                                    (0 + 122) / 133][Stage 154:>                                                    (0 + 126) / 133][Stage 154:>                                                    (0 + 133) / 133][Stage 154:>                                                    (0 + 133) / 133][Stage 154:>                                                    (1 + 132) / 133][Stage 154:>                                                    (2 + 131) / 133][Stage 154:=>                                                   (3 + 130) / 133][Stage 154:=>                                                   (4 + 129) / 133][Stage 154:=>                                                   (5 + 128) / 133][Stage 154:==>                                                  (7 + 126) / 133][Stage 154:===>                                                 (8 + 125) / 133][Stage 154:===>                                                 (9 + 124) / 133][Stage 154:===>                                                (10 + 123) / 133][Stage 154:====>                                               (11 + 122) / 133][Stage 154:====>                                               (12 + 121) / 133][Stage 154:=====>                                              (13 + 120) / 133][Stage 154:=====>                                              (14 + 119) / 133][Stage 154:=====>                                              (15 + 118) / 133][Stage 154:======>                                             (16 + 117) / 133][Stage 154:======>                                             (17 + 116) / 133][Stage 154:=======>                                            (18 + 115) / 133][Stage 154:=======>                                            (19 + 114) / 133][Stage 154:=======>                                            (20 + 113) / 133][Stage 154:========>                                           (21 + 112) / 133][Stage 154:========>                                           (22 + 111) / 133][Stage 154:========>                                           (23 + 110) / 133][Stage 154:=========>                                          (24 + 109) / 133][Stage 154:==========>                                         (26 + 107) / 133][Stage 154:==========>                                         (27 + 106) / 133][Stage 154:===========>                                        (30 + 103) / 133][Stage 154:============>                                       (31 + 102) / 133][Stage 154:============>                                       (32 + 101) / 133][Stage 154:============>                                       (33 + 100) / 133][Stage 154:===============>                                     (39 + 94) / 133][Stage 154:===============>                                     (40 + 93) / 133][Stage 154:================>                                    (42 + 91) / 133][Stage 154:=================>                                   (44 + 89) / 133][Stage 154:==================>                                  (47 + 86) / 133][Stage 154:===================>                                 (48 + 85) / 133][Stage 154:====================>                                (52 + 81) / 133][Stage 154:=====================>                               (53 + 80) / 133][Stage 154:=====================>                               (54 + 79) / 133]23/12/18 19:29:51 ERROR TransportClient: Failed to send RPC RPC 8596791751973957940 to /10.116.3.208:42638: io.netty.channel.StacklessClosedChannelException
io.netty.channel.StacklessClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)
[Stage 154:=====================>                               (55 + 78) / 133]23/12/18 19:29:51 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 163 from block manager BlockManagerId(126, pluto-mig-adhoc-c-cimage-c69c6e1d-ps9r.c.fks-fdp-galaxy.internal, 29413, None)
java.io.IOException: Failed to send RPC RPC 8596791751973957940 to /10.116.3.208:42638: io.netty.channel.StacklessClosedChannelException
	at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:395)
	at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:372)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)
	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)
	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:877)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:940)
	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1247)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.netty.channel.StacklessClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)
[Stage 154:=========================>                           (64 + 69) / 133][Stage 154:==========================>                          (66 + 67) / 133][Stage 154:===========================>                         (70 + 63) / 133][Stage 154:============================>                        (71 + 62) / 133][Stage 154:=============================>                       (73 + 60) / 133][Stage 154:=============================>                       (75 + 58) / 133][Stage 154:==============================>                      (76 + 57) / 133][Stage 154:================================>                    (81 + 52) / 133][Stage 154:==================================>                  (87 + 46) / 133][Stage 154:======================================>              (97 + 36) / 133][Stage 154:==========================================>         (109 + 24) / 133][Stage 154:===========================================>        (110 + 23) / 133][Stage 154:===============================================>    (121 + 12) / 133][Stage 154:================================================>   (123 + 10) / 133]                                                                                23/12/18 19:30:04 WARN DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
23/12/18 19:30:07 WARN DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
2023-12-18T19:30:08.355 END took 168542 ms: create-table-store_sales
2023-12-18T19:30:08.355 ================================================================================
[Stage 156:===================================>                 (89 + 44) / 133][Stage 156:=============================================>      (116 + 17) / 133][Stage 156:===================================================> (130 + 3) / 133]                                                                                2023-12-18T19:30:14.843 Generating time_dim at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/time_dim
2023-12-18T19:30:14.843 ================================================================================
2023-12-18T19:30:14.843 START: drop-table-time_dim
2023-12-18T19:30:14.843 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`time_dim`
2023-12-18T19:30:14.855 END took 11 ms: drop-table-time_dim
2023-12-18T19:30:14.855 ================================================================================
2023-12-18T19:30:14.855 ================================================================================
2023-12-18T19:30:14.855 START: create-table-time_dim
2023-12-18T19:30:14.855 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`time_dim` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/time_dim/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/time_dim_100gb_parquet`  
2023-12-18T19:30:16.091 END took 1236 ms: create-table-time_dim
2023-12-18T19:30:16.091 ================================================================================
2023-12-18T19:30:17.070 Generating warehouse at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/warehouse
2023-12-18T19:30:17.070 ================================================================================
2023-12-18T19:30:17.070 START: drop-table-warehouse
2023-12-18T19:30:17.070 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`warehouse`
2023-12-18T19:30:17.081 END took 11 ms: drop-table-warehouse
2023-12-18T19:30:17.081 ================================================================================
2023-12-18T19:30:17.081 ================================================================================
2023-12-18T19:30:17.081 START: create-table-warehouse
2023-12-18T19:30:17.081 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`warehouse` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/warehouse/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/warehouse_100gb_parquet`  
2023-12-18T19:30:17.997 END took 915 ms: create-table-warehouse
2023-12-18T19:30:17.997 ================================================================================
2023-12-18T19:30:19.038 Generating web_page at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/web_page
2023-12-18T19:30:19.038 ================================================================================
2023-12-18T19:30:19.038 START: drop-table-web_page
2023-12-18T19:30:19.038 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`web_page`
2023-12-18T19:30:19.050 END took 11 ms: drop-table-web_page
2023-12-18T19:30:19.050 ================================================================================
2023-12-18T19:30:19.050 ================================================================================
2023-12-18T19:30:19.050 START: create-table-web_page
2023-12-18T19:30:19.050 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`web_page` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/web_page/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/web_page_100gb_parquet`  
2023-12-18T19:30:20.256 END took 1206 ms: create-table-web_page
2023-12-18T19:30:20.256 ================================================================================
2023-12-18T19:30:21.170 Generating web_returns at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/web_returns
2023-12-18T19:30:21.170 ================================================================================
2023-12-18T19:30:21.170 START: drop-table-web_returns
2023-12-18T19:30:21.170 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`web_returns`
2023-12-18T19:30:21.180 END took 10 ms: drop-table-web_returns
2023-12-18T19:30:21.180 ================================================================================
2023-12-18T19:30:21.180 ================================================================================
2023-12-18T19:30:21.181 START: create-table-web_returns
2023-12-18T19:30:21.181 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`web_returns` USING ICEBERG PARTITIONED BY (wr_returned_date_sk)  LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/web_returns/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/web_returns_100gb_parquet` WHERE wr_returned_date_sk IS NOT NULL 
[Stage 190:==================================================>  (111 + 6) / 117][Stage 190:==================================================>  (112 + 5) / 117][Stage 190:===================================================> (113 + 4) / 117][Stage 190:===================================================> (114 + 3) / 117][Stage 190:====================================================>(115 + 2) / 117][Stage 190:====================================================>(116 + 1) / 117]                                                                                2023-12-18T19:31:39.848 END took 78667 ms: create-table-web_returns
2023-12-18T19:31:39.848 ================================================================================
2023-12-18T19:31:41.153 Generating web_sales at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/web_sales
2023-12-18T19:31:41.153 ================================================================================
2023-12-18T19:31:41.153 START: drop-table-web_sales
2023-12-18T19:31:41.153 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`web_sales`
2023-12-18T19:31:41.163 END took 10 ms: drop-table-web_sales
2023-12-18T19:31:41.163 ================================================================================
2023-12-18T19:31:41.163 ================================================================================
2023-12-18T19:31:41.163 START: create-table-web_sales
2023-12-18T19:31:41.163 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`web_sales` USING ICEBERG PARTITIONED BY (ws_sold_date_sk)  LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/web_sales/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/web_sales_100gb_parquet` WHERE ws_sold_date_sk IS NOT NULL 
[Stage 199:>                                                       (0 + 5) / 43][Stage 199:>                                                       (0 + 6) / 43][Stage 199:>                                                       (0 + 8) / 43][Stage 199:>                                                      (0 + 16) / 43][Stage 199:>                                                      (0 + 18) / 43][Stage 199:>                                                      (0 + 26) / 43][Stage 199:>                                                      (0 + 28) / 43][Stage 199:>                                                      (0 + 32) / 43][Stage 199:>                                                      (0 + 40) / 43][Stage 199:>                                                      (0 + 43) / 43][Stage 199:=>                                                     (1 + 42) / 43][Stage 199:==>                                                    (2 + 41) / 43][Stage 199:===>                                                   (3 + 40) / 43][Stage 199:=====>                                                 (4 + 39) / 43][Stage 199:======>                                                (5 + 38) / 43][Stage 199:=======>                                               (6 + 37) / 43][Stage 199:========>                                              (7 + 36) / 43][Stage 199:==========>                                            (8 + 35) / 43][Stage 199:===========>                                           (9 + 34) / 43][Stage 199:============>                                         (10 + 33) / 43][Stage 199:=============>                                        (11 + 32) / 43][Stage 199:================>                                     (13 + 30) / 43][Stage 199:==================>                                   (15 + 28) / 43][Stage 199:=====================>                                (17 + 26) / 43][Stage 199:======================>                               (18 + 25) / 43][Stage 199:==========================>                           (21 + 22) / 43][Stage 199:===========================>                          (22 + 21) / 43][Stage 199:============================>                         (23 + 20) / 43][Stage 199:==============================>                       (24 + 19) / 43][Stage 199:===============================>                      (25 + 18) / 43][Stage 199:================================>                     (26 + 17) / 43][Stage 199:======================================>               (31 + 12) / 43][Stage 199:========================================>             (32 + 11) / 43][Stage 199:=========================================>            (33 + 10) / 43][Stage 199:===========================================>           (34 + 9) / 43][Stage 199:============================================>          (35 + 8) / 43][Stage 199:===================================================>   (40 + 3) / 43][Stage 199:====================================================>  (41 + 2) / 43][Stage 199:=====================================================> (42 + 1) / 43]                                                                                23/12/18 19:33:21 WARN DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
2023-12-18T19:33:24.297 END took 103133 ms: create-table-web_sales
2023-12-18T19:33:24.297 ================================================================================
[Stage 201:========================================>             (32 + 11) / 43]                                                                                2023-12-18T19:33:28.196 Generating web_site at hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/web_site
2023-12-18T19:33:28.196 ================================================================================
2023-12-18T19:33:28.196 START: drop-table-web_site
2023-12-18T19:33:28.196 SQL: DROP TABLE IF EXISTS `hive_pluto`.`tpcds_sf100_ICEBERG`.`web_site`
2023-12-18T19:33:28.206 END took 10 ms: drop-table-web_site
2023-12-18T19:33:28.206 ================================================================================
2023-12-18T19:33:28.206 ================================================================================
2023-12-18T19:33:28.206 START: create-table-web_site
2023-12-18T19:33:28.206 SQL: CREATE TABLE `hive_pluto`.`tpcds_sf100_ICEBERG`.`web_site` USING ICEBERG LOCATION 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load/web_site/' TBLPROPERTIES ('write.spark.fanout.enabled'='true', 'format-version'=2) AS SELECT * FROM `parquet`.`gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb/web_site_100gb_parquet`  
2023-12-18T19:33:29.405 END took 1199 ms: create-table-web_site
2023-12-18T19:33:29.405 ================================================================================
2023-12-18T19:33:30.380 ====== Created all tables in database tpcds_sf100_ICEBERG at 'hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/databases/tpcds_sf100_ICEBERG_20231218_192117_tpcds_100gb_iceberg_load' =======
2023-12-18T19:33:30.380 ================================================================================
2023-12-18T19:33:30.380 START: USE hive_pluto.tpcds_sf100_ICEBERG;
2023-12-18T19:33:30.380 SQL: USE hive_pluto.tpcds_sf100_ICEBERG;
2023-12-18T19:33:30.492 END took 111 ms: USE hive_pluto.tpcds_sf100_ICEBERG;
2023-12-18T19:33:30.492 ================================================================================
2023-12-18T19:33:30.492 ================================================================================
2023-12-18T19:33:30.492 START: SHOW TABLES
2023-12-18T19:33:30.492 SQL: SHOW TABLES
+-------------------+----------------------+-----------+
|namespace          |tableName             |isTemporary|
+-------------------+----------------------+-----------+
|tpcds_sf100_ICEBERG|call_center           |false      |
|tpcds_sf100_ICEBERG|catalog_page          |false      |
|tpcds_sf100_ICEBERG|catalog_returns       |false      |
|tpcds_sf100_ICEBERG|catalog_sales         |false      |
|tpcds_sf100_ICEBERG|customer              |false      |
|tpcds_sf100_ICEBERG|customer_address      |false      |
|tpcds_sf100_ICEBERG|customer_demographics |false      |
|tpcds_sf100_ICEBERG|date_dim              |false      |
|tpcds_sf100_ICEBERG|household_demographics|false      |
|tpcds_sf100_ICEBERG|income_band           |false      |
|tpcds_sf100_ICEBERG|inventory             |false      |
|tpcds_sf100_ICEBERG|item                  |false      |
|tpcds_sf100_ICEBERG|promotion             |false      |
|tpcds_sf100_ICEBERG|reason                |false      |
|tpcds_sf100_ICEBERG|ship_mode             |false      |
|tpcds_sf100_ICEBERG|store                 |false      |
|tpcds_sf100_ICEBERG|store_returns         |false      |
|tpcds_sf100_ICEBERG|store_sales           |false      |
|tpcds_sf100_ICEBERG|time_dim              |false      |
|tpcds_sf100_ICEBERG|warehouse             |false      |
+-------------------+----------------------+-----------+
only showing top 20 rows

2023-12-18T19:33:31.231 END took 293 ms: SHOW TABLES
2023-12-18T19:33:31.231 ================================================================================
2023-12-18T19:33:31.231 ================================================================================
RESULT:
{
  "benchmarkSpecs" : {
    "benchmarkId" : "20231218-192117-tpcds-100gb-iceberg-load",
    "benchmarkPath" : "hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking",
    "excludeNulls" : "true",
    "format" : "ICEBERG",
    "scaleInGB" : "100",
    "sourcePath" : "gs://fksdp-mhora-zrwe-0f09-discoverycontentimpression/benchmarking/iceberg-benchmarking/100gb"
  },
  "extraMetrics" : { },
  "queryResults" : [ {
    "durationMs" : 1758,
    "name" : "drop-database"
  }, {
    "durationMs" : 219,
    "name" : "create-database"
  }, {
    "durationMs" : 437,
    "name" : "drop-table-call_center"
  }, {
    "durationMs" : 18970,
    "name" : "create-table-call_center"
  }, {
    "durationMs" : 17,
    "name" : "drop-table-catalog_page"
  }, {
    "durationMs" : 36418,
    "name" : "create-table-catalog_page"
  }, {
    "durationMs" : 13,
    "name" : "drop-table-catalog_returns"
  }, {
    "durationMs" : 38686,
    "name" : "create-table-catalog_returns"
  }, {
    "durationMs" : 17,
    "name" : "drop-table-catalog_sales"
  }, {
    "durationMs" : 14164,
    "name" : "create-table-catalog_sales"
  }, {
    "durationMs" : 15,
    "name" : "drop-table-customer"
  }, {
    "durationMs" : 7908,
    "name" : "create-table-customer"
  }, {
    "durationMs" : 14,
    "name" : "drop-table-customer_address"
  }, {
    "durationMs" : 4324,
    "name" : "create-table-customer_address"
  }, {
    "durationMs" : 13,
    "name" : "drop-table-customer_demographics"
  }, {
    "durationMs" : 2566,
    "name" : "create-table-customer_demographics"
  }, {
    "durationMs" : 14,
    "name" : "drop-table-date_dim"
  }, {
    "durationMs" : 1481,
    "name" : "create-table-date_dim"
  }, {
    "durationMs" : 12,
    "name" : "drop-table-household_demographics"
  }, {
    "durationMs" : 976,
    "name" : "create-table-household_demographics"
  }, {
    "durationMs" : 12,
    "name" : "drop-table-income_band"
  }, {
    "durationMs" : 760,
    "name" : "create-table-income_band"
  }, {
    "durationMs" : 10,
    "name" : "drop-table-inventory"
  }, {
    "durationMs" : 16435,
    "name" : "create-table-inventory"
  }, {
    "durationMs" : 11,
    "name" : "drop-table-item"
  }, {
    "durationMs" : 2781,
    "name" : "create-table-item"
  }, {
    "durationMs" : 10,
    "name" : "drop-table-promotion"
  }, {
    "durationMs" : 870,
    "name" : "create-table-promotion"
  }, {
    "durationMs" : 11,
    "name" : "drop-table-reason"
  }, {
    "durationMs" : 807,
    "name" : "create-table-reason"
  }, {
    "durationMs" : 12,
    "name" : "drop-table-ship_mode"
  }, {
    "durationMs" : 782,
    "name" : "create-table-ship_mode"
  }, {
    "durationMs" : 10,
    "name" : "drop-table-store"
  }, {
    "durationMs" : 1178,
    "name" : "create-table-store"
  }, {
    "durationMs" : 12,
    "name" : "drop-table-store_returns"
  }, {
    "durationMs" : 76855,
    "name" : "create-table-store_returns"
  }, {
    "durationMs" : 11,
    "name" : "drop-table-store_sales"
  }, {
    "durationMs" : 168542,
    "name" : "create-table-store_sales"
  }, {
    "durationMs" : 11,
    "name" : "drop-table-time_dim"
  }, {
    "durationMs" : 1236,
    "name" : "create-table-time_dim"
  }, {
    "durationMs" : 11,
    "name" : "drop-table-warehouse"
  }, {
    "durationMs" : 915,
    "name" : "create-table-warehouse"
  }, {
    "durationMs" : 11,
    "name" : "drop-table-web_page"
  }, {
    "durationMs" : 1206,
    "name" : "create-table-web_page"
  }, {
    "durationMs" : 10,
    "name" : "drop-table-web_returns"
  }, {
    "durationMs" : 78667,
    "name" : "create-table-web_returns"
  }, {
    "durationMs" : 10,
    "name" : "drop-table-web_sales"
  }, {
    "durationMs" : 103133,
    "name" : "create-table-web_sales"
  }, {
    "durationMs" : 10,
    "name" : "drop-table-web_site"
  }, {
    "durationMs" : 1199,
    "name" : "create-table-web_site"
  }, {
    "durationMs" : 111,
    "name" : ""
  }, {
    "durationMs" : 293,
    "name" : ""
  } ],
  "sparkEnvInfo" : {
    "classpathEntries" : {
      "//fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar" : "System Classpath",
      "/etc/hadoop/conf/" : "System Classpath",
      "/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/conf/" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/HikariCP-2.5.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/JLargeArrays-1.5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/JTransforms-3.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/RoaringBitmap-0.9.38.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/ST4-4.0.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/activation-1.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/aircompressor-0.21.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/algebra_2.12-2.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/annotations-17.0.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/antlr-runtime-3.5.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/antlr4-runtime-4.9.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/aopalliance-repackaged-2.6.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/arpack-3.0.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/arpack_combined_all-0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/arrow-format-11.0.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/arrow-memory-core-11.0.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/arrow-memory-netty-11.0.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/arrow-vector-11.0.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/audience-annotations-0.5.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/avro-1.11.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/avro-ipc-1.11.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/avro-mapred-1.11.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/blas-3.0.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/bonecp-0.8.0.RELEASE.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/breeze-macros_2.12-2.1.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/breeze_2.12-2.1.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/cats-kernel_2.12-2.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/chill-java-0.10.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/chill_2.12-0.10.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-cli-1.5.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-codec-1.15.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-collections-3.2.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-collections4-4.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-compiler-3.1.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-compress-1.22.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-crypto-1.1.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-dbcp-1.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-io-2.11.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-lang-2.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-lang3-3.12.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-logging-1.1.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-math3-3.6.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-pool-1.5.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/commons-text-1.10.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/compress-lzf-1.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/curator-client-2.13.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/curator-framework-2.13.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/curator-recipes-2.13.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/datanucleus-api-jdo-4.2.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/datanucleus-core-4.1.17.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/datanucleus-rdbms-4.1.19.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/delta-core_2.12-2.4.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/derby-10.14.2.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/fdp-deps-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/flatbuffers-java-1.12.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/gcs-connector-hadoop3-2.2.7-shaded.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/gson-2.2.4.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/guava-14.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hadoop-client-api-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hadoop-client-runtime-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hadoop-yarn-server-web-proxy-3.2.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-beeline-2.3.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-cli-2.3.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-common-2.3.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-exec-2.3.9-core.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-jdbc-2.3.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-llap-common-2.3.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-metastore-2.3.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-serde-2.3.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-service-rpc-3.1.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-shims-0.23-2.3.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-shims-2.3.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-shims-common-2.3.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-shims-scheduler-2.3.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hive-storage-api-2.8.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hk2-api-2.6.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hk2-locator-2.6.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hk2-utils-2.6.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/htrace-core4-4.1.0-incubating.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/httpclient-4.4.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/httpcore-4.4.16.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/hudi-spark3.4-bundle_2.12-0.14.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/iceberg-spark-runtime-3.4_2.12-1.4.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/istack-commons-runtime-3.0.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/ivy-2.5.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jackson-annotations-2.14.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jackson-core-2.14.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jackson-core-asl-1.9.13.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jackson-databind-2.14.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jackson-datatype-jsr310-2.14.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jackson-mapper-asl-1.9.13.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jackson-module-scala_2.12-2.14.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jakarta.annotation-api-1.3.5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jakarta.inject-2.6.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jakarta.servlet-api-4.0.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jakarta.validation-api-2.0.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jakarta.ws.rs-api-2.1.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jakarta.xml.bind-api-2.3.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/janino-3.1.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/javassist-3.25.0-GA.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/javax.jdo-3.2.0-m3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/javolution-5.5.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jaxb-runtime-2.3.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jcl-over-slf4j-2.0.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jdo-api-3.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jersey-client-2.36.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jersey-common-2.36.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jersey-container-servlet-2.36.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jersey-container-servlet-core-2.36.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jersey-hk2-2.36.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jersey-server-2.36.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jline-2.14.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/joda-time-2.12.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jodd-core-3.5.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jpam-1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/json-1.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/json4s-ast_2.12-3.7.0-M11.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/json4s-core_2.12-3.7.0-M11.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/json4s-jackson_2.12-3.7.0-M11.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/json4s-scalap_2.12-3.7.0-M11.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jsr305-3.0.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jta-1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/jul-to-slf4j-2.0.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/kryo-shaded-4.0.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/lapack-3.0.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/leveldbjni-all-1.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/libfb303-0.9.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/libthrift-0.12.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/log4j-1.2-api-2.19.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/log4j-api-2.19.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/log4j-core-2.19.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/log4j-slf4j2-impl-2.19.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/lz4-java-1.8.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/metrics-core-4.2.15.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/metrics-graphite-4.2.15.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/metrics-jmx-4.2.15.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/metrics-json-4.2.15.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/metrics-jvm-4.2.15.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/minlog-1.3.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-all-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-buffer-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-codec-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-codec-http-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-codec-http2-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-codec-socks-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-common-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-handler-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-handler-proxy-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-resolver-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-transport-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-transport-classes-epoll-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-transport-classes-kqueue-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-transport-native-epoll-4.1.87.Final-linux-aarch_64.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-transport-native-epoll-4.1.87.Final-linux-x86_64.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-transport-native-kqueue-4.1.87.Final-osx-aarch_64.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-transport-native-kqueue-4.1.87.Final-osx-x86_64.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/netty-transport-native-unix-common-4.1.87.Final.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/objenesis-3.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/opencsv-2.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/orc-core-1.8.5-shaded-protobuf.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/orc-mapreduce-1.8.5-shaded-protobuf.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/orc-shims-1.8.5.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/oro-2.0.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/osgi-resource-locator-1.0.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/paranamer-2.8.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/parquet-column-1.12.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/parquet-common-1.12.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/parquet-encoding-1.12.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/parquet-format-structures-1.12.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/parquet-hadoop-1.12.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/parquet-jackson-1.12.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/pickle-1.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/protobuf-java-2.5.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/py4j-0.10.9.7.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/rocksdbjni-7.9.2.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/scala-collection-compat_2.12-2.7.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/scala-compiler-2.12.17.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/scala-library-2.12.17.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/scala-parser-combinators_2.12-2.1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/scala-reflect-2.12.17.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/scala-xml_2.12-2.1.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/shims-0.9.38.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/slf4j-api-2.0.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/snappy-java-1.1.10.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-catalyst_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-core_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-graphx_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-hive-thriftserver_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-hive_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-kvstore_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-launcher_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-mllib-local_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-mllib_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-network-common_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-network-shuffle_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-repl_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-sketch_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-sql_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-streaming_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-tags_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-unsafe_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spark-yarn_2.12-3.4.1.fdp.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spire-macros_2.12-0.17.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spire-platform_2.12-0.17.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spire-util_2.12-0.17.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/spire_2.12-0.17.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/stax-api-1.0.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/stream-2.9.6.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/super-csv-2.2.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/threeten-extra-1.7.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/tink-1.7.0.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/transaction-api-1.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/univocity-parsers-2.9.1.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/xbean-asm9-shaded-4.22.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/xz-1.9.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/zookeeper-3.6.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/zookeeper-jute-3.6.3.jar" : "System Classpath",
      "/var/lib/fk-pf-spark3-4/jars/zstd-jni-1.5.2-5.jar" : "System Classpath",
      "gs" : "System Classpath"
    },
    "hadoopProps" : {
      "adl.feature.ownerandgroup.enableupn" : "false",
      "adl.http.timeout" : "-1",
      "ambari.hive.db.schema.name" : "hive",
      "datanucleus.cache.level2.type" : "none",
      "datanucleus.connectionPool.maxPoolSize" : "10",
      "datanucleus.schema.autoCreateTables" : "true",
      "dfs.blocksize" : "536870912",
      "dfs.client.failover.proxy.provider.pluto" : "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",
      "dfs.client.failover.random.order" : "true",
      "dfs.client.read.shortcircuit" : "true",
      "dfs.client.read.shortcircuit.streams.cache.size" : "4096",
      "dfs.domain.socket.path" : "/var/lib/hadoop-hdfs/dn_socket",
      "dfs.ha.fencing.ssh.connect-timeout" : "30000",
      "dfs.ha.namenodes.pluto" : "nn1,nn2",
      "dfs.namenode.http-address.pluto.nn1" : "fks-fdp-galaxy-pluto-nn-0001.c.fks-fdp-galaxy.internal:50070",
      "dfs.namenode.http-address.pluto.nn2" : "fks-fdp-galaxy-pluto-nn-0002.c.fks-fdp-galaxy.internal:50070",
      "dfs.namenode.https-address.pluto.nn1" : "fks-fdp-galaxy-pluto-nn-0001.c.fks-fdp-galaxy.internal:50470",
      "dfs.namenode.https-address.pluto.nn2" : "fks-fdp-galaxy-pluto-nn-0002.c.fks-fdp-galaxy.internal:50470",
      "dfs.namenode.rpc-address.pluto.nn1" : "fks-fdp-galaxy-pluto-nn-0001.c.fks-fdp-galaxy.internal:8020",
      "dfs.namenode.rpc-address.pluto.nn2" : "fks-fdp-galaxy-pluto-nn-0002.c.fks-fdp-galaxy.internal:8020",
      "dfs.nameservices" : "pluto",
      "dfs.replication" : "3",
      "fdp.default.tier" : "Regular",
      "fdp.ironbank.url" : "http://console.fdp-ironbank-prod.fkcloud.in/",
      "fdp.orgqueue.cache.expire" : "300",
      "fdp.orgqueue.cache.size" : "100",
      "fdp.orgqueue.defaultQueue" : "adhoc",
      "fdp.orgqueue.gringotts.clientId" : "QAAS",
      "fdp.orgqueue.gringotts.clientSecret" : "*********(redacted)",
      "fdp.orgqueue.gringotts.url" : "http://gringotts.fdp-gringotts-prod.fkcloud.in/billingOrg/user",
      "fdp.orgqueue.queueNotFound.errorMessage" : "Queue mapping not found",
      "fdp.orgqueue.validInitiators" : "BADGER,QAAS,LQE,SUPERBI",
      "fdp.tier.expression" : "Regular",
      "fdp.tier.lqe.lock.wait.time.secs" : "30",
      "fdp.tier.lqe.max.sessions" : "3",
      "file.blocksize" : "67108864",
      "file.bytes-per-checksum" : "512",
      "file.client-write-packet-size" : "65536",
      "file.replication" : "1",
      "file.stream-buffer-size" : "4096",
      "fs.AbstractFileSystem.abfs.impl" : "org.apache.hadoop.fs.azurebfs.Abfs",
      "fs.AbstractFileSystem.abfss.impl" : "org.apache.hadoop.fs.azurebfs.Abfss",
      "fs.AbstractFileSystem.adl.impl" : "org.apache.hadoop.fs.adl.Adl",
      "fs.AbstractFileSystem.file.impl" : "org.apache.hadoop.fs.local.LocalFs",
      "fs.AbstractFileSystem.ftp.impl" : "org.apache.hadoop.fs.ftp.FtpFs",
      "fs.AbstractFileSystem.gs.impl" : "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS",
      "fs.AbstractFileSystem.har.impl" : "org.apache.hadoop.fs.HarFs",
      "fs.AbstractFileSystem.hdfs.impl" : "org.apache.hadoop.fs.Hdfs",
      "fs.AbstractFileSystem.s3a.impl" : "org.apache.hadoop.fs.s3a.S3A",
      "fs.AbstractFileSystem.swebhdfs.impl" : "org.apache.hadoop.fs.SWebHdfs",
      "fs.AbstractFileSystem.viewfs.impl" : "org.apache.hadoop.fs.viewfs.ViewFs",
      "fs.AbstractFileSystem.wasb.impl" : "org.apache.hadoop.fs.azure.Wasb",
      "fs.AbstractFileSystem.wasbs.impl" : "org.apache.hadoop.fs.azure.Wasbs",
      "fs.AbstractFileSystem.webhdfs.impl" : "org.apache.hadoop.fs.WebHdfs",
      "fs.abfs.impl" : "org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem",
      "fs.abfss.impl" : "org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem",
      "fs.adl.impl" : "org.apache.hadoop.fs.adl.AdlFileSystem",
      "fs.adl.oauth2.access.token.provider.type" : "*********(redacted)",
      "fs.automatic.close" : "true",
      "fs.azure.authorization" : "false",
      "fs.azure.authorization.caching.enable" : "true",
      "fs.azure.local.sas.key.mode" : "false",
      "fs.azure.sas.expiry.period" : "90d",
      "fs.azure.saskey.usecontainersaskeyforallaccess" : "true",
      "fs.azure.secure.mode" : "false",
      "fs.azure.user.agent.prefix" : "unknown",
      "fs.client.resolve.remote.symlinks" : "true",
      "fs.client.resolve.topology.enabled" : "false",
      "fs.defaultFS" : "hdfs://pluto",
      "fs.df.interval" : "60000",
      "fs.du.interval" : "600000",
      "fs.ftp.data.connection.mode" : "ACTIVE_LOCAL_DATA_CONNECTION_MODE",
      "fs.ftp.host" : "0.0.0.0",
      "fs.ftp.host.port" : "21",
      "fs.ftp.impl" : "org.apache.hadoop.fs.ftp.FTPFileSystem",
      "fs.ftp.transfer.mode" : "BLOCK_TRANSFER_MODE",
      "fs.gs.auth.service.account.enable" : "true",
      "fs.gs.batch.threads" : "60",
      "fs.gs.impl" : "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem",
      "fs.gs.max.requests.per.batch" : "300",
      "fs.gs.reported.permissions" : "777",
      "fs.har.impl.disable.cache" : "true",
      "fs.permissions.umask-mode" : "022",
      "fs.s3.useRequesterPaysHeader" : "true",
      "fs.s3a.assumed.role.credentials.provider" : "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider",
      "fs.s3a.assumed.role.session.duration" : "30m",
      "fs.s3a.assumed.role.sts.endpoint.region" : "us-west-1",
      "fs.s3a.attempts.maximum" : "20",
      "fs.s3a.block.size" : "32M",
      "fs.s3a.buffer.dir" : "${hadoop.tmp.dir}/s3a",
      "fs.s3a.change.detection.mode" : "server",
      "fs.s3a.change.detection.source" : "etag",
      "fs.s3a.change.detection.version.required" : "true",
      "fs.s3a.committer.magic.enabled" : "false",
      "fs.s3a.committer.name" : "file",
      "fs.s3a.committer.staging.abort.pending.uploads" : "true",
      "fs.s3a.committer.staging.conflict-mode" : "fail",
      "fs.s3a.committer.staging.tmp.path" : "tmp/staging",
      "fs.s3a.committer.staging.unique-filenames" : "true",
      "fs.s3a.committer.threads" : "8",
      "fs.s3a.connection.establish.timeout" : "5000",
      "fs.s3a.connection.maximum" : "15",
      "fs.s3a.connection.ssl.enabled" : "true",
      "fs.s3a.connection.timeout" : "200000",
      "fs.s3a.downgrade.syncable.exceptions" : "true",
      "fs.s3a.endpoint" : "s3.amazonaws.com",
      "fs.s3a.etag.checksum.enabled" : "false",
      "fs.s3a.fast.upload.active.blocks" : "4",
      "fs.s3a.fast.upload.buffer" : "disk",
      "fs.s3a.impl" : "org.apache.hadoop.fs.s3a.S3AFileSystem",
      "fs.s3a.list.version" : "2",
      "fs.s3a.max.total.tasks" : "5",
      "fs.s3a.metadatastore.authoritative" : "false",
      "fs.s3a.metadatastore.impl" : "org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore",
      "fs.s3a.multiobjectdelete.enable" : "true",
      "fs.s3a.multipart.purge" : "false",
      "fs.s3a.multipart.purge.age" : "86400",
      "fs.s3a.multipart.size" : "100M",
      "fs.s3a.multipart.threshold" : "2147483647",
      "fs.s3a.paging.maximum" : "5000",
      "fs.s3a.path.style.access" : "false",
      "fs.s3a.readahead.range" : "64K",
      "fs.s3a.retry.interval" : "500ms",
      "fs.s3a.retry.limit" : "${fs.s3a.attempts.maximum}",
      "fs.s3a.retry.throttle.interval" : "1000ms",
      "fs.s3a.retry.throttle.limit" : "${fs.s3a.attempts.maximum}",
      "fs.s3a.s3guard.cli.prune.age" : "86400000",
      "fs.s3a.s3guard.ddb.background.sleep" : "25ms",
      "fs.s3a.s3guard.ddb.max.retries" : "9",
      "fs.s3a.s3guard.ddb.table.capacity.read" : "500",
      "fs.s3a.s3guard.ddb.table.capacity.write" : "100",
      "fs.s3a.s3guard.ddb.table.create" : "false",
      "fs.s3a.s3guard.ddb.throttle.retry.interval" : "100ms",
      "fs.s3a.socket.recv.buffer" : "8192",
      "fs.s3a.socket.send.buffer" : "8192",
      "fs.s3a.threads.keepalivetime" : "60",
      "fs.s3a.threads.max" : "10",
      "fs.swift.impl" : "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem",
      "fs.trash.checkpoint.interval" : "0",
      "fs.trash.interval" : "0",
      "fs.viewfs.rename.strategy" : "SAME_MOUNTPOINT",
      "fs.wasb.impl" : "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
      "fs.wasbs.impl" : "org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure",
      "ftp.blocksize" : "67108864",
      "ftp.bytes-per-checksum" : "512",
      "ftp.client-write-packet-size" : "65536",
      "ftp.replication" : "3",
      "ftp.stream-buffer-size" : "4096",
      "google.cloud.auth.service.account.enable" : "true",
      "ha.failover-controller.cli-check.rpc-timeout.ms" : "20000",
      "ha.failover-controller.graceful-fence.connection.retries" : "1",
      "ha.failover-controller.graceful-fence.rpc-timeout.ms" : "5000",
      "ha.failover-controller.new-active.rpc-timeout.ms" : "60000",
      "ha.health-monitor.check-interval.ms" : "1000",
      "ha.health-monitor.connect-retry-interval.ms" : "1000",
      "ha.health-monitor.rpc-timeout.ms" : "45000",
      "ha.health-monitor.sleep-after-disconnect.ms" : "1000",
      "ha.zookeeper.acl" : "world:anyone:rwcda",
      "ha.zookeeper.parent-znode" : "/hadoop-ha",
      "ha.zookeeper.session-timeout.ms" : "10000",
      "hadoop.caller.context.enabled" : "false",
      "hadoop.caller.context.max.size" : "128",
      "hadoop.caller.context.signature.max.size" : "40",
      "hadoop.common.configuration.version" : "3.0.0",
      "hadoop.http.authentication.kerberos.keytab" : "${user.home}/hadoop.keytab",
      "hadoop.http.authentication.kerberos.principal" : "HTTP/_HOST@LOCALHOST",
      "hadoop.http.authentication.signature.secret.file" : "*********(redacted)",
      "hadoop.http.authentication.simple.anonymous.allowed" : "true",
      "hadoop.http.authentication.token.validity" : "*********(redacted)",
      "hadoop.http.authentication.type" : "simple",
      "hadoop.http.cross-origin.allowed-headers" : "X-Requested-With,Content-Type,Accept,Origin",
      "hadoop.http.cross-origin.allowed-methods" : "GET,POST,HEAD",
      "hadoop.http.cross-origin.allowed-origins" : "*",
      "hadoop.http.cross-origin.enabled" : "false",
      "hadoop.http.cross-origin.max-age" : "1800",
      "hadoop.http.filter.initializers" : "org.apache.hadoop.http.lib.StaticUserWebFilter",
      "hadoop.http.logs.enabled" : "true",
      "hadoop.http.staticuser.user" : "dr.who",
      "hadoop.jetty.logs.serve.aliases" : "true",
      "hadoop.kerberos.kinit.command" : "kinit",
      "hadoop.kerberos.min.seconds.before.relogin" : "60",
      "hadoop.proxyuser.hive.groups" : "*",
      "hadoop.proxyuser.hive.hosts" : "*",
      "hadoop.registry.jaas.context" : "Client",
      "hadoop.registry.secure" : "false",
      "hadoop.registry.system.acls" : "sasl:yarn@, sasl:mapred@, sasl:hdfs@",
      "hadoop.registry.zk.connection.timeout.ms" : "15000",
      "hadoop.registry.zk.quorum" : "localhost:2181",
      "hadoop.registry.zk.retry.ceiling.ms" : "60000",
      "hadoop.registry.zk.retry.interval.ms" : "1000",
      "hadoop.registry.zk.retry.times" : "5",
      "hadoop.registry.zk.root" : "/registry",
      "hadoop.registry.zk.session.timeout.ms" : "60000",
      "hadoop.rpc.protection" : "authentication",
      "hadoop.rpc.socket.factory.class.default" : "org.apache.hadoop.net.StandardSocketFactory",
      "hadoop.security.auth_to_local" : "DEFAULT",
      "hadoop.security.auth_to_local.mechanism" : "hadoop",
      "hadoop.security.authentication" : "simple",
      "hadoop.security.authorization" : "false",
      "hadoop.security.credential.clear-text-fallback" : "true",
      "hadoop.security.crypto.buffer.size" : "8192",
      "hadoop.security.crypto.cipher.suite" : "AES/CTR/NoPadding",
      "hadoop.security.crypto.codec.classes.aes.ctr.nopadding" : "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec",
      "hadoop.security.dns.log-slow-lookups.enabled" : "false",
      "hadoop.security.dns.log-slow-lookups.threshold.ms" : "1000",
      "hadoop.security.group.mapping" : "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback",
      "hadoop.security.group.mapping.ldap.connection.timeout.ms" : "60000",
      "hadoop.security.group.mapping.ldap.conversion.rule" : "none",
      "hadoop.security.group.mapping.ldap.directory.search.timeout" : "10000",
      "hadoop.security.group.mapping.ldap.num.attempts" : "3",
      "hadoop.security.group.mapping.ldap.num.attempts.before.failover" : "3",
      "hadoop.security.group.mapping.ldap.posix.attr.gid.name" : "gidNumber",
      "hadoop.security.group.mapping.ldap.posix.attr.uid.name" : "uidNumber",
      "hadoop.security.group.mapping.ldap.read.timeout.ms" : "60000",
      "hadoop.security.group.mapping.ldap.search.attr.group.name" : "cn",
      "hadoop.security.group.mapping.ldap.search.attr.member" : "member",
      "hadoop.security.group.mapping.ldap.search.filter.group" : "(objectClass=group)",
      "hadoop.security.group.mapping.ldap.search.filter.user" : "(&(objectClass=user)(sAMAccountName={0}))",
      "hadoop.security.group.mapping.ldap.search.group.hierarchy.levels" : "0",
      "hadoop.security.group.mapping.ldap.ssl" : "false",
      "hadoop.security.group.mapping.providers.combined" : "true",
      "hadoop.security.groups.cache.background.reload" : "false",
      "hadoop.security.groups.cache.background.reload.threads" : "3",
      "hadoop.security.groups.cache.secs" : "300",
      "hadoop.security.groups.cache.warn.after.ms" : "5000",
      "hadoop.security.groups.negative-cache.secs" : "30",
      "hadoop.security.groups.shell.command.timeout" : "0s",
      "hadoop.security.instrumentation.requires.admin" : "false",
      "hadoop.security.java.secure.random.algorithm" : "SHA1PRNG",
      "hadoop.security.key.default.bitlength" : "128",
      "hadoop.security.key.default.cipher" : "AES/CTR/NoPadding",
      "hadoop.security.kms.client.authentication.retry-count" : "1",
      "hadoop.security.kms.client.encrypted.key.cache.expiry" : "43200000",
      "hadoop.security.kms.client.encrypted.key.cache.low-watermark" : "0.3f",
      "hadoop.security.kms.client.encrypted.key.cache.num.refill.threads" : "2",
      "hadoop.security.kms.client.encrypted.key.cache.size" : "500",
      "hadoop.security.kms.client.failover.sleep.base.millis" : "100",
      "hadoop.security.kms.client.failover.sleep.max.millis" : "2000",
      "hadoop.security.kms.client.timeout" : "60",
      "hadoop.security.random.device.file.path" : "/dev/urandom",
      "hadoop.security.sensitive-config-keys" : "*********(redacted)",
      "hadoop.security.uid.cache.secs" : "14400",
      "hadoop.service.shutdown.timeout" : "30s",
      "hadoop.shell.missing.defaultFs.warning" : "false",
      "hadoop.shell.safely.delete.limit.num.files" : "100",
      "hadoop.ssl.client.conf" : "ssl-client.xml",
      "hadoop.ssl.enabled" : "false",
      "hadoop.ssl.enabled.protocols" : "TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2",
      "hadoop.ssl.hostname.verifier" : "DEFAULT",
      "hadoop.ssl.keystores.factory.class" : "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory",
      "hadoop.ssl.require.client.cert" : "false",
      "hadoop.ssl.server.conf" : "ssl-server.xml",
      "hadoop.system.tags" : "YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT\n      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL",
      "hadoop.tags.system" : "YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT\n      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL",
      "hadoop.tmp.dir" : "/tmp/hadoop-${user.name}",
      "hadoop.user.group.static.mapping.overrides" : "dr.who=;",
      "hadoop.util.hash.type" : "murmur",
      "hadoop.workaround.non.threadsafe.getpwuid" : "true",
      "hadoop.zk.acl" : "world:anyone:rwcda",
      "hadoop.zk.num-retries" : "1000",
      "hadoop.zk.retry-interval-ms" : "1000",
      "hadoop.zk.timeout-ms" : "10000",
      "hbase.zookeeper.quorum" : "fks-fdp-galaxy-pluto-zkjn-0001.c.fks-fdp-galaxy.internal:2181,fks-fdp-galaxy-pluto-zkjn-0002.c.fks-fdp-galaxy.internal:2181,fks-fdp-galaxy-pluto-zkjn-0003.c.fks-fdp-galaxy.internal:2181",
      "hive.auto.convert.join" : "false",
      "hive.auto.convert.join.noconditionaltask" : "false",
      "hive.auto.convert.join.noconditionaltask.size" : "134217728",
      "hive.auto.convert.sortmerge.join" : "false",
      "hive.auto.convert.sortmerge.join.noconditionaltask" : "false",
      "hive.auto.convert.sortmerge.join.to.mapjoin" : "false",
      "hive.cbo.enable" : "true",
      "hive.cli.print.header" : "false",
      "hive.cluster.delegation.token.store.class" : "*********(redacted)",
      "hive.cluster.delegation.token.store.zookeeper.connectString" : "*********(redacted)",
      "hive.cluster.delegation.token.store.zookeeper.znode" : "*********(redacted)",
      "hive.compactor.abortedtxn.threshold" : "1000",
      "hive.compactor.check.interval" : "300L",
      "hive.compactor.delta.num.threshold" : "10",
      "hive.compactor.delta.pct.threshold" : "0.1f",
      "hive.compactor.initiator.on" : "false",
      "hive.compactor.worker.threads" : "0",
      "hive.compactor.worker.timeout" : "86400L",
      "hive.compute.query.using.stats" : "false",
      "hive.conf.restricted.list" : "hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role",
      "hive.convert.join.bucket.mapjoin.tez" : "true",
      "hive.default.fileformat.managed" : "ORC",
      "hive.driver.parallel.compilation" : "true",
      "hive.enforce.bucketing" : "true",
      "hive.enforce.sorting" : "true",
      "hive.enforce.sortmergebucketmapjoin" : "true",
      "hive.exec.compress.intermediate" : "true",
      "hive.exec.compress.output" : "false",
      "hive.exec.dynamic.partition" : "true",
      "hive.exec.dynamic.partition.mode" : "nonstrict",
      "hive.exec.failure.hooks" : "com.flipkart.fdp.hive.hooks.EnforceSingleQueryPostExecHook",
      "hive.exec.max.created.files" : "100000",
      "hive.exec.max.dynamic.partitions" : "1000000",
      "hive.exec.max.dynamic.partitions.pernode" : "1000000",
      "hive.exec.orc.compression.strategy" : "SPEED",
      "hive.exec.orc.default.compress" : "ZLIB",
      "hive.exec.orc.default.stripe.size" : "67108864",
      "hive.exec.parallel" : "true",
      "hive.exec.parallel.thread.number" : "8",
      "hive.exec.post.hooks" : "org.apache.hadoop.hive.ql.hooks.LineageLogger,com.flipkart.fdp.hive.hooks.EnforceSingleQueryPostExecHook",
      "hive.exec.pre.hooks" : "com.flipkart.fdp.hive.hooks.EnforceSingleQueryPreExecHook",
      "hive.exec.reducers.bytes.per.reducer" : "134217728",
      "hive.exec.reducers.max" : "250",
      "hive.exec.scratchdir" : "gs://stage-hive-metastore-pluto/user/hiveexternaldir",
      "hive.exec.submit.local.task.via.child" : "true",
      "hive.exec.submitviachild" : "false",
      "hive.execution.engine" : "tez",
      "hive.fetch.task.aggr" : "true",
      "hive.fetch.task.conversion" : "more",
      "hive.fetch.task.conversion.threshold" : "1073741824",
      "hive.fileformat.managed" : "ORC",
      "hive.heapsize" : "2048",
      "hive.input.format" : "org.apache.hadoop.hive.ql.io.CombineHiveInputFormat",
      "hive.limit.optimize.enable" : "true",
      "hive.limit.pushdown.memory.usage" : "0.2",
      "hive.map.aggr" : "true",
      "hive.map.aggr.hash.force.flush.memory.threshold" : "0.5",
      "hive.map.aggr.hash.min.reduction" : "0.5",
      "hive.map.aggr.hash.percentmemory" : "0.5",
      "hive.mapjoin.bucket.cache.size" : "1000",
      "hive.mapjoin.localtask.max.memory.usage" : "0.4",
      "hive.mapjoin.optimized.hashtable" : "false",
      "hive.mapred.reduce.tasks.speculative.execution" : "false",
      "hive.merge.mapfiles" : "true",
      "hive.merge.mapredfiles" : "false",
      "hive.merge.orcfile.stripe.level" : "true",
      "hive.merge.rcfile.block.level" : "true",
      "hive.merge.size.per.task" : "256000000",
      "hive.merge.smallfiles.avgsize" : "16000000",
      "hive.merge.tezfiles" : "true",
      "hive.metastore.authorization.storage.checks" : "false",
      "hive.metastore.cache.pinobjtypes" : "Table,Database,Type,FieldSchema,Order",
      "hive.metastore.client.connect.retry.delay" : "5s",
      "hive.metastore.client.scheme.handlers" : "com.flipkart.fdp.hive.metastore.ELBSchemeHandler",
      "hive.metastore.client.socket.timeout" : "1800s",
      "hive.metastore.connect.retries" : "24",
      "hive.metastore.execute.setugi" : "true",
      "hive.metastore.failure.retries" : "24",
      "hive.metastore.fshandler.threads" : "15",
      "hive.metastore.kerberos.keytab.file" : "/etc/security/keytabs/hive.service.keytab",
      "hive.metastore.kerberos.principal" : "hive/_HOST@EXAMPLE.COM",
      "hive.metastore.limit.partition.request" : "-1",
      "hive.metastore.metrics.enabled" : "true",
      "hive.metastore.pre.event.listeners" : "org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener",
      "hive.metastore.sasl.enabled" : "false",
      "hive.metastore.server.max.threads" : "100000",
      "hive.metastore.uris" : "thrift://10.116.17.2:9083",
      "hive.metastore.warehouse.dir" : "gs://stage-hive-metastore-pluto/apps/hive/warehouse",
      "hive.msck.path.validation" : "ignore",
      "hive.msck.repair.batch.size" : "70",
      "hive.optimize.bucketmapjoin" : "true",
      "hive.optimize.bucketmapjoin.sortedmerge" : "true",
      "hive.optimize.constant.propagation" : "true",
      "hive.optimize.index.filter" : "true",
      "hive.optimize.mapjoin.mapreduce" : "true",
      "hive.optimize.metadataonly" : "true",
      "hive.optimize.null.scan" : "true",
      "hive.optimize.reducededuplication" : "true",
      "hive.optimize.reducededuplication.min.reducer" : "4",
      "hive.optimize.sort.dynamic.partition" : "true",
      "hive.orc.compute.splits.num.threads" : "10",
      "hive.orc.splits.include.file.footer" : "false",
      "hive.prewarm.enabled" : "false",
      "hive.prewarm.numcontainers" : "10",
      "hive.querylog.enable.plan.progress" : "false",
      "hive.security.authenticator.manager" : "org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator",
      "hive.security.authorization.enabled" : "false",
      "hive.security.authorization.manager" : "org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory",
      "hive.security.authorization.sqlstd.confwhitelist.append" : "|initiator.*|job.*|mapred.*|badger.*|azkaban.*|tez.*|dfs.*|mapreduce.*|hive.*|hbase.*|light.*|beeline.*|orc.*|fdp.*|.*impersonation.*|fs.gs.*",
      "hive.security.metastore.authenticator.manager" : "org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator",
      "hive.security.metastore.authorization.auth.reads" : "true",
      "hive.security.metastore.authorization.manager" : "\n            org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider\n        ",
      "hive.server2.allow.user.substitution" : "true",
      "hive.server2.authentication" : "NONE",
      "hive.server2.authentication.spnego.keytab" : "HTTP/_HOST@EXAMPLE.COM",
      "hive.server2.authentication.spnego.principal" : "/etc/security/keytabs/spnego.service.keytab",
      "hive.server2.clear.dangling.scratchdir" : "true",
      "hive.server2.clear.dangling.scratchdir.interval" : "1800",
      "hive.server2.enable.doAs" : "true",
      "hive.server2.enable.impersonation" : "true",
      "hive.server2.idle.operation.timeout" : "1d",
      "hive.server2.idle.session.timeout" : "12h",
      "hive.server2.logging.operation.enabled" : "true",
      "hive.server2.logging.operation.log.location" : "${java.io.tmpdir}/${user.name}/operation_logs",
      "hive.server2.metrics.enabled" : "true",
      "hive.server2.session.check.interval" : "60m",
      "hive.server2.support.dynamic.service.discovery" : "true",
      "hive.server2.table.type.mapping" : "CLASSIC",
      "hive.server2.tez.default.queues" : "default",
      "hive.server2.tez.initialize.default.sessions" : "false",
      "hive.server2.tez.sessions.per.default.queue" : "1",
      "hive.server2.thrift.http.path" : "cliservice",
      "hive.server2.thrift.http.port" : "10001",
      "hive.server2.thrift.max.worker.threads" : "500",
      "hive.server2.thrift.sasl.qop" : "auth",
      "hive.server2.transport.mode" : "http",
      "hive.server2.use.SSL" : "false",
      "hive.server2.zookeeper.namespace" : "fks-fdp-galaxy-hive3-hs2-pluto",
      "hive.session.history.enabled" : "false",
      "hive.smbjoin.cache.rows" : "1000",
      "hive.stats.autogather" : "true",
      "hive.stats.dbclass" : "fs",
      "hive.stats.fetch.column.stats" : "false",
      "hive.stats.fetch.partition.stats" : "false",
      "hive.strict.checks.cartesian.product" : "false",
      "hive.strict.checks.type.safety" : "false",
      "hive.support.concurrency" : "false",
      "hive.support.sql11.reserved.keywords" : "true",
      "hive.tez.auto.reducer.parallelism" : "false",
      "hive.tez.container.size" : "2560",
      "hive.tez.cpu.vcores" : "-1",
      "hive.tez.dynamic.partition.pruning" : "true",
      "hive.tez.dynamic.partition.pruning.max.data.size" : "104857600",
      "hive.tez.dynamic.partition.pruning.max.event.size" : "1048576",
      "hive.tez.input.format" : "org.apache.hadoop.hive.ql.io.HiveInputFormat",
      "hive.tez.log.level" : "INFO",
      "hive.tez.max.partition.factor" : "2.0",
      "hive.tez.min.partition.factor" : "0.25",
      "hive.tez.smb.number.waves" : "0.5",
      "hive.txn.manager" : "org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager",
      "hive.txn.max.open.batch" : "1000",
      "hive.txn.timeout" : "300",
      "hive.user.install.directory" : "gs://stage-hive-metastore-pluto/user/",
      "hive.vectorized.complex.types.enabled" : "false",
      "hive.vectorized.execution.enabled" : "true",
      "hive.vectorized.execution.ptf.enabled" : "false",
      "hive.vectorized.execution.reduce.enabled" : "true",
      "hive.vectorized.groupby.checkinterval" : "4096",
      "hive.vectorized.groupby.complex.types.enabled" : "false",
      "hive.vectorized.groupby.flush.percent" : "0.1",
      "hive.vectorized.groupby.maxentries" : "10000",
      "hive.zookeeper.client.port" : "2181",
      "hive.zookeeper.namespace" : "fks-sco-hive-pluto-zookeeper-namespace",
      "hive.zookeeper.quorum" : "fks-fdp-galaxy-pluto-zkjn-0001.c.fks-fdp-galaxy.internal:2181,fks-fdp-galaxy-pluto-zkjn-0002.c.fks-fdp-galaxy.internal:2181,fks-fdp-galaxy-pluto-zkjn-0003.c.fks-fdp-galaxy.internal:2181",
      "hive.zookeeper.session.timeout" : "120s",
      "io.bytes.per.checksum" : "512",
      "io.compression.codec.bzip2.library" : "system-native",
      "io.compression.codec.lzo.class" : "com.hadoop.compression.lzo.LzoCodec",
      "io.compression.codecs" : "org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,com.hadoop.compression.fourmc.Lz4Codec,com.hadoop.compression.fourmc.Lz4MediumCodec,com.hadoop.compression.fourmc.Lz4HighCodec,com.hadoop.compression.fourmc.Lz4UltraCodec,com.hadoop.compression.fourmc.FourMcCodec,com.hadoop.compression.fourmc.FourMcMediumCodec,com.hadoop.compression.fourmc.FourMcHighCodec,com.hadoop.compression.fourmc.FourMcUltraCodec",
      "io.erasurecode.codec.rs-legacy.rawcoders" : "rs-legacy_java",
      "io.erasurecode.codec.rs.rawcoders" : "rs_native,rs_java",
      "io.erasurecode.codec.xor.rawcoders" : "xor_native,xor_java",
      "io.file.buffer.size" : "65536",
      "io.map.index.interval" : "128",
      "io.map.index.skip" : "0",
      "io.mapfile.bloom.error.rate" : "0.005",
      "io.mapfile.bloom.size" : "1048576",
      "io.seqfile.compress.blocksize" : "1000000",
      "io.seqfile.local.dir" : "${hadoop.tmp.dir}/io/local",
      "io.serializations" : "org.apache.hadoop.io.serializer.WritableSerialization",
      "io.skip.checksum.errors" : "false",
      "ipc.client.bind.wildcard.addr" : "false",
      "ipc.client.connect.max.retries" : "50",
      "ipc.client.connect.max.retries.on.timeouts" : "45",
      "ipc.client.connect.retry.interval" : "1000",
      "ipc.client.connect.timeout" : "20000",
      "ipc.client.connection.maxidletime" : "30000",
      "ipc.client.fallback-to-simple-auth-allowed" : "false",
      "ipc.client.idlethreshold" : "8000",
      "ipc.client.kill.max" : "10",
      "ipc.client.low-latency" : "false",
      "ipc.client.ping" : "true",
      "ipc.client.rpc-timeout.ms" : "0",
      "ipc.client.tcpnodelay" : "true",
      "ipc.maximum.data.length" : "67108864",
      "ipc.maximum.response.length" : "134217728",
      "ipc.ping.interval" : "60000",
      "ipc.server.listen.queue.size" : "128",
      "ipc.server.log.slow.rpc" : "false",
      "ipc.server.max.connections" : "0",
      "javax.jdo.option.ConnectionDriverName" : "com.mysql.jdbc.Driver",
      "javax.jdo.option.ConnectionPassword" : "*********(redacted)",
      "javax.jdo.option.ConnectionURL" : "jdbc:mysql://10.117.192.58/stage_hive_metastore?createDatabaseIfNotExist=true",
      "javax.jdo.option.ConnectionUserName" : "stage_sco_rw",
      "jobname.enricher.class" : "com.flipkart.fdp.hive.orgqueue.FDPJobNameEnricher",
      "map.sort.class" : "org.apache.hadoop.util.QuickSort",
      "net.topology.impl" : "org.apache.hadoop.net.NetworkTopology",
      "net.topology.node.switch.mapping.impl" : "org.apache.hadoop.net.ScriptBasedMapping",
      "net.topology.script.file.name" : "/etc/hadoop/conf/topology.py",
      "net.topology.script.number.args" : "100",
      "nfs.exports.allowed.hosts" : "* rw",
      "orc.schema.evolution.case.sensitive" : "false",
      "parser.timeoutSec" : "900",
      "queue.enforcer.class" : "com.flipkart.fdp.hive.orgqueue.OrgQueueEnforcerForInitiator",
      "rpc.metrics.quantile.enable" : "false",
      "seq.io.sort.factor" : "100",
      "seq.io.sort.mb" : "100",
      "tfile.fs.input.buffer.size" : "262144",
      "tfile.fs.output.buffer.size" : "262144",
      "tfile.io.chunk.size" : "1048576",
      "zookeeper.znode.parent" : "/hbase-unsecure"
    },
    "runtimeInfo" : {
      "javaHome" : "/usr/lib/jvm/oracle-java8-jdk-amd64/jre",
      "javaVersion" : "1.8.0_172 (Oracle Corporation)",
      "scalaVersion" : "version 2.12.17"
    },
    "sparkBuildInfo" : {
      "sparkBuildBranch" : "branch-3.4",
      "sparkBuildDate" : "2023-11-10T09:15:23Z",
      "sparkBuildRevision" : "43d4fad95b10e5c1e58d012048c97fed0912a64c",
      "sparkBuildUser" : "somi.biswas",
      "sparkBuildVersion" : "3.4.1.fdp.1"
    },
    "sparkProps" : {
      "spark.app.id" : "application_1699449121496_0763",
      "spark.app.name" : "Spark shell",
      "spark.app.startTime" : "1702907543468",
      "spark.app.submitTime" : "1702907539300",
      "spark.benchmarkId" : "20231218-192117-tpcds-100gb-iceberg-load",
      "spark.cleaner.ttl" : "86400",
      "spark.delta.logStore.gs.impl" : "io.delta.storage.GCSLogStore",
      "spark.driver.appUIAddress" : "http://fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:4040",
      "spark.driver.cores" : "1",
      "spark.driver.extraClassPath" : "gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar",
      "spark.driver.extraJavaOptions" : "-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Denv=prod -Dcom.sun.management.jmxremote.port=0 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=0 -XX:MaxDirectMemorySize=800M -XX:MaxMetaspaceSize=256M -XX:CompressedClassSpaceSize=100M -XX:+UnlockDiagnosticVMOptions -Djob.numOfRePartitions=30",
      "spark.driver.extraLibraryPath" : "/usr/flipkart/3.2.1-1/hadoop-hdfs/lib:/usr/flipkart/3.2.1-1/hadoop-hdfs/lib/native:/usr/lib/hadoop/lib:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/lib/x86_64-linux-gnu",
      "spark.driver.host" : "fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal",
      "spark.driver.memory" : "5120m",
      "spark.driver.memoryOverhead" : "4096",
      "spark.driver.port" : "43135",
      "spark.dynamicAllocation.enabled" : "true",
      "spark.dynamicAllocation.executorIdleTimeout" : "60s",
      "spark.dynamicAllocation.maxExecutors" : "200",
      "spark.eventLog.dir" : "gs://fks-fdp-infra-job-history/pluto/spark-job-history",
      "spark.eventLog.enabled" : "true",
      "spark.executor.cores" : "1",
      "spark.executor.extraClassPath" : "gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar",
      "spark.executor.extraJavaOptions" : "-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:MaxDirectMemorySize=8096m -Dio.netty.maxDirectMemory=8096m -XX:+UseG1GC -XX:ConcGCThreads=2",
      "spark.executor.extraLibraryPath" : "/usr/flipkart/3.2.1-1/hadoop-hdfs/lib:/usr/flipkart/3.2.1-1/hadoop-hdfs/lib/native:/usr/lib/hadoop/lib:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/lib/x86_64-linux-gnu",
      "spark.executor.id" : "driver",
      "spark.executor.memory" : "10240m",
      "spark.executor.memoryOverhead" : "4096",
      "spark.fdp.orgqueue.cache.expire" : "300",
      "spark.fdp.orgqueue.cache.size" : "100",
      "spark.fdp.orgqueue.defaultQueue" : "adhoc",
      "spark.fdp.orgqueue.gringotts.clientId" : "QAAS",
      "spark.fdp.orgqueue.gringotts.clientSecret" : "*********(redacted)",
      "spark.fdp.orgqueue.gringotts.url" : "http://gringotts.fdp-gringotts-prod.fkcloud.in/billingOrg/user",
      "spark.fdp.orgqueue.ironbank.url" : "http://console.fdp-ironbank-prod.fkcloud.in/queue",
      "spark.fdp.orgqueue.queueNotFound.errorMessage" : "Queue mapping not found",
      "spark.fdp.orgqueue.validInitiators" : "BADGER,QAAS",
      "spark.hadoop.fs.s3.useRequesterPaysHeader" : "true",
      "spark.hadoop.yarn.timeline-service.enabled" : "false",
      "spark.history.fs.cleaner.interval" : "1d",
      "spark.history.fs.cleaner.maxAge" : "60d",
      "spark.history.fs.logDirectory" : "gs://fks-fdp-infra-job-history/pluto/spark-job-history",
      "spark.history.provider" : "org.apache.spark.deploy.history.FsHistoryProvider",
      "spark.home" : "/var/lib/fk-pf-spark3-4",
      "spark.jars" : "",
      "spark.master" : "yarn",
      "spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS" : "fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal,fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal",
      "spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES" : "http://fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal:8088/proxy/application_1699449121496_0763,http://fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal:8088/proxy/application_1699449121496_0763",
      "spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.RM_HA_URLS" : "fks-fdp-galaxy-pluto-rm-0001.c.fks-fdp-galaxy.internal:8088,fks-fdp-galaxy-pluto-rm-0002.c.fks-fdp-galaxy.internal:8088",
      "spark.queue.enforcer.class" : "com.flipkart.fdp.orgqueue.OrgQueueEnforcerForInitiator",
      "spark.repl.class.outputDir" : "/grid/1/spark3-4/tmp/spark-a3663513-5f27-4905-bf52-03c680aeae93/repl-bf162d39-3be6-4e96-8df1-6d1260105b27",
      "spark.repl.class.uri" : "spark://fdp-stage-azk-scheduled-executor-b5cff3b1-nftj.c.fks-sco-azkaban.internal:43135/classes",
      "spark.repl.local.jars" : "file:///home/vanshika.yadav/20231218-192117-tpcds-100gb-iceberg-load-benchmarks.jar,file:///var/lib/fk-pf-spark3-4/jars/iceberg-spark-runtime-3.4_2.12-1.4.0.jar",
      "spark.scheduler.mode" : "FIFO",
      "spark.shuffle.service.enabled" : "true",
      "spark.shuffle.useOldFetchProtocol" : "true",
      "spark.sql.catalog.hive_pluto" : "org.apache.iceberg.spark.SparkCatalog",
      "spark.sql.catalog.hive_pluto.type" : "hive",
      "spark.sql.catalog.hive_pluto.uri" : "thrift://10.116.17.2:9083",
      "spark.sql.catalogImplementation" : "hive",
      "spark.sql.extensions" : "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions",
      "spark.sql.sources.partitionOverwriteMode" : "dynamic",
      "spark.streaming.concurrentJobs" : "4",
      "spark.submit.deployMode" : "client",
      "spark.submit.pyFiles" : "",
      "spark.ui.filters" : "org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter",
      "spark.ui.showConsoleProgress" : "true",
      "spark.yarn.dist.jars" : "gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/hadoop-lzo-0.6.0.2.4.0.0-169.jar,file:/usr/share/fk-bigfoot-4mc/lib/hadoop-4mc-1.1.0.jar,gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/json-serde-1.3-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,gs://fksdp-mhosy-3nug-2a5d-systemlibs/libraries/hive/jars/dimlookup-hive-udf-1.0-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,file:///home/vanshika.yadav/20231218-192117-tpcds-100gb-iceberg-load-benchmarks.jar,file:///var/lib/fk-pf-spark3-4/jars/iceberg-spark-runtime-3.4_2.12-1.4.0.jar",
      "spark.yarn.historyServer.address" : "http://fks-fdp-galaxy-pluto-hs-0001-m.c.fks-fdp-galaxy.internal:18080",
      "spark.yarn.jars" : "",
      "spark.yarn.queue" : "de_adhoc",
      "spark.yarn.report.interval" : "60s",
      "spark.yarn.secondary.jars" : "hadoop-lzo-0.6.0.2.4.0.0-169.jar,hadoop-4mc-1.1.0.jar,json-serde-1.3-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,dimlookup-hive-udf-1.0-SNAPSHOT-jar-with-dependencies-jackson-shaded.jar,20231218-192117-tpcds-100gb-iceberg-load-benchmarks.jar,iceberg-spark-runtime-3.4_2.12-1.4.0.jar"
    },
    "systemProps" : {
      "SPARK_SUBMIT" : "true",
      "awt.toolkit" : "sun.awt.X11.XToolkit",
      "com.sun.management.jmxremote.authenticate" : "false",
      "com.sun.management.jmxremote.port" : "0",
      "com.sun.management.jmxremote.ssl" : "false",
      "env" : "prod",
      "file.encoding" : "ANSI_X3.4-1968",
      "file.encoding.pkg" : "sun.io",
      "file.separator" : "/",
      "java.awt.graphicsenv" : "sun.awt.X11GraphicsEnvironment",
      "java.awt.printerjob" : "sun.print.PSPrinterJob",
      "java.class.version" : "52.0",
      "java.endorsed.dirs" : "/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/endorsed",
      "java.ext.dirs" : "/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/ext:/usr/java/packages/lib/ext",
      "java.home" : "/usr/lib/jvm/oracle-java8-jdk-amd64/jre",
      "java.io.tmpdir" : "/tmp",
      "java.library.path" : "/usr/flipkart/3.2.1-1/hadoop-hdfs/lib:/usr/flipkart/3.2.1-1/hadoop-hdfs/lib/native:/usr/lib/hadoop/lib:/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/Linux-amd64-64:/usr/lib/x86_64-linux-gnu:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib",
      "java.rmi.server.randomIDs" : "true",
      "java.runtime.name" : "Java(TM) SE Runtime Environment",
      "java.runtime.version" : "1.8.0_172-b11",
      "java.specification.name" : "Java Platform API Specification",
      "java.specification.vendor" : "Oracle Corporation",
      "java.specification.version" : "1.8",
      "java.vendor" : "Oracle Corporation",
      "java.vendor.url" : "http://java.oracle.com/",
      "java.vendor.url.bug" : "http://bugreport.sun.com/bugreport/",
      "java.version" : "1.8.0_172",
      "java.vm.info" : "mixed mode",
      "java.vm.name" : "Java HotSpot(TM) 64-Bit Server VM",
      "java.vm.specification.name" : "Java Virtual Machine Specification",
      "java.vm.specification.vendor" : "Oracle Corporation",
      "java.vm.specification.version" : "1.8",
      "java.vm.vendor" : "Oracle Corporation",
      "java.vm.version" : "25.172-b11",
      "jdk.reflect.useDirectMethodHandle" : "false",
      "jetty.git.hash" : "da9a0b30691a45daf90a9f17b5defa2f1434f882",
      "job.numOfRePartitions" : "30",
      "line.separator" : "\n",
      "os.arch" : "amd64",
      "os.name" : "Linux",
      "os.version" : "4.19.0-19-cloud-amd64",
      "path.separator" : ":",
      "scala.usejavacp" : "true",
      "sun.arch.data.model" : "64",
      "sun.boot.class.path" : "/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/oracle-java8-jdk-amd64/jre/classes",
      "sun.boot.library.path" : "/usr/lib/jvm/oracle-java8-jdk-amd64/jre/lib/amd64",
      "sun.cpu.endian" : "little",
      "sun.cpu.isalist" : "",
      "sun.io.unicode.encoding" : "UnicodeLittle",
      "sun.java.command" : "org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.sql.sources.partitionOverwriteMode=dynamic --conf spark.executor.memory=10240m --conf spark.sql.catalog.hive_pluto=org.apache.iceberg.spark.SparkCatalog --conf spark.driver.memory=5120m --conf spark.sql.catalog.hive_pluto.uri=thrift://10.116.17.2:9083 --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.hive_pluto.type=hive --conf spark.benchmarkId=20231218-192117-tpcds-100gb-iceberg-load --conf spark.executor.extraJavaOptions=-XX:MaxDirectMemorySize=8096m -Dio.netty.maxDirectMemory=8096m -XX:+UseG1GC -XX:ConcGCThreads=2 --conf spark.hadoop.fs.s3.useRequesterPaysHeader=true --conf spark.delta.logStore.gs.impl=io.delta.storage.GCSLogStore --class org.apache.spark.repl.Main --name Spark shell --queue de_adhoc --jars /home/vanshika.yadav/20231218-192117-tpcds-100gb-iceberg-load-benchmarks.jar,/var/lib/fk-pf-spark3-4/jars/iceberg-spark-runtime-3.4_2.12-1.4.0.jar spark-shell -I 20231218-192117-tpcds-100gb-iceberg-load_shell_init.scala",
      "sun.java.launcher" : "SUN_STANDARD",
      "sun.jnu.encoding" : "ANSI_X3.4-1968",
      "sun.management.compiler" : "HotSpot 64-Bit Tiered Compilers",
      "sun.os.patch.level" : "unknown",
      "user.country" : "US",
      "user.dir" : "/home/vanshika.yadav",
      "user.home" : "/home/vanshika.yadav",
      "user.language" : "en",
      "user.name" : "vanshika.yadav",
      "user.timezone" : "Asia/Kolkata"
    }
  }
}
2023-12-18T19:33:31.289 FILE UPLOAD: Failed to upload /home/vanshika.yadav/20231218-192117-tpcds-100gb-iceberg-load-report.json to hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/reports/json/: java.lang.IllegalArgumentException: Unsupported scheme hdfs.
2023-12-18T19:33:31.291 FILE UPLOAD: Failed to upload /home/vanshika.yadav/20231218-192117-tpcds-100gb-iceberg-load-report.csv to hdfs://pluto/tmp/vanshika-yadav/hudi-benchmarking/reports/csv/: java.lang.IllegalArgumentException: Unsupported scheme hdfs.
SUCCESS
